<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=description content="another hardware blog"><link href=https://yehowshuaimmanuel.com/mathematics/loss_function_gradient_derivation/ rel=canonical><meta name=author content="Yehowshua Immanuel"><meta name=lang:clipboard.copy content="Copy to clipboard"><meta name=lang:clipboard.copied content="Copied to clipboard"><meta name=lang:search.language content=en><meta name=lang:search.pipeline.stopwords content=True><meta name=lang:search.pipeline.trimmer content=True><meta name=lang:search.result.none content="No matching documents"><meta name=lang:search.result.one content="1 matching document"><meta name=lang:search.result.other content="# matching documents"><meta name=lang:search.tokenizer content=[\s\-]+><link rel="shortcut icon" href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.1, mkdocs-material-4.6.3"><title>loss function gradient derivation - planet program</title><link rel=stylesheet href=../../assets/stylesheets/application.8c7b2d21.css><link rel=stylesheet href=../../assets/stylesheets/application-palette.6fba2f6a.css><meta name=theme-color content=#fb8c00><script src=../../assets/javascripts/modernizr.eca31fed.js></script><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,400i,700%7CUbuntu+Mono&display=fallback"><style>body,input{font-family:"Ubuntu","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Ubuntu Mono","Courier New",Courier,monospace}</style><link rel=stylesheet href=../../assets/fonts/material-icons.css><script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-144903341-1", "yehowshuaimmanuel.com")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script><script async src=https://www.google-analytics.com/analytics.js></script></head> <body dir=ltr data-md-color-primary=orange data-md-color-accent=indigo> <svg class=md-svg> <defs> </defs> </svg> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay data-md-component=overlay for=__drawer></label> <a href=#gradients-on-loss-functions tabindex=0 class=md-skip> Skip to content </a> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid"> <div class=md-flex> <div class="md-flex__cell md-flex__cell--shrink"> <a href=https://yehowshuaimmanuel.com/ title="planet program" aria-label="planet program" class="md-header-nav__button md-logo"> <i class=md-icon>videogame_asset</i> </a> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--menu md-header-nav__button" for=__drawer></label> </div> <div class="md-flex__cell md-flex__cell--stretch"> <div class="md-flex__ellipsis md-header-nav__title" data-md-component=title> <span class=md-header-nav__topic> planet program </span> <span class=md-header-nav__topic> loss function gradient derivation </span> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--search md-header-nav__button" for=__search></label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input aria-label=search name=query placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=query data-md-state=active> <label class="md-icon md-search__icon" for=__search></label> <button type=reset class="md-icon md-search__icon" data-md-component=reset tabindex=-1> &#xE5CD; </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=result> <div class=md-search-result__meta> Type to start searching </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container> <nav class="md-tabs md-tabs--active" data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> home </a> </li> <li class=md-tabs__item> <a href=../../fpga/ class=md-tabs__link> hardware hacking </a> </li> <li class=md-tabs__item> <a href=./ class="md-tabs__link md-tabs__link--active"> mathematics </a> </li> <li class=md-tabs__item> <a href=../../computing_culture/intro/ class=md-tabs__link> blog </a> </li> <li class=md-tabs__item> <a href=../../Bible/Proverbs/14/ class=md-tabs__link> bible notes </a> </li> </ul> </div> </nav> <main class=md-main role=main> <div class="md-main__inner md-grid" data-md-component=container> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" data-md-level=0> <label class="md-nav__title md-nav__title--site" for=__drawer> <a href=https://yehowshuaimmanuel.com/ title="planet program" class="md-nav__button md-logo"> <i class=md-icon>videogame_asset</i> </a> planet program </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-1 type=checkbox id=nav-1> <label class=md-nav__link for=nav-1> home </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-1> home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. title=home class=md-nav__link> home </a> </li> <li class=md-nav__item> <a href=../../about/ title="about me" class=md-nav__link> about me </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-1-3 type=checkbox id=nav-1-3> <label class=md-nav__link for=nav-1-3> my setup </label> <nav class=md-nav data-md-component=collapsible data-md-level=2> <label class=md-nav__title for=nav-1-3> my setup </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../my_setup/hardware/ title=hardware class=md-nav__link> hardware </a> </li> <li class=md-nav__item> <a href=../../my_setup/software/ title=software class=md-nav__link> software </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-2 type=checkbox id=nav-2> <label class=md-nav__link for=nav-2> hardware hacking </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-2> hardware hacking </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../fpga/ title="getting started" class=md-nav__link> getting started </a> </li> <li class=md-nav__item> <a href=../../fpga/vhdl/getting_started_with_vhdl/ title="FOSS intro to VHDL" class=md-nav__link> FOSS intro to VHDL </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-2-3 type=checkbox id=nav-2-3> <label class=md-nav__link for=nav-2-3> fpga fun </label> <nav class=md-nav data-md-component=collapsible data-md-level=2> <label class=md-nav__title for=nav-2-3> fpga fun </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../fpga/hdl_wars/ title="hdl wars" class=md-nav__link> hdl wars </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-2-3-2 type=checkbox id=nav-2-3-2> <label class=md-nav__link for=nav-2-3-2> migen </label> <nav class=md-nav data-md-component=collapsible data-md-level=3> <label class=md-nav__title for=nav-2-3-2> migen </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../fpga/migen/why_migen/ title="why migen" class=md-nav__link> why migen </a> </li> <li class=md-nav__item> <a href=../../fpga/migen/ethernet_ecp5/ title="ethernet on ECP5" class=md-nav__link> ethernet on ECP5 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-2-3-3 type=checkbox id=nav-2-3-3> <label class=md-nav__link for=nav-2-3-3> verilator </label> <nav class=md-nav data-md-component=collapsible data-md-level=3> <label class=md-nav__title for=nav-2-3-3> verilator </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../fpga/verilator/why_verilator/ title="why verilator" class=md-nav__link> why verilator </a> </li> <li class=md-nav__item> <a href=../../fpga/verilator/some_notes_on_verilator/ title="some notes on verilator" class=md-nav__link> some notes on verilator </a> </li> <li class=md-nav__item> <a href=../../fpga/verilator/dpi/ title="verilator as a nMigen Python backend" class=md-nav__link> verilator as a nMigen Python backend </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-2-4 type=checkbox id=nav-2-4> <label class=md-nav__link for=nav-2-4> vintage tech projects </label> <nav class=md-nav data-md-component=collapsible data-md-level=2> <label class=md-nav__title for=nav-2-4> vintage tech projects </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Vintage_Tech/ title="hacking old tech" class=md-nav__link> hacking old tech </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-2-4-2 type=checkbox id=nav-2-4-2> <label class=md-nav__link for=nav-2-4-2> my classic mac </label> <nav class=md-nav data-md-component=collapsible data-md-level=3> <label class=md-nav__title for=nav-2-4-2> my classic mac </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Vintage_Tech/My_Mac_SE/ title="acquisition and ideas" class=md-nav__link> acquisition and ideas </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-2-4-3 type=checkbox id=nav-2-4-3> <label class=md-nav__link for=nav-2-4-3> gt retrotech </label> <nav class=md-nav data-md-component=collapsible data-md-level=3> <label class=md-nav__title for=nav-2-4-3> gt retrotech </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Vintage_Tech/retrotech/my_setup/ title="my setup" class=md-nav__link> my setup </a> </li> <li class=md-nav__item> <a href=../../Vintage_Tech/retrotech/first_steps/ title="first steps" class=md-nav__link> first steps </a> </li> <li class=md-nav__item> <a href=../../Vintage_Tech/retrotech/la_inspection/ title="la bus inpection" class=md-nav__link> la bus inpection </a> </li> <li class=md-nav__item> <a href=../../Vintage_Tech/retrotech/why_an_fpga/ title="why an fpga?" class=md-nav__link> why an fpga? </a> </li> <li class=md-nav__item> <a href=../../Vintage_Tech/retrotech/crt_smoke_test/ title="crt smoke test" class=md-nav__link> crt smoke test </a> </li> <li class=md-nav__item> <a href=../../Vintage_Tech/retrotech/fpga_logic_analyzer/ title="fpga logic analyzer" class=md-nav__link> fpga logic analyzer </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-3 type=checkbox id=nav-3 checked> <label class=md-nav__link for=nav-3> mathematics </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-3> mathematics </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-toggle md-nav__toggle" data-md-toggle=toc type=checkbox id=__toc> <a href=./ title="loss function gradient derivation" class="md-nav__link md-nav__link--active"> loss function gradient derivation </a> </li> <li class=md-nav__item> <a href=../EMAG_HW3/EMAG_HW3/ title="goodbye matlab, hello python with emag examples" class=md-nav__link> goodbye matlab, hello python with emag examples </a> </li> <li class=md-nav__item> <a href=../matlabs_very_real_problems/ title="Matlab's very real problems" class=md-nav__link> Matlab's very real problems </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> blog </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-4> blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../computing_culture/intro/ title="looking to the future" class=md-nav__link> looking to the future </a> </li> <li class=md-nav__item> <a href=../../computing_culture/on_open_source/ title="thoughts on open source" class=md-nav__link> thoughts on open source </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-5 type=checkbox id=nav-5> <label class=md-nav__link for=nav-5> bible notes </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-5> bible notes </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-5-1 type=checkbox id=nav-5-1> <label class=md-nav__link for=nav-5-1> Proverbs </label> <nav class=md-nav data-md-component=collapsible data-md-level=2> <label class=md-nav__title for=nav-5-1> Proverbs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Bible/Proverbs/14/ title="Chapter 14" class=md-nav__link> Chapter 14 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-5-2 type=checkbox id=nav-5-2> <label class=md-nav__link for=nav-5-2> Ezekiel </label> <nav class=md-nav data-md-component=collapsible data-md-level=2> <label class=md-nav__title for=nav-5-2> Ezekiel </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Bible/Ezekiel/3/ title="Chapter 3" class=md-nav__link> Chapter 3 </a> </li> <li class=md-nav__item> <a href=../../Bible/Ezekiel/13/ title="Chapter 13" class=md-nav__link> Chapter 13 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-5-3 type=checkbox id=nav-5-3> <label class=md-nav__link for=nav-5-3> Matthew </label> <nav class=md-nav data-md-component=collapsible data-md-level=2> <label class=md-nav__title for=nav-5-3> Matthew </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Bible/Matthew/4/ title="Chapter 4" class=md-nav__link> Chapter 4 </a> </li> <li class=md-nav__item> <a href=../../Bible/Matthew/13/ title="Chapter 13" class=md-nav__link> Chapter 13 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-5-4 type=checkbox id=nav-5-4> <label class=md-nav__link for=nav-5-4> John </label> <nav class=md-nav data-md-component=collapsible data-md-level=2> <label class=md-nav__title for=nav-5-4> John </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Bible/John/15/ title="Chapter 15" class=md-nav__link> Chapter 15 </a> </li> <li class=md-nav__item> <a href=../../Bible/John/17/ title="Chapter 17" class=md-nav__link> Chapter 17 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-5-5 type=checkbox id=nav-5-5> <label class=md-nav__link for=nav-5-5> Romans </label> <nav class=md-nav data-md-component=collapsible data-md-level=2> <label class=md-nav__title for=nav-5-5> Romans </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Bible/Romans/1/ title="Chapter 1" class=md-nav__link> Chapter 1 </a> </li> <li class=md-nav__item> <a href=../../Bible/Romans/3/ title="Chapter 3" class=md-nav__link> Chapter 3 </a> </li> <li class=md-nav__item> <a href=../../Bible/Romans/4/ title="Chapter 4" class=md-nav__link> Chapter 4 </a> </li> <li class=md-nav__item> <a href=../../Bible/Romans/6/ title="Chapter 6" class=md-nav__link> Chapter 6 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-5-6 type=checkbox id=nav-5-6> <label class=md-nav__link for=nav-5-6> Peter </label> <nav class=md-nav data-md-component=collapsible data-md-level=2> <label class=md-nav__title for=nav-5-6> Peter </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Bible/Peter/1/ title="Chapter 1" class=md-nav__link> Chapter 1 </a> </li> <li class=md-nav__item> <a href=../../Bible/Peter/5/ title="Chapter 5" class=md-nav__link> Chapter 5 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-5-7 type=checkbox id=nav-5-7> <label class=md-nav__link for=nav-5-7> 2 Peter </label> <nav class=md-nav data-md-component=collapsible data-md-level=2> <label class=md-nav__title for=nav-5-7> 2 Peter </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Bible/2_Peter/2/ title="Chapter 2" class=md-nav__link> Chapter 2 </a> </li> <li class=md-nav__item> <a href=../../Bible/2_Peter/3/ title="Chapter 3" class=md-nav__link> Chapter 3 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-5-8 type=checkbox id=nav-5-8> <label class=md-nav__link for=nav-5-8> Revelation </label> <nav class=md-nav data-md-component=collapsible data-md-level=2> <label class=md-nav__title for=nav-5-8> Revelation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Bible/Revelation/5/ title="Chapter 1" class=md-nav__link> Chapter 1 </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary"> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1 id=gradients-on-loss-functions>Gradients on Loss Functions<a class=headerlink href=#gradients-on-loss-functions title="Permanent link">&para;</a></h1> <p>The formulation for the gradient on the loss function with respect to the weights arises a lot in machine learning as the following:</p> <p>Consider a score matrix <span><span class=MathJax_Preview>S</span><script type=math/tex>S</script></span>, a weight matrix <span><span class=MathJax_Preview>W</span><script type=math/tex>W</script></span>, an input matrix <span><span class=MathJax_Preview>X</span><script type=math/tex>X</script></span> of compatible dimensions, and a scalar-valued loss function <span><span class=MathJax_Preview>L(S)</span><script type=math/tex>L(S)</script></span>. </p> <p>That is, we have:</p> <div> <div class=MathJax_Preview>\mathbf{S} = \mathbf{W} \mathbf{X}</div> <script type="math/tex; mode=display">\mathbf{S} = \mathbf{W} \mathbf{X}</script> </div> <p>I will provide intuition for the following relationships:</p> <div> <div class=MathJax_Preview> \frac{\partial L}{\partial \mathbf{W}_{ij}} = (\nabla_\mathbf{X} (L(\mathbf{X})) X^T))_{ij}</div> <script type="math/tex; mode=display"> \frac{\partial L}{\partial \mathbf{W}_{ij}} = (\nabla_\mathbf{X} (L(\mathbf{X})) X^T))_{ij}</script> </div> <p>and</p> <div> <div class=MathJax_Preview> \frac{\partial L}{\partial \mathbf{X}_{ij}} = (\mathbf{W^T} \nabla_\mathbf{W} (L(\mathbf{W}))_{ij}</div> <script type="math/tex; mode=display"> \frac{\partial L}{\partial \mathbf{X}_{ij}} = (\mathbf{W^T} \nabla_\mathbf{W} (L(\mathbf{W}))_{ij}</script> </div> <p>It is a simple formula but perhaps not so obvious. Here, I show where it comes from.</p> <p>Also see Wikipedia on <a href=https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-matrix>Scalar by Matrix</a>.</p> <p>For the following derivation, we have a bias of 0.</p> <h1 id=setup>Setup<a class=headerlink href=#setup title="Permanent link">&para;</a></h1> <pre class=codehilite><code class=language-python>from sympy import *</code></pre> <h2 id=weight-matrix-and-data-matrix>Weight Matrix and Data Matrix<a class=headerlink href=#weight-matrix-and-data-matrix title="Permanent link">&para;</a></h2> <pre class=codehilite><code class=language-python>#Weight Matrix - 3 classes and dimensionality of two
W = MatrixSymbol('W',3,2)
W.is_real = True
#Data to be trained on - 3 examples
X = MatrixSymbol('X',2,3)
X.is_real = True</code></pre> <pre class=codehilite><code class=language-python>Matrix(W)</code></pre> <p><span><span class=MathJax_Preview>\displaystyle \left[\begin{array}{cc}W_{0, 0} &amp; W_{0, 1}\\W_{1, 0} &amp; W_{1, 1}\\W_{2, 0} &amp; W_{2, 1}\end{array}\right]</span><script type=math/tex>\displaystyle \left[\begin{array}{cc}W_{0, 0} & W_{0, 1}\\W_{1, 0} & W_{1, 1}\\W_{2, 0} & W_{2, 1}\end{array}\right]</script></span></p> <pre class=codehilite><code class=language-python>Matrix(X)</code></pre> <p><span><span class=MathJax_Preview>\displaystyle \left[\begin{array}{ccc}X_{0, 0} &amp; X_{0, 1} &amp; X_{0, 2}\\X_{1, 0} &amp; X_{1, 1} &amp; X_{1, 2}\end{array}\right]</span><script type=math/tex>\displaystyle \left[\begin{array}{ccc}X_{0, 0} & X_{0, 1} & X_{0, 2}\\X_{1, 0} & X_{1, 1} & X_{1, 2}\end{array}\right]</script></span></p> <h2 id=classification>Classification<a class=headerlink href=#classification title="Permanent link">&para;</a></h2> <p>Y holds the classification for each vector in X. That is, vector <span><span class=MathJax_Preview>X_0</span><script type=math/tex>X_0</script></span>(column 0 of X) is labelled beforehand as belonging class 0, <span><span class=MathJax_Preview>X_1</span><script type=math/tex>X_1</script></span> belongs to class 1 and so on...</p> <pre class=codehilite><code class=language-python>Y = Matrix([0,1,2])</code></pre> <h2 id=score-matrix>Score Matrix<a class=headerlink href=#score-matrix title="Permanent link">&para;</a></h2> <pre class=codehilite><code class=language-python>S = Matrix(MatrixSymbol('s',3,3))
S</code></pre> <p><span><span class=MathJax_Preview>\displaystyle \left[\begin{array}{ccc}s_{0, 0} &amp; s_{0, 1} &amp; s_{0, 2}\\s_{1, 0} &amp; s_{1, 1} &amp; s_{1, 2}\\s_{2, 0} &amp; s_{2, 1} &amp; s_{2, 2}\end{array}\right]</span><script type=math/tex>\displaystyle \left[\begin{array}{ccc}s_{0, 0} & s_{0, 1} & s_{0, 2}\\s_{1, 0} & s_{1, 1} & s_{1, 2}\\s_{2, 0} & s_{2, 1} & s_{2, 2}\end{array}\right]</script></span></p> <p>Computing the values of the Score Matrix <span><span class=MathJax_Preview>S</span><script type=math/tex>S</script></span></p> <pre class=codehilite><code class=language-python>S_expanded = W*X
Matrix(S_expanded)</code></pre> <p><span><span class=MathJax_Preview>\displaystyle \left[\begin{array}{ccc}W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0} &amp; W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1} &amp; W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}\\W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0} &amp; W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1} &amp; W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}\\W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0} &amp; W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1} &amp; W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}\end{array}\right]</span><script type=math/tex>\displaystyle \left[\begin{array}{ccc}W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0} & W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1} & W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}\\W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0} & W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1} & W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}\\W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0} & W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1} & W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}\end{array}\right]</script></span></p> <h2 id=loss-function>Loss Function<a class=headerlink href=#loss-function title="Permanent link">&para;</a></h2> <p><span><span class=MathJax_Preview>L_i = -\log ( \frac {e^{S_{y[i],i}}} {\sum _r {e^{S_r,i}}})</span><script type=math/tex>L_i = -\log ( \frac {e^{S_{y[i],i}}} {\sum _r {e^{S_r,i}}})</script></span></p> <p><span><span class=MathJax_Preview>L = \frac {1} {N} \sum _i {L_i}</span><script type=math/tex>L = \frac {1} {N} \sum _i {L_i}</script></span></p> <p>Softmax computes the above loss on each column and then averages the columns.</p> <p>The elements in a particular column of the score vector above represent the clasification scores for a particular input vector column from X.</p> <p><span><span class=MathJax_Preview>[S_{0,0}, S_{1,0}, S_{2,0}]</span><script type=math/tex>[S_{0,0}, S_{1,0}, S_{2,0}]</script></span> is the first column in <span><span class=MathJax_Preview>S</span><script type=math/tex>S</script></span>. <span><span class=MathJax_Preview>S_{0,0}</span><script type=math/tex>S_{0,0}</script></span> is the score that vector <span><span class=MathJax_Preview>X_0</span><script type=math/tex>X_0</script></span> recieves for class 0. <span><span class=MathJax_Preview>S_{2,1}</span><script type=math/tex>S_{2,1}</script></span> is the score that vector <span><span class=MathJax_Preview>X_1</span><script type=math/tex>X_1</script></span> recieves for class 2 and so on.</p> <p>Consider a column <span><span class=MathJax_Preview>S_i</span><script type=math/tex>S_i</script></span> from <span><span class=MathJax_Preview>S</span><script type=math/tex>S</script></span>; a perfect weight matrix would generate a 0 in the rows of that column that do not correspond with the label of the corresponding <span><span class=MathJax_Preview>X</span><script type=math/tex>X</script></span> column, and a nonzero value in the row that corresponds to the correct label. </p> <p>The index of the correct label in column <span><span class=MathJax_Preview>S_i</span><script type=math/tex>S_i</script></span> is given by <span><span class=MathJax_Preview>S_{y[i],i}</span><script type=math/tex>S_{y[i],i}</script></span> where y is defined above.</p> <h3 id=compute-denominator-for-each-column>Compute denominator for each column<a class=headerlink href=#compute-denominator-for-each-column title="Permanent link">&para;</a></h3> <pre class=codehilite><code class=language-python>denoms = []
f_exp = lambda x : exp(x)
for col in range(3):
    denoms += [sum(Matrix(S[:,col]).applyfunc(f_exp))]</code></pre> <h4 id=column-sum-sanity-check>Column Sum Sanity Check<a class=headerlink href=#column-sum-sanity-check title="Permanent link">&para;</a></h4> <pre class=codehilite><code class=language-python>denoms[2]</code></pre> <p><span><span class=MathJax_Preview>\displaystyle e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}</span><script type=math/tex>\displaystyle e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}</script></span></p> <h3 id=get-numerator>Get numerator<a class=headerlink href=#get-numerator title="Permanent link">&para;</a></h3> <p>The Y vector tells us which row from each column in the score matrix S is the ground truth(a.k.a the correct class).</p> <p>The numerator of the operand of the -log in the loss formula is is e raised to the ground truth score.</p> <pre class=codehilite><code class=language-python>truth_scores = []
for col in range(3):
    truth_scores += [S[Y[col],col]]
numers = [exp(val) for val in truth_scores]</code></pre> <h3 id=compute-l_il_i>Compute <span><span class=MathJax_Preview>L_i</span><script type=math/tex>L_i</script></span><a class=headerlink href=#compute-l_il_i title="Permanent link">&para;</a></h3> <pre class=codehilite><code class=language-python>L_i = []
for i in range(3):
    L_i += [-(log(numers[i]) - log(denoms[i]))]</code></pre> <h4 id=sanity-check>Sanity Check<a class=headerlink href=#sanity-check title="Permanent link">&para;</a></h4> <pre class=codehilite><code class=language-python>L_i[0]</code></pre> <p><span><span class=MathJax_Preview>\displaystyle \log{\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \right)} - \log{\left(e^{s_{0, 0}} \right)}</span><script type=math/tex>\displaystyle \log{\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \right)} - \log{\left(e^{s_{0, 0}} \right)}</script></span></p> <h3 id=total-loss>Total Loss<a class=headerlink href=#total-loss title="Permanent link">&para;</a></h3> <p>Below is the analytical expression for our total loss - note that we have a bias of 0 in this example. </p> <p>Also note that the loss function is a function of the entries in the score matrix and the elements in the score matrix are in turn a function of the elements in the weight matrix.</p> <pre class=codehilite><code class=language-python>from functools import reduce
from operator import add
Loss = Rational(1,3)*reduce(add, L_i)
Loss</code></pre> <p><span><span class=MathJax_Preview>\displaystyle \frac{\log{\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \right)}}{3} + \frac{\log{\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}} \right)}}{3} + \frac{\log{\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}} \right)}}{3} - \frac{\log{\left(e^{s_{0, 0}} \right)}}{3} - \frac{\log{\left(e^{s_{1, 1}} \right)}}{3} - \frac{\log{\left(e^{s_{2, 2}} \right)}}{3}</span><script type=math/tex>\displaystyle \frac{\log{\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \right)}}{3} + \frac{\log{\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}} \right)}}{3} + \frac{\log{\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}} \right)}}{3} - \frac{\log{\left(e^{s_{0, 0}} \right)}}{3} - \frac{\log{\left(e^{s_{1, 1}} \right)}}{3} - \frac{\log{\left(e^{s_{2, 2}} \right)}}{3}</script></span></p> <h2 id=computing-the-gradient-with-respect-to>Computing the Gradient with respect to:<a class=headerlink href=#computing-the-gradient-with-respect-to title="Permanent link">&para;</a></h2> <p>Note: We substitute <span><span class=MathJax_Preview>S</span><script type=math/tex>S</script></span> for its expanded <span><span class=MathJax_Preview>W \times X</span><script type=math/tex>W \times X</script></span> form in the following computations.</p> <pre class=codehilite><code class=language-python>S_flat = [el for sublist in S.tolist() for el in sublist]
S_expanded_flat = [el for sublist in Matrix(S_expanded).tolist() for el in sublist]
sublist_S_to_W = [(S_flat[i], S_expanded_flat[i]) for i in range(9)]
sublist_W_to_S = [(S_expanded_flat[i],S_flat[i]) for i in range(9)]</code></pre> <h3 id=1-frac-partial-l-partial-w_00frac-partial-l-partial-w_00>1 .<span><span class=MathJax_Preview>\frac {\partial L} {\partial W_{0,0}}</span><script type=math/tex>\frac {\partial L} {\partial W_{0,0}}</script></span><a class=headerlink href=#1-frac-partial-l-partial-w_00frac-partial-l-partial-w_00 title="Permanent link">&para;</a></h3> <pre class=codehilite><code class=language-python>diff(Loss.subs(sublist_S_to_W),W[0,0])</code></pre> <p><span><span class=MathJax_Preview>\displaystyle - \frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3} + \frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{0, 2}}{3 \left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\right)} + \frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{0, 1}}{3 \left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\right)} + \frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3 \left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\right)}</span><script type=math/tex>\displaystyle - \frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3} + \frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{0, 2}}{3 \left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\right)} + \frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{0, 1}}{3 \left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\right)} + \frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3 \left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\right)}</script></span></p> <p>As you should notice, we have a sum of rational terms with <span><span class=MathJax_Preview>X_{0,0}</span><script type=math/tex>X_{0,0}</script></span>, <span><span class=MathJax_Preview>X_{0,1}</span><script type=math/tex>X_{0,1}</script></span>, and <span><span class=MathJax_Preview>X_{0,2}</span><script type=math/tex>X_{0,2}</script></span>. This seems to imply a sort of dot product between row 0 of X and some other vector to get the gradient with respect to <span><span class=MathJax_Preview>W_{0,0}</span><script type=math/tex>W_{0,0}</script></span>.</p> <h3 id=2-frac-partial-l-partial-w_10frac-partial-l-partial-w_10>2 . <span><span class=MathJax_Preview>\frac {\partial L} {\partial W_{1,0}}</span><script type=math/tex>\frac {\partial L} {\partial W_{1,0}}</script></span><a class=headerlink href=#2-frac-partial-l-partial-w_10frac-partial-l-partial-w_10 title="Permanent link">&para;</a></h3> <pre class=codehilite><code class=language-python>diff(Loss.subs(sublist_S_to_W),W[1,0])</code></pre> <p><span><span class=MathJax_Preview>\displaystyle - \frac{e^{- W_{1, 0} X_{0, 1} - W_{1, 1} X_{1, 1}} e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3} + \frac{e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} X_{0, 2}}{3 \left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\right)} + \frac{e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3 \left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\right)} + \frac{e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} X_{0, 0}}{3 \left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\right)}</span><script type=math/tex>\displaystyle - \frac{e^{- W_{1, 0} X_{0, 1} - W_{1, 1} X_{1, 1}} e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3} + \frac{e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} X_{0, 2}}{3 \left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\right)} + \frac{e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3 \left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\right)} + \frac{e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} X_{0, 0}}{3 \left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\right)}</script></span></p> <p>We again see a dependency on <span><span class=MathJax_Preview>X_{0,0}</span><script type=math/tex>X_{0,0}</script></span>, <span><span class=MathJax_Preview>X_{0,1}</span><script type=math/tex>X_{0,1}</script></span>, and <span><span class=MathJax_Preview>X_{0,2}</span><script type=math/tex>X_{0,2}</script></span> </p> <h3 id=3-frac-partial-l-partial-w_01frac-partial-l-partial-w_01>3 . <span><span class=MathJax_Preview>\frac {\partial L} {\partial W_{0,1}}</span><script type=math/tex>\frac {\partial L} {\partial W_{0,1}}</script></span><a class=headerlink href=#3-frac-partial-l-partial-w_01frac-partial-l-partial-w_01 title="Permanent link">&para;</a></h3> <pre class=codehilite><code class=language-python>diff(Loss.subs(sublist_S_to_W),W[0,1])</code></pre> <p><span><span class=MathJax_Preview>\displaystyle - \frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3} + \frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{1, 2}}{3 \left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\right)} + \frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{1, 1}}{3 \left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\right)} + \frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3 \left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\right)}</span><script type=math/tex>\displaystyle - \frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3} + \frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{1, 2}}{3 \left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\right)} + \frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{1, 1}}{3 \left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\right)} + \frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3 \left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\right)}</script></span></p> <p>We now see a dependency on <span><span class=MathJax_Preview>X_{1,0}</span><script type=math/tex>X_{1,0}</script></span>, <span><span class=MathJax_Preview>X_{1,1}</span><script type=math/tex>X_{1,1}</script></span>, and <span><span class=MathJax_Preview>X_{1,2}</span><script type=math/tex>X_{1,2}</script></span> </p> <h3 id=score-matrix_1>Score Matrix<a class=headerlink href=#score-matrix_1 title="Permanent link">&para;</a></h3> <p>Lets take a look at the gradient with respect to the score matrix. We accomplish this by substituing <span><span class=MathJax_Preview>S_{i,j}</span><script type=math/tex>S_{i,j}</script></span> = <span><span class=MathJax_Preview>W_{i,0}X_{0,j} + W_{i,1}X_{1,j}</span><script type=math/tex>W_{i,0}X_{0,j} + W_{i,1}X_{1,j}</script></span></p> <pre class=codehilite><code class=language-python>diff(Loss.subs(sublist_S_to_W),W[0,1]).subs(sublist_W_to_S)</code></pre> <p><span><span class=MathJax_Preview>\displaystyle - \frac{X_{1, 0}}{3} + \frac{e^{s_{0, 2}} X_{1, 2}}{3 \left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}\right)} + \frac{e^{s_{0, 1}} X_{1, 1}}{3 \left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}}\right)} + \frac{e^{s_{0, 0}} X_{1, 0}}{3 \left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\right)}</span><script type=math/tex>\displaystyle - \frac{X_{1, 0}}{3} + \frac{e^{s_{0, 2}} X_{1, 2}}{3 \left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}\right)} + \frac{e^{s_{0, 1}} X_{1, 1}}{3 \left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}}\right)} + \frac{e^{s_{0, 0}} X_{1, 0}}{3 \left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\right)}</script></span></p> <p>As you can see, in general, the gradient, <span><span class=MathJax_Preview>\frac {\partial L} {\partial W_{i,j}}</span><script type=math/tex>\frac {\partial L} {\partial W_{i,j}}</script></span> is a function of the score matrix S, and the input matrix X.</p> <h3 id=a-closer-look>A Closer Look<a class=headerlink href=#a-closer-look title="Permanent link">&para;</a></h3> <p>If you play around a bit more, you'll notice the <span><span class=MathJax_Preview>-\frac {X_{i,j}} {3}</span><script type=math/tex>-\frac {X_{i,j}} {3}</script></span> term that shows up whenever we differentiate with respect to <span><span class=MathJax_Preview>W_{i,j}</span><script type=math/tex>W_{i,j}</script></span>. This is no coincidence.</p> <p>Let's take a look at <span><span class=MathJax_Preview>\frac {\partial L_0} {\partial W_{0,0}}</span><script type=math/tex>\frac {\partial L_0} {\partial W_{0,0}}</script></span> - that is, the gradient of the loss on the first column in the score matrix with respect to <span><span class=MathJax_Preview>W_{0,0}</span><script type=math/tex>W_{0,0}</script></span></p> <pre class=codehilite><code class=language-python>diff(Rational(1,3)*L_i[0].subs(sublist_S_to_W), W[0,0]).subs(sublist_W_to_S)</code></pre> <p><span><span class=MathJax_Preview>\displaystyle - \frac{X_{0, 0}}{3} + \frac{e^{s_{0, 0}} X_{0, 0}}{3 \left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\right)}</span><script type=math/tex>\displaystyle - \frac{X_{0, 0}}{3} + \frac{e^{s_{0, 0}} X_{0, 0}}{3 \left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\right)}</script></span></p> <p>We see that <span><span class=MathJax_Preview>- \frac {X_{0,0}} {3}</span><script type=math/tex>- \frac {X_{0,0}} {3}</script></span> shows up in the first term!</p> <p>We know that the entire loss function gradient with respect to <span><span class=MathJax_Preview>W_{0,0}</span><script type=math/tex>W_{0,0}</script></span> is composed of the sum of the loss gradient on each score column again with respect to <span><span class=MathJax_Preview>W_{0,0}</span><script type=math/tex>W_{0,0}</script></span>. Another way to say this is:</p> <p><span><span class=MathJax_Preview>\frac {\partial L} {\partial W_{0,0}} = \frac {\partial L_0} {\partial W_{0,0}} + \frac {\partial L_1} {\partial W_{0,0}} +. \frac {\partial L_2} {\partial W_{0,0}}</span><script type=math/tex>\frac {\partial L} {\partial W_{0,0}} = \frac {\partial L_0} {\partial W_{0,0}} + \frac {\partial L_1} {\partial W_{0,0}} +. \frac {\partial L_2} {\partial W_{0,0}}</script></span></p> <p>To understand where that term comes from, we must look more closely at <span><span class=MathJax_Preview>L_i = -\log ( \frac {e^{S_{y[i],i}}} {\sum _r {e^{S_r,i}}}) = \log (e^{S_{y[i],i}}) - \log ({\sum _r {e^{S_r,i}}})</span><script type=math/tex>L_i = -\log ( \frac {e^{S_{y[i],i}}} {\sum _r {e^{S_r,i}}}) = \log (e^{S_{y[i],i}}) - \log ({\sum _r {e^{S_r,i}}})</script></span>.</p> <h3 id=playing-with-l_0l_0>Playing with <span><span class=MathJax_Preview>L_0</span><script type=math/tex>L_0</script></span><a class=headerlink href=#playing-with-l_0l_0 title="Permanent link">&para;</a></h3> <p>We have <span><span class=MathJax_Preview>L_0 = \log (e^{S_{y[0],0}}) - \log ({\sum _r {e^{S_r,0}}})</span><script type=math/tex>L_0 = \log (e^{S_{y[0],0}}) - \log ({\sum _r {e^{S_r,0}}})</script></span>.</p> <p><span><span class=MathJax_Preview>= \log (e^{S_{y[0],0}}) - \log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}})</span><script type=math/tex>= \log (e^{S_{y[0],0}}) - \log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}})</script></span></p> <p><span><span class=MathJax_Preview>= \log (e^{S_{0,0}}) - \log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}})</span><script type=math/tex>= \log (e^{S_{0,0}}) - \log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}})</script></span></p> <p>Taking the derivative with respect to <span><span class=MathJax_Preview>S_{0,0}</span><script type=math/tex>S_{0,0}</script></span> we have:</p> <p><span><span class=MathJax_Preview>\frac {\partial L_0} {\partial S_{0,0}} = 1 - \frac {e^{S_{0,0}}} {e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}}</span><script type=math/tex>\frac {\partial L_0} {\partial S_{0,0}} = 1 - \frac {e^{S_{0,0}}} {e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}}</script></span></p> <p>And taking the derivative with respect to <span><span class=MathJax_Preview>S_{1,0}</span><script type=math/tex>S_{1,0}</script></span> we have:</p> <p><span><span class=MathJax_Preview>\frac {\partial L_0} {\partial S_{0,0}} = - \frac {e^{S_{0,0}}} {e^{S_{1,0}} + e^{S_{1,0}} + e^{S_{2,0}}}</span><script type=math/tex>\frac {\partial L_0} {\partial S_{0,0}} = - \frac {e^{S_{0,0}}} {e^{S_{1,0}} + e^{S_{1,0}} + e^{S_{2,0}}}</script></span></p> <p>So we see that the <span><span class=MathJax_Preview>1</span><script type=math/tex>1</script></span> term shows up when we differentiate with respect to <span><span class=MathJax_Preview>S_{y[i],i}</span><script type=math/tex>S_{y[i],i}</script></span>.</p> <p>This <span><span class=MathJax_Preview>1</span><script type=math/tex>1</script></span> becomes <span><span class=MathJax_Preview>\frac {1} {3}</span><script type=math/tex>\frac {1} {3}</script></span> when we divide by the number of classes <span><span class=MathJax_Preview>N</span><script type=math/tex>N</script></span>.</p> <h2 id=wrapping-it-all-up>Wrapping it all up<a class=headerlink href=#wrapping-it-all-up title="Permanent link">&para;</a></h2> <p>Looking at the expanded expression for <span><span class=MathJax_Preview>\frac {\partial L} {\partial W_{0,0}}</span><script type=math/tex>\frac {\partial L} {\partial W_{0,0}}</script></span>, <span><span class=MathJax_Preview>\frac {\partial L} {\partial W_{1,0}}</span><script type=math/tex>\frac {\partial L} {\partial W_{1,0}}</script></span>, and <span><span class=MathJax_Preview>\frac {\partial L} {\partial W_{0,1}}</span><script type=math/tex>\frac {\partial L} {\partial W_{0,1}}</script></span> above suggest that there is a dot product between row of X and the gradient of the Loss function over a column of <span><span class=MathJax_Preview>S</span><script type=math/tex>S</script></span>. </p> <p>This is indeed true and careful inspection captures our observation in the following nice Matrix operation.</p> <div> <div class=MathJax_Preview> \begin{bmatrix} \frac{\partial L}{\partial W_{00}}&amp; \frac{\partial L}{\partial W_{01}}\\ \frac{\partial L}{\partial W_{10}}&amp; \frac{\partial L}{\partial W_{11}}\\ \frac{\partial L}{\partial W_{20}}&amp; \frac{\partial L}{\partial W_{21}} \end{bmatrix} =\begin{bmatrix} \frac{\partial L}{\partial S_{00}}&amp; \frac{\partial L}{\partial S_{01}}&amp; \frac{\partial L}{\partial S_{02}}\\ \frac{\partial L}{\partial S_{10}}&amp; \frac{\partial L}{\partial S_{11}}&amp; \frac{\partial L}{\partial S_{12}}\\ \frac{\partial L}{\partial S_{20}}&amp; \frac{\partial L}{\partial S_{21}}&amp; \frac{\partial L}{\partial S_{22}}\\ \end{bmatrix} X^T</div> <script type="math/tex; mode=display">
 \begin{bmatrix}
 \frac{\partial L}{\partial W_{00}}&
 \frac{\partial L}{\partial W_{01}}\\
 \frac{\partial L}{\partial W_{10}}&
 \frac{\partial L}{\partial W_{11}}\\
 \frac{\partial L}{\partial W_{20}}&
 \frac{\partial L}{\partial W_{21}}
 \end{bmatrix}
 =\begin{bmatrix}
 \frac{\partial L}{\partial S_{00}}&
 \frac{\partial L}{\partial S_{01}}&
 \frac{\partial L}{\partial S_{02}}\\
 \frac{\partial L}{\partial S_{10}}&
 \frac{\partial L}{\partial S_{11}}&
 \frac{\partial L}{\partial S_{12}}\\
 \frac{\partial L}{\partial S_{20}}&
 \frac{\partial L}{\partial S_{21}}&
 \frac{\partial L}{\partial S_{22}}\\
 \end{bmatrix} X^T</script> </div> <p>Here we confirm that <span><span class=MathJax_Preview>\frac {\partial L} {\partial W_{0,0}}</span><script type=math/tex>\frac {\partial L} {\partial W_{0,0}}</script></span> is indeed equal to element <span><span class=MathJax_Preview>(0,0)</span><script type=math/tex>(0,0)</script></span> of </p> <div> <div class=MathJax_Preview>\begin{bmatrix} \frac{\partial L}{\partial S_{00}}&amp; \frac{\partial L}{\partial S_{01}}&amp; \frac{\partial L}{\partial S_{02}}\\ \frac{\partial L}{\partial S_{10}}&amp; \frac{\partial L}{\partial S_{11}}&amp; \frac{\partial L}{\partial S_{12}}\\ \frac{\partial L}{\partial S_{20}}&amp; \frac{\partial L}{\partial S_{21}}&amp; \frac{\partial L}{\partial S_{22}}\\ \end{bmatrix} X^T</div> <script type="math/tex; mode=display">\begin{bmatrix}
 \frac{\partial L}{\partial S_{00}}&
 \frac{\partial L}{\partial S_{01}}&
 \frac{\partial L}{\partial S_{02}}\\
 \frac{\partial L}{\partial S_{10}}&
 \frac{\partial L}{\partial S_{11}}&
 \frac{\partial L}{\partial S_{12}}\\
 \frac{\partial L}{\partial S_{20}}&
 \frac{\partial L}{\partial S_{21}}&
 \frac{\partial L}{\partial S_{22}}\\
 \end{bmatrix} X^T</script> </div> <pre class=codehilite><code class=language-python>#elementwise derivate of Loss function with respect to elements of 
#score matrix as shown above
dLoss_dX = lambda x : diff(Loss,x)
dL_dS = S.applyfunc(dLoss_dX)</code></pre> <pre class=codehilite><code class=language-python>dW = dL_dS*Matrix(X.T)</code></pre> <pre class=codehilite><code class=language-python>expand(diff(Loss.subs(sublist_S_to_W),W[0,0])) - expand(dW[0,0].subs(sublist_S_to_W))</code></pre> <p><span><span class=MathJax_Preview>\displaystyle 0</span><script type=math/tex>\displaystyle 0</script></span></p> <p>Numerically evaluating dW would give us an incremental update to add to our weight matrix in order for us to minimize our loss function.</p> <h2 id=closing-remarks>Closing Remarks<a class=headerlink href=#closing-remarks title="Permanent link">&para;</a></h2> <p>What if we wish to take the derivative of the loss function wit respect to <span><span class=MathJax_Preview>X_{0,0}</span><script type=math/tex>X_{0,0}</script></span>?</p> <p>We can do the following:</p> <p><span><span class=MathJax_Preview>\begin{bmatrix} \frac{\partial L}{\partial X_{00}}&amp; \frac{\partial L}{\partial X_{01}}&amp; \frac{\partial L}{\partial X_{02}}\\ \frac{\partial L}{\partial X_{10}}&amp; \frac{\partial L}{\partial X_{11}}&amp; \frac{\partial L}{\partial X_{12}} \end{bmatrix} = W^T \begin{bmatrix} \frac{\partial L}{\partial S_{00}}&amp; \frac{\partial L}{\partial S_{01}}&amp; \frac{\partial L}{\partial S_{02}}\\ \frac{\partial L}{\partial S_{10}}&amp; \frac{\partial L}{\partial S_{11}}&amp; \frac{\partial L}{\partial S_{12}}\\ \frac{\partial L}{\partial S_{20}}&amp; \frac{\partial L}{\partial S_{21}}&amp; \frac{\partial L}{\partial S_{22}}\\ \end{bmatrix}</span><script type=math/tex>\begin{bmatrix}
 \frac{\partial L}{\partial X_{00}}&
 \frac{\partial L}{\partial X_{01}}&
 \frac{\partial L}{\partial X_{02}}\\
 \frac{\partial L}{\partial X_{10}}&
 \frac{\partial L}{\partial X_{11}}&
 \frac{\partial L}{\partial X_{12}}
 \end{bmatrix}
 = W^T  \begin{bmatrix}
 \frac{\partial L}{\partial S_{00}}&
 \frac{\partial L}{\partial S_{01}}&
 \frac{\partial L}{\partial S_{02}}\\
 \frac{\partial L}{\partial S_{10}}&
 \frac{\partial L}{\partial S_{11}}&
 \frac{\partial L}{\partial S_{12}}\\
 \frac{\partial L}{\partial S_{20}}&
 \frac{\partial L}{\partial S_{21}}&
 \frac{\partial L}{\partial S_{22}}\\
\end{bmatrix}</script></span></p> <p>Again, here we confirm that <span><span class=MathJax_Preview>\frac {\partial L} {\partial X_{0,0}}</span><script type=math/tex>\frac {\partial L} {\partial X_{0,0}}</script></span> is indeed equal to element <span><span class=MathJax_Preview>(0,0)</span><script type=math/tex>(0,0)</script></span> of <span><span class=MathJax_Preview>W^T \begin{pmatrix} \frac{\partial L}{\partial S_{00}}&amp; \frac{\partial L}{\partial S_{01}}&amp; \frac{\partial L}{\partial S_{02}}\\ \frac{\partial L}{\partial S_{10}}&amp; \frac{\partial L}{\partial S_{11}}&amp; \frac{\partial L}{\partial S_{12}}\\ \frac{\partial L}{\partial S_{20}}&amp; \frac{\partial L}{\partial S_{21}}&amp; \frac{\partial L}{\partial S_{22}}\\ \end{pmatrix}</span><script type=math/tex>W^T \begin{pmatrix}
 \frac{\partial L}{\partial S_{00}}&
 \frac{\partial L}{\partial S_{01}}&
 \frac{\partial L}{\partial S_{02}}\\
 \frac{\partial L}{\partial S_{10}}&
 \frac{\partial L}{\partial S_{11}}&
 \frac{\partial L}{\partial S_{12}}\\
 \frac{\partial L}{\partial S_{20}}&
 \frac{\partial L}{\partial S_{21}}&
 \frac{\partial L}{\partial S_{22}}\\
 \end{pmatrix}</script></span></p> <pre class=codehilite><code class=language-python>dX = (Matrix(W.T)*dL_dS)</code></pre> <pre class=codehilite><code class=language-python>expand(diff(Loss.subs(sublist_S_to_W),X[0,0])) - expand(dX[0,0].subs(sublist_S_to_W))</code></pre> <p><span><span class=MathJax_Preview>\displaystyle 0</span><script type=math/tex>\displaystyle 0</script></span></p> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid"> <a href=../../Vintage_Tech/retrotech/fpga_logic_analyzer/ title="fpga logic analyzer" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-flex__cell md-flex__cell--shrink"> <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i> </div> <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"> <span class=md-flex__ellipsis> <span class=md-footer-nav__direction> Previous </span> fpga logic analyzer </span> </div> </a> <a href=../EMAG_HW3/EMAG_HW3/ title="goodbye matlab, hello python with emag examples" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel=next> <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"> <span class=md-flex__ellipsis> <span class=md-footer-nav__direction> Next </span> goodbye matlab, hello python with emag examples </span> </div> <div class="md-flex__cell md-flex__cell--shrink"> <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2016 - 2020 Yehowshua Immanuel </div> powered by <a href=https://www.mkdocs.org target=_blank rel=noopener>MkDocs</a> and <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs</a> </div> <div class=md-footer-social> <link rel=stylesheet href=../../assets/fonts/font-awesome.css> <a href=https://github.com/BracketMaster target=_blank rel=noopener title=github-alt class="md-footer-social__link fa fa-github-alt"></a> <a href=https://www.linkedin.com/in/yehowshua-immanuel/ target=_blank rel=noopener title=linkedin class="md-footer-social__link fa fa-linkedin"></a> </div> </div> </div> </footer> </div> <script src=../../assets/javascripts/application.8e081faa.js></script> <script>app.initialize({version:"1.1",url:{base:"../.."}})</script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> </body> </html>