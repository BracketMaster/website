{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Planet Program \u00b6 I wonder if I would say I'm primarily a programmer. I wrote my first program when I was seven and have been programming on and off ever since. That said, software is very important to what I do, but at the end of the day, its still a tool that makes other activities I enjoy that much easier. Some incarnation of this website began when I was an undergrad mainly to help me keep track of projects I did in the past such as reverse enginneering the MacSE 1987 . It still serves that function; hence its current name. But it finds other uses such as helping document interesting artifacts of programmer culture. Enjoy! Recent Phenonmena \u00b6 verilator backend for nMigen \u00b6 I have been working on adding a verilator backend to nMigen-HDL Pythonthat is almost ready. More information on that effort here . nMigen is a high level HDL quite similar to Chisel but width the advantages of: No simulator boilerplate and easy support for multiple clock domains Implmented in Python. This means no slow SBT compile times or doing the SBT package manager dance. Pip just works. I also find Python idiosms much more simple/useful/natural than Scala's I offer a more in depth comparison of various HDLs and their problems here . recent new-old tech acquisition \u00b6 One of my youth pastors/mentors recently gave me his MacSE 1987! I started writing some drivers and designing an FPGA based graphics card for the MacSE. This is much harder than I originally thought.","title":"home"},{"location":"#welcome-to-planet-program","text":"I wonder if I would say I'm primarily a programmer. I wrote my first program when I was seven and have been programming on and off ever since. That said, software is very important to what I do, but at the end of the day, its still a tool that makes other activities I enjoy that much easier. Some incarnation of this website began when I was an undergrad mainly to help me keep track of projects I did in the past such as reverse enginneering the MacSE 1987 . It still serves that function; hence its current name. But it finds other uses such as helping document interesting artifacts of programmer culture. Enjoy!","title":"Welcome to Planet Program"},{"location":"#recent-phenonmena","text":"","title":"Recent Phenonmena"},{"location":"#verilator-backend-for-nmigen","text":"I have been working on adding a verilator backend to nMigen-HDL Pythonthat is almost ready. More information on that effort here . nMigen is a high level HDL quite similar to Chisel but width the advantages of: No simulator boilerplate and easy support for multiple clock domains Implmented in Python. This means no slow SBT compile times or doing the SBT package manager dance. Pip just works. I also find Python idiosms much more simple/useful/natural than Scala's I offer a more in depth comparison of various HDLs and their problems here .","title":"verilator backend for nMigen"},{"location":"#recent-new-old-tech-acquisition","text":"One of my youth pastors/mentors recently gave me his MacSE 1987! I started writing some drivers and designing an FPGA based graphics card for the MacSE. This is much harder than I originally thought.","title":"recent new-old tech acquisition"},{"location":"about/","text":"I am currently a master's student at Georgia Tech in Synergy Computer Hardware Systems Lab investigating discrete Convolutional Neural Networks accelerators. I wrote my first program when I was seven or so and have been programming on and off ever since. I am a Christian and an improving follower of Christ. I participate in various local ministries from time to time. You can view my r\u00e9sum\u00e9 here . Things that currently seem to consume my time Reverse Engineering FPGAs Porting Software to the venerable RISCV architecture Adding modern capabilities to really old computers (think 30yr old+) physics, electromagnetics, and mathematics skateboarding","title":"about me"},{"location":"Bible/2_Peter/2/","text":"17 \u00b6 A well without water - this is a well that is either not deep enough - or not near the water table which is replenished either by rainwater or river water. 19 \u00b6 Could this be those who receive the word of God - but have weeds in their life? This results from incomplete surrender of the mind to Jesus. The world considers its ways free - but does not understand they are a slave to curroption.","title":"Chapter 2"},{"location":"Bible/2_Peter/2/#17","text":"A well without water - this is a well that is either not deep enough - or not near the water table which is replenished either by rainwater or river water.","title":"17"},{"location":"Bible/2_Peter/2/#19","text":"Could this be those who receive the word of God - but have weeds in their life? This results from incomplete surrender of the mind to Jesus. The world considers its ways free - but does not understand they are a slave to curroption.","title":"19"},{"location":"Bible/2_Peter/3/","text":"14 \u00b6 Why must we be found in peace? - I vaguely remember some verse that suggested that peace is one of the signs of our salvation.","title":"Chapter 3"},{"location":"Bible/2_Peter/3/#14","text":"Why must we be found in peace? - I vaguely remember some verse that suggested that peace is one of the signs of our salvation.","title":"14"},{"location":"Bible/Acts/18/","text":"9 \u00b6 This lets us know that Paul was was afraid. 26 \u00b6 Notice that thus far in Acts, all who have recieved the baptism of John have readily recieved the Gospel of Jesus Christ. This agrees with John's words - \"I am one sent to prepare the way of the Lord\"","title":"18"},{"location":"Bible/Acts/18/#9","text":"This lets us know that Paul was was afraid.","title":"9"},{"location":"Bible/Acts/18/#26","text":"Notice that thus far in Acts, all who have recieved the baptism of John have readily recieved the Gospel of Jesus Christ. This agrees with John's words - \"I am one sent to prepare the way of the Lord\"","title":"26"},{"location":"Bible/Acts/19/","text":"26 \u00b6 Paul's ministry of the Gospel turned away MANY(according to Demetrius). This shows the power and effectiveness of the Gospel. 30 \u00b6 Paul was accountable to the disciples. He wanted to go, but he took the advice of his brethren. Paul was humble. 34 \u00b6 They cried out for about two hours. This is quite a long time. There is clearly a spiritual stronghold here.","title":"19"},{"location":"Bible/Acts/19/#26","text":"Paul's ministry of the Gospel turned away MANY(according to Demetrius). This shows the power and effectiveness of the Gospel.","title":"26"},{"location":"Bible/Acts/19/#30","text":"Paul was accountable to the disciples. He wanted to go, but he took the advice of his brethren. Paul was humble.","title":"30"},{"location":"Bible/Acts/19/#34","text":"They cried out for about two hours. This is quite a long time. There is clearly a spiritual stronghold here.","title":"34"},{"location":"Bible/Acts/21/","text":"1 \u00b6 Apparently - Luke was always following Paul - but he never mentions this. And interestingly, Luke was not one of the 12 disciples or 12 apostles, but he followed Jesus, then Peter, and now Paul. 8 \u00b6 The seven were a powerful bunch. They included the likes of Phillip and Steven.","title":"21"},{"location":"Bible/Acts/21/#1","text":"Apparently - Luke was always following Paul - but he never mentions this. And interestingly, Luke was not one of the 12 disciples or 12 apostles, but he followed Jesus, then Peter, and now Paul.","title":"1"},{"location":"Bible/Acts/21/#8","text":"The seven were a powerful bunch. They included the likes of Phillip and Steven.","title":"8"},{"location":"Bible/Acts/23/","text":"5 \u00b6 It is always important to respect the aithority that God has set up. 16 \u00b6 Paul had a sister.","title":"23"},{"location":"Bible/Acts/23/#5","text":"It is always important to respect the aithority that God has set up.","title":"5"},{"location":"Bible/Acts/23/#16","text":"Paul had a sister.","title":"16"},{"location":"Bible/Acts/24/","text":"13 \u00b6 It was very easy to dismantle the enemyies's accusations. 14 \u00b6 It seems that at this point in time, the Jews considered Christianity a sect of Judiasm? 25 \u00b6 Felix did not want to repent. The gospel of Jesus Christ demands a response from everyone.","title":"24"},{"location":"Bible/Acts/24/#13","text":"It was very easy to dismantle the enemyies's accusations.","title":"13"},{"location":"Bible/Acts/24/#14","text":"It seems that at this point in time, the Jews considered Christianity a sect of Judiasm?","title":"14"},{"location":"Bible/Acts/24/#25","text":"Felix did not want to repent. The gospel of Jesus Christ demands a response from everyone.","title":"25"},{"location":"Bible/Ezekiel/13/","text":"9, 14, 21, 22 \u00b6 In these verses, God makes it clear that His judgments are such that the people may know that he is the Lord. This lets us know that one way to pray is to ask God to act so that people may know that He is the Lord.","title":"Chapter 13"},{"location":"Bible/Ezekiel/13/#9-14-21-22","text":"In these verses, God makes it clear that His judgments are such that the people may know that he is the Lord. This lets us know that one way to pray is to ask God to act so that people may know that He is the Lord.","title":"9, 14, 21, 22"},{"location":"Bible/Ezekiel/20/","text":"9, 14, 21, 22 \u00b6 In these verses, God makes it clear that His judgments are such that the people may know that he is the Lord. This lets us know that one way to pray is to ask God to act so that people may know that He is the Lord.","title":"20"},{"location":"Bible/Ezekiel/20/#9-14-21-22","text":"In these verses, God makes it clear that His judgments are such that the people may know that he is the Lord. This lets us know that one way to pray is to ask God to act so that people may know that He is the Lord.","title":"9, 14, 21, 22"},{"location":"Bible/Ezekiel/3/","text":"7 \u00b6 But the house of Israel will not listen to you, because they will not listen to Me; for all the house of Israel are impudent and hard-hearted. If we don't listen to who God sends to us, how will we listen to God?","title":"Chapter 3"},{"location":"Bible/Ezekiel/3/#7","text":"But the house of Israel will not listen to you, because they will not listen to Me; for all the house of Israel are impudent and hard-hearted. If we don't listen to who God sends to us, how will we listen to God?","title":"7"},{"location":"Bible/John/15/","text":"4 \u00b6 It is impossible to live the empowered believer life without abiding in Jesus - who is the vine. What does and empowered believer life look like? Jesus spells out quite clearly what the empowered believer life looks like in the great commission. Healing and miracles Casting out demons Raising the dead","title":"Chapter 15"},{"location":"Bible/John/15/#4","text":"It is impossible to live the empowered believer life without abiding in Jesus - who is the vine. What does and empowered believer life look like? Jesus spells out quite clearly what the empowered believer life looks like in the great commission. Healing and miracles Casting out demons Raising the dead","title":"4"},{"location":"Bible/John/17/","text":"17 \u00b6 NKJV: John 17:17 Sanctify them by Your truth. Your word is truth. This makes sense. Sanctification is division. God's word seperates us from the world around us - clearly marking us for Jesus. Another scripture that shows the divisive action of God's word is NKJV: Heb 4:12 For the word of God is living and powerful, and sharper than any two-edged sword...","title":"Chapter 17"},{"location":"Bible/John/17/#17","text":"NKJV: John 17:17 Sanctify them by Your truth. Your word is truth. This makes sense. Sanctification is division. God's word seperates us from the world around us - clearly marking us for Jesus. Another scripture that shows the divisive action of God's word is NKJV: Heb 4:12 For the word of God is living and powerful, and sharper than any two-edged sword...","title":"17"},{"location":"Bible/Jude/1/","text":"God is able to keep us from stumbling.","title":"Jude 1"},{"location":"Bible/Luke/11/","text":"29 \u00b6 Unbelief is the same thing as evil. See Luke 9:41.","title":"11"},{"location":"Bible/Luke/11/#29","text":"Unbelief is the same thing as evil. See Luke 9:41.","title":"29"},{"location":"Bible/Luke/12/","text":"","title":"12"},{"location":"Bible/Luke/16/","text":"11 \u00b6 What are true riches?","title":"16"},{"location":"Bible/Luke/16/#11","text":"What are true riches?","title":"11"},{"location":"Bible/Luke/5/","text":"26 \u00b6 The crowd realized that He who can forgive sins must also have authority.","title":"5"},{"location":"Bible/Luke/5/#26","text":"The crowd realized that He who can forgive sins must also have authority.","title":"26"},{"location":"Bible/Luke/9/","text":"26 \u00b6 The crowd realized that He who can forgive sins must also have authority.","title":"9"},{"location":"Bible/Luke/9/#26","text":"The crowd realized that He who can forgive sins must also have authority.","title":"26"},{"location":"Bible/Matthew/13/","text":"Notice how the man who received the word by the wayside heard the word but did not have understanding. Contrast with the guy who both heard and understood the word in verse 23. I believe that the man who heard the word and received it with joy, but had a stony heart is a man with trust issues. Verse 22 indicates that when trouble arose, this man doesn't endure. This suggests that the man may ask himself \"can I trust God? I've been failed before. Let me trust in myself...\" This is exactly the position of a stony heart. The kingdom of God is worth everything I have and own.","title":"Chapter 13"},{"location":"Bible/Matthew/18/","text":"They answered Him, \u201cJesus of Nazareth.\u201d Jesus said to them, \u201cI am He.\u201d And Judas, who betrayed Him, also stood with them. Now when He said to them, \u201cI am He,\u201d they drew back and fell to the ground. Jesus knew His God given identity. His God given identity is backed by the power of the the Holy Spirit, bringing with it, tangible effects.","title":"Matthew 18"},{"location":"Bible/Matthew/21/","text":"16 \u00b6 Yes. Have you never read, Out of the mouth of babes and nursing infants You have perfected praise ? I finally know what this verse from Psalms 8:2 means! The young children were worshipping Jesus after he rode in on a colt. 21 \u00b6 I must have faith like Jesus.","title":"Matthew 21"},{"location":"Bible/Matthew/21/#16","text":"Yes. Have you never read, Out of the mouth of babes and nursing infants You have perfected praise ? I finally know what this verse from Psalms 8:2 means! The young children were worshipping Jesus after he rode in on a colt.","title":"16"},{"location":"Bible/Matthew/21/#21","text":"I must have faith like Jesus.","title":"21"},{"location":"Bible/Matthew/24/","text":"An atmosphere of sin stifles love. It is important to endure until the end.","title":"Matthew 24"},{"location":"Bible/Matthew/3/","text":"17 \u00b6 God is proud of sons that please Him.","title":"Matthew 3"},{"location":"Bible/Matthew/3/#17","text":"God is proud of sons that please Him.","title":"17"},{"location":"Bible/Matthew/4/","text":"5 \u00b6 Jesus allowed Himself to be tempted by the Devil. He didn't have to go where the Devil took Him, but he went nonetheless. Hebrews 4:15 provides some insight into this. For we do not have a High Priest who cannot sympathize with our weaknesses, but was in all points tempted as we are, yet without sin. Jesus allowed Himself to be tempted so that he can relate to our weaknesses.","title":"Chapter 4"},{"location":"Bible/Matthew/4/#5","text":"Jesus allowed Himself to be tempted by the Devil. He didn't have to go where the Devil took Him, but he went nonetheless. Hebrews 4:15 provides some insight into this. For we do not have a High Priest who cannot sympathize with our weaknesses, but was in all points tempted as we are, yet without sin. Jesus allowed Himself to be tempted so that he can relate to our weaknesses.","title":"5"},{"location":"Bible/Matthew/5/","text":"8 \u00b6 The pure in heart shall see God. If I want to see God, I must be pure in heart. 45, 48 \u00b6 We must do as our Father in heaven does.","title":"Matthew 5"},{"location":"Bible/Matthew/5/#8","text":"The pure in heart shall see God. If I want to see God, I must be pure in heart.","title":"8"},{"location":"Bible/Matthew/5/#45-48","text":"We must do as our Father in heaven does.","title":"45, 48"},{"location":"Bible/Matthew/7/","text":"23 \u00b6 I know it is possible to perform miracles and still live a completely un-godly life.","title":"Matthew 7"},{"location":"Bible/Matthew/7/#23","text":"I know it is possible to perform miracles and still live a completely un-godly life.","title":"23"},{"location":"Bible/Matthew/9/","text":"18 \u00b6 Jesus always responded to people's cries for help.","title":"Matthew 7"},{"location":"Bible/Matthew/9/#18","text":"Jesus always responded to people's cries for help.","title":"18"},{"location":"Bible/Peter/1/","text":"16 - 17 \u00b6 God purchased us with the most expensive price - the blood of his own son - so we must conduct ourselves in the fear of the Lord.","title":"Chapter 1"},{"location":"Bible/Peter/1/#16-17","text":"God purchased us with the most expensive price - the blood of his own son - so we must conduct ourselves in the fear of the Lord.","title":"16 - 17"},{"location":"Bible/Peter/5/","text":"5 \u00b6 Submission and humility go hand in hand. 9 \u00b6 Peter 1 mentions suffering a lot. NKJV: 1 Pet 4:12 Beloved, do not think it strange concerning the fiery trial which is to try you, as though some strange thing happened to you; but rejoice to the extent that you partake of Christ\u2019s sufferings, that when His glory is revealed And in this verse, Peter reminds us that our brothers in the faith also suffer.","title":"Chapter 5"},{"location":"Bible/Peter/5/#5","text":"Submission and humility go hand in hand.","title":"5"},{"location":"Bible/Peter/5/#9","text":"Peter 1 mentions suffering a lot. NKJV: 1 Pet 4:12 Beloved, do not think it strange concerning the fiery trial which is to try you, as though some strange thing happened to you; but rejoice to the extent that you partake of Christ\u2019s sufferings, that when His glory is revealed And in this verse, Peter reminds us that our brothers in the faith also suffer.","title":"9"},{"location":"Bible/Proverbs/10/","text":"Being righteous is very profitable as it delivers from death. Diligence is extremely valuable. He who walks in integrity has nothing to worry about. The words that a righteous man says are always uplifting and life-giving. Whatever money or goods the wicked gains always go towards more wickedness. Rejecting reproof also leads others astray. There's rarely any harm in talking less. Doing wrong is like a joke to a fool, but wisdom is pleasure to a man of understanding. This means that walking in the fear of the Lord is pleasure to those who have understanding.","title":"Proverbs 10"},{"location":"Bible/Proverbs/11/","text":"4 \u00b6 Righteousness delivers from death.","title":"Proverbs 11"},{"location":"Bible/Proverbs/11/#4","text":"Righteousness delivers from death.","title":"4"},{"location":"Bible/Proverbs/12/","text":"2 \u00b6 A good man obtains favor from the Lord. 27 \u00b6 Diligence is my precious posession.","title":"Proverbs 12"},{"location":"Bible/Proverbs/12/#2","text":"A good man obtains favor from the Lord.","title":"2"},{"location":"Bible/Proverbs/12/#27","text":"Diligence is my precious posession.","title":"27"},{"location":"Bible/Proverbs/13/","text":"13 \u00b6 But he who fears the commandment will be rewarded. There is reward in fearing God's commandments.","title":"Proverbs 13"},{"location":"Bible/Proverbs/13/#13","text":"But he who fears the commandment will be rewarded. There is reward in fearing God's commandments.","title":"13"},{"location":"Bible/Proverbs/14/","text":"1 \u00b6 The wise woman builds her house, But the foolish pulls it down with her hands. Wisdom builds up, but foolishness destroys. 7 \u00b6 Go from the presence of a foolish man, When you do not perceive in him the lips of knowledge. Hanging around a foolish man is dangerous. 8 \u00b6 The wisdom of the prudent is to understand his way, The wise understand the choices they make in life. 18 \u00b6 The simple inherit folly, But the prudent are crowned with knowledge. I think the simple are those don't see the value in assessing the meaning of their life and their ways. 19 \u00b6 The evil will bow before the good, And the wicked at the gates of the righteous. But I bet you will never find a righteous man bowing before an evil man... I think there is a verse for this. 23 \u00b6 In all labor there is profit, But idle chatter leads only to poverty. Useless chit chat does nothing. It does not add value to your business, in fact, its opportunity cost is high. 25 \u00b6 A true witness delivers souls, But a deceitful witness speaks lies. We should seek out those who speak truth. 29 \u00b6 But he who is impulsive exalts folly. The wise wait.","title":"Chapter 14"},{"location":"Bible/Proverbs/14/#1","text":"The wise woman builds her house, But the foolish pulls it down with her hands. Wisdom builds up, but foolishness destroys.","title":"1"},{"location":"Bible/Proverbs/14/#7","text":"Go from the presence of a foolish man, When you do not perceive in him the lips of knowledge. Hanging around a foolish man is dangerous.","title":"7"},{"location":"Bible/Proverbs/14/#8","text":"The wisdom of the prudent is to understand his way, The wise understand the choices they make in life.","title":"8"},{"location":"Bible/Proverbs/14/#18","text":"The simple inherit folly, But the prudent are crowned with knowledge. I think the simple are those don't see the value in assessing the meaning of their life and their ways.","title":"18"},{"location":"Bible/Proverbs/14/#19","text":"The evil will bow before the good, And the wicked at the gates of the righteous. But I bet you will never find a righteous man bowing before an evil man... I think there is a verse for this.","title":"19"},{"location":"Bible/Proverbs/14/#23","text":"In all labor there is profit, But idle chatter leads only to poverty. Useless chit chat does nothing. It does not add value to your business, in fact, its opportunity cost is high.","title":"23"},{"location":"Bible/Proverbs/14/#25","text":"A true witness delivers souls, But a deceitful witness speaks lies. We should seek out those who speak truth.","title":"25"},{"location":"Bible/Proverbs/14/#29","text":"But he who is impulsive exalts folly. The wise wait.","title":"29"},{"location":"Bible/Proverbs/6/","text":"6 \u00b6 There is no excuse for laziness.","title":"6"},{"location":"Bible/Proverbs/6/#6","text":"There is no excuse for laziness.","title":"6"},{"location":"Bible/Revelation/5/","text":"1 \u00b6 What is in the scroll? The strong angel was strong, but opening the scroll is about being worthy - not strength. Nobody was even able to look at the scroll. 8 \u00b6 It would make sense if the elders were people - they sang - \"you redeemed us\" 14 \u00b6 At some point - every created thing WILL worship God. But it appears to be a priviledge to worship at the throne of God. This idea seems consistent with other scriptures, for example, only the high priests could enter the inner temple - worship of this form was a priviledge. Perhaps a better word is restriction. We note that the psalmist says that only the pure of heart and those with clean hands can ascend the hill of the Lord. My conclusion is that worship is required, but proximity is a priviledge. Proximity is a component of relationship.","title":"Chapter 1"},{"location":"Bible/Revelation/5/#1","text":"What is in the scroll? The strong angel was strong, but opening the scroll is about being worthy - not strength. Nobody was even able to look at the scroll.","title":"1"},{"location":"Bible/Revelation/5/#8","text":"It would make sense if the elders were people - they sang - \"you redeemed us\"","title":"8"},{"location":"Bible/Revelation/5/#14","text":"At some point - every created thing WILL worship God. But it appears to be a priviledge to worship at the throne of God. This idea seems consistent with other scriptures, for example, only the high priests could enter the inner temple - worship of this form was a priviledge. Perhaps a better word is restriction. We note that the psalmist says that only the pure of heart and those with clean hands can ascend the hill of the Lord. My conclusion is that worship is required, but proximity is a priviledge. Proximity is a component of relationship.","title":"14"},{"location":"Bible/Romans/1/","text":"14 \u00b6 Why is Paul a debtor? 21 & 28 \u00b6 Unthankfulness towards God and failing to retain God in your heart go hand in hand. They both lead towards a debased thought patterns and consequently lifestyle. 32 \u00b6 You can know the righteous judgment of God and still not retain God in your thoughts.","title":"Chapter 1"},{"location":"Bible/Romans/1/#14","text":"Why is Paul a debtor?","title":"14"},{"location":"Bible/Romans/1/#21-28","text":"Unthankfulness towards God and failing to retain God in your heart go hand in hand. They both lead towards a debased thought patterns and consequently lifestyle.","title":"21 &amp; 28"},{"location":"Bible/Romans/1/#32","text":"You can know the righteous judgment of God and still not retain God in your thoughts.","title":"32"},{"location":"Bible/Romans/2/","text":"","title":"2"},{"location":"Bible/Romans/3/","text":"20 \u00b6 The law can never reveal our righteousness - it can only ever reveal our sin. 25 \u00b6 The blood of Jesus Christ applied to our lives causes God to pass over our sins during judgement. 27 \u00b6 There are two laws, the law of faith, and the law of works.","title":"Chapter 3"},{"location":"Bible/Romans/3/#20","text":"The law can never reveal our righteousness - it can only ever reveal our sin.","title":"20"},{"location":"Bible/Romans/3/#25","text":"The blood of Jesus Christ applied to our lives causes God to pass over our sins during judgement.","title":"25"},{"location":"Bible/Romans/3/#27","text":"There are two laws, the law of faith, and the law of works.","title":"27"},{"location":"Bible/Romans/4/","text":"5 \u00b6 God offers two ways of obtaining right standing before Him. Be 100% right all the time. Or","title":"Chapter 4"},{"location":"Bible/Romans/4/#5","text":"God offers two ways of obtaining right standing before Him. Be 100% right all the time. Or","title":"5"},{"location":"Bible/Romans/6/","text":"4 \u00b6 Being buried into death with Christ Jesus - there is another verse in the bible that mentions that Noah's boat in the flood was an archetype for baptism. God flooded the earth bringing death to those who were curropt leaving behind righteous Noah. 12 \u00b6 This means that Christ has empowered us to dethrone sin!! 14 \u00b6 Very fascinating!! When we are under Christ, sin has no dominion over us. So under Christ, if we sin, it is us who have submitted to sin. 20 \u00b6 Freedom to sin is a freedom that leads to death - which is no freedom at all. 23 \u00b6 When you sin, you earn death, - but eternal life is a GIFT from God. It can't be earned.","title":"Chapter 6"},{"location":"Bible/Romans/6/#4","text":"Being buried into death with Christ Jesus - there is another verse in the bible that mentions that Noah's boat in the flood was an archetype for baptism. God flooded the earth bringing death to those who were curropt leaving behind righteous Noah.","title":"4"},{"location":"Bible/Romans/6/#12","text":"This means that Christ has empowered us to dethrone sin!!","title":"12"},{"location":"Bible/Romans/6/#14","text":"Very fascinating!! When we are under Christ, sin has no dominion over us. So under Christ, if we sin, it is us who have submitted to sin.","title":"14"},{"location":"Bible/Romans/6/#20","text":"Freedom to sin is a freedom that leads to death - which is no freedom at all.","title":"20"},{"location":"Bible/Romans/6/#23","text":"When you sin, you earn death, - but eternal life is a GIFT from God. It can't be earned.","title":"23"},{"location":"Vintage_Tech/","text":"Hacking Old Tech!! \u00b6 The purpose of this set of pages is to make hardware hacking more accessible to the general populus. Although anyone is welcome to use this site, it is assumed that the user is comfortable filling in their knowledge gaps as necessary and knows how to use google. I set out to do this project with the intention of repairing the Macintosh SE. I also wanted to put my hardware skills to a practical test and holistically improve my knowledge of electronics. I originally started reverse engineering hardware as an intern at Georgia Tech RetroTech Laboratories. When my internship ended - about a year passed before I had another chance to do any serious retro-reverse engineering. Fortunately - over the course of that year - I acquired much more knowledge in the area of computing, mathematics, controls, signal processing, and 3d computer graphics. I should now be more equipped than ever to perform some amazing feats with old hardware. My Latest Vintage Acquisition \u00b6 A youth pastor at my church gave me his MacSE recently and it works perfectly. Since there is nothing to fix, I plan to write some software for it as well as design an accelerator for its PDS(Processor Direct Slot).","title":"hacking old tech"},{"location":"Vintage_Tech/#hacking-old-tech","text":"The purpose of this set of pages is to make hardware hacking more accessible to the general populus. Although anyone is welcome to use this site, it is assumed that the user is comfortable filling in their knowledge gaps as necessary and knows how to use google. I set out to do this project with the intention of repairing the Macintosh SE. I also wanted to put my hardware skills to a practical test and holistically improve my knowledge of electronics. I originally started reverse engineering hardware as an intern at Georgia Tech RetroTech Laboratories. When my internship ended - about a year passed before I had another chance to do any serious retro-reverse engineering. Fortunately - over the course of that year - I acquired much more knowledge in the area of computing, mathematics, controls, signal processing, and 3d computer graphics. I should now be more equipped than ever to perform some amazing feats with old hardware.","title":"Hacking Old Tech!!"},{"location":"Vintage_Tech/#my-latest-vintage-acquisition","text":"A youth pastor at my church gave me his MacSE recently and it works perfectly. Since there is nothing to fix, I plan to write some software for it as well as design an accelerator for its PDS(Processor Direct Slot).","title":"My Latest Vintage Acquisition"},{"location":"Vintage_Tech/My_Mac_SE/","text":"My MacSE \u00b6 This MacSE is mine! It was graciously given to me by the youth pastor at my church. As I mentioned earlier, it is in perfect working order. So I plan to make some serious modifications to make it a more compelling computer in the 21 st century. This computer has a fairly simple processor with no MMU, so it should be fairly easy to write software and install my own accelerators onto its bus slot in the back. Apple provided fairly good documentation for this machine and as far as I can tell - it should be fairly straight forward to write drivers for this machine. Writing Software \u00b6 Thanks to Retro68 for making a complete cross compiler toolchain that even works with MacSE emulators. This way, I can write and test all software while never leaving my Macbook Pro! The Retro68 toolchain is built on top of GCC and also has a framework that can generate graphical applications for Classic Mac. Interesting Ideas \u00b6 Custom ADB Mouse \u00b6 My youth pastor forgot or lost his mouse - so when I first got the MacSE, I had no mouse. I had a spare ADB mouse at school which I picked up later - but became seriously fascinated with the ADB protocol in the process - so one thing I would like to do is build my own ADB compliant mouse. FPGA Graphical Accelerator with Accompanying Drivers \u00b6 The MacSE also comes with an PDS expansion slot. The SE's system architecture is so simple that it should be possible to design an FPGA graphical accelerator and write a driver to interface with it. This might be useful for writing a rendering engine - something the MacSE could never do otherwise. The graphical accelerator could perhaps even be used to decode videos. Parallel Dot-Matrix Printer Driver \u00b6 My friend Michael Nolan recently ordered a parallel dot-matrix printer and wrote some graphical dithering algorithm to support printing pictures. It would be interesting to package these algorithms and accompanying fonts into a driver allowing my MacSE to print documents. The MacSE lacks a parallel port, so making the appropriate RS422 serial to parallel convertor would probably also be part of the project. SD to SCSI card Convertor \u00b6 Now I know that this has already been done, but these are expensive($75). I don't want to spend more than $50 total playing with this MacSE - so I opt to make my own. It should be perfectly possible to do this with a decently fast Arduino as the MacSE CPU is only clocked at 8MHZ.","title":"acquisition and ideas"},{"location":"Vintage_Tech/My_Mac_SE/#my-macse","text":"This MacSE is mine! It was graciously given to me by the youth pastor at my church. As I mentioned earlier, it is in perfect working order. So I plan to make some serious modifications to make it a more compelling computer in the 21 st century. This computer has a fairly simple processor with no MMU, so it should be fairly easy to write software and install my own accelerators onto its bus slot in the back. Apple provided fairly good documentation for this machine and as far as I can tell - it should be fairly straight forward to write drivers for this machine.","title":"My MacSE"},{"location":"Vintage_Tech/My_Mac_SE/#writing-software","text":"Thanks to Retro68 for making a complete cross compiler toolchain that even works with MacSE emulators. This way, I can write and test all software while never leaving my Macbook Pro! The Retro68 toolchain is built on top of GCC and also has a framework that can generate graphical applications for Classic Mac.","title":"Writing Software"},{"location":"Vintage_Tech/My_Mac_SE/#interesting-ideas","text":"","title":"Interesting Ideas"},{"location":"Vintage_Tech/My_Mac_SE/#custom-adb-mouse","text":"My youth pastor forgot or lost his mouse - so when I first got the MacSE, I had no mouse. I had a spare ADB mouse at school which I picked up later - but became seriously fascinated with the ADB protocol in the process - so one thing I would like to do is build my own ADB compliant mouse.","title":"Custom ADB Mouse"},{"location":"Vintage_Tech/My_Mac_SE/#fpga-graphical-accelerator-with-accompanying-drivers","text":"The MacSE also comes with an PDS expansion slot. The SE's system architecture is so simple that it should be possible to design an FPGA graphical accelerator and write a driver to interface with it. This might be useful for writing a rendering engine - something the MacSE could never do otherwise. The graphical accelerator could perhaps even be used to decode videos.","title":"FPGA Graphical Accelerator with Accompanying Drivers"},{"location":"Vintage_Tech/My_Mac_SE/#parallel-dot-matrix-printer-driver","text":"My friend Michael Nolan recently ordered a parallel dot-matrix printer and wrote some graphical dithering algorithm to support printing pictures. It would be interesting to package these algorithms and accompanying fonts into a driver allowing my MacSE to print documents. The MacSE lacks a parallel port, so making the appropriate RS422 serial to parallel convertor would probably also be part of the project.","title":"Parallel Dot-Matrix Printer Driver"},{"location":"Vintage_Tech/My_Mac_SE/#sd-to-scsi-card-convertor","text":"Now I know that this has already been done, but these are expensive($75). I don't want to spend more than $50 total playing with this MacSE - so I opt to make my own. It should be perfectly possible to do this with a decently fast Arduino as the MacSE CPU is only clocked at 8MHZ.","title":"SD to SCSI card Convertor"},{"location":"Vintage_Tech/retrotech/crt_smoke_test/","text":"After confirming the Logic Board had some issues, I wanted to make sure the CRT screen didn't have any issues. Should I manage to have the logic board generate proper video signals in the future, I need to be sure that the CRT can display them in order to verify the logic board is behaving properly. Since verifying the CRT screen works is fairly simple (primarily due to the fact that the CRT screen can really only do two things), I decided to tackle that first. I chose to program to FPGA to first draw bars on the screen as a simple smoke test. Prerequisites \u00b6 ghdl GTKwave IceStorm Implementation \u00b6 I wrote some VHDL to execute the behavior in the diagram to the right. It is important to note that the video signal is only allowed to be high in the \"active video\" portions of the signal. I then wrote a test bench, simulated the design in GHDL, and began managing my code with git once I wrote enough. Source Here Next, I had to configure the PLL to run at 15.667MHZ. I accomplished this by pushing the in clock to 141MHZ and using a clock divider synced at 9 cycles with a 44/55 duty cycle. After multiple commits and revisions from my friend Michael, I synthesized the design and plugged in my FPGA to the Mac logic board connector using the reference card here . Nothing happened at first because I used resistors as voltage dividers to protect my FPGA from sourcing too much current. I pulled out the resistors(in retrospect, I should have simply used a multi-meter to measure the resistance offered by the CRT interface instead of taking my chances) and heard the CRT make an erratic ticking noise, suggesting it had trouble syncing. I noticed I had swapped my H_SYNC and V_SYNC by accident, so I reversed them and voila! Updates \u00b6 I later realized that the VHDL I wrote did not correctly follow the video protocol as described in the figure above, so I updated the code, pushed changes, and also modified the code to display square test patterns, which reasonably confirms to me that the screen works.","title":"crt smoke test"},{"location":"Vintage_Tech/retrotech/crt_smoke_test/#prerequisites","text":"ghdl GTKwave IceStorm","title":"Prerequisites"},{"location":"Vintage_Tech/retrotech/crt_smoke_test/#implementation","text":"I wrote some VHDL to execute the behavior in the diagram to the right. It is important to note that the video signal is only allowed to be high in the \"active video\" portions of the signal. I then wrote a test bench, simulated the design in GHDL, and began managing my code with git once I wrote enough. Source Here Next, I had to configure the PLL to run at 15.667MHZ. I accomplished this by pushing the in clock to 141MHZ and using a clock divider synced at 9 cycles with a 44/55 duty cycle. After multiple commits and revisions from my friend Michael, I synthesized the design and plugged in my FPGA to the Mac logic board connector using the reference card here . Nothing happened at first because I used resistors as voltage dividers to protect my FPGA from sourcing too much current. I pulled out the resistors(in retrospect, I should have simply used a multi-meter to measure the resistance offered by the CRT interface instead of taking my chances) and heard the CRT make an erratic ticking noise, suggesting it had trouble syncing. I noticed I had swapped my H_SYNC and V_SYNC by accident, so I reversed them and voila!","title":"Implementation"},{"location":"Vintage_Tech/retrotech/crt_smoke_test/#updates","text":"I later realized that the VHDL I wrote did not correctly follow the video protocol as described in the figure above, so I updated the code, pushed changes, and also modified the code to display square test patterns, which reasonably confirms to me that the screen works.","title":"Updates"},{"location":"Vintage_Tech/retrotech/first_steps/","text":"First Steps \u00b6 The first thing I always do is power on the device. I was greeted with this screen. I wasn't quite sure what to do, so naturally I disassembled the machine and took a look inside. The logic board looked surprisingly clean. Finding no obvious issues with the motherboard, I proceeded to find some documentation on the internals of the Macintosh SE. Below I list some documents which proved rather helpful. Macintosh Family 2 nd Edition May 1990 Motorola 68K Microprocessor Datasheet Macintosh SE Schematics and Pinout Macintosh Family Hardware Reference With respect to the large body of knowledge that I gleaned from these resources, the following pieces of information largely determined my next steps. The CRT will behave like a 100KV capacitor if improperly handled The Macintosh SE has a Processor Direct Slot - The 1980s equivalent of a PCIE slot The Macintosh SE System I kernel partially resides on board in a ROM The SE uses a BBU which is essentially a very primitive MMU/bus arbiter To Do List \u00b6 Remove CopyRighted Material and insert legal reproductions Refresh Board Itemize components on board Remove all capacitors and Resistors Scrub Board with Alcohol Install newly purchased components Add updates to display controller Display Text Take text input from keyboard - (also useful to gain experience with UART for writing automatic C based prober in the future.) Test SUMP2 Design PCB for FPGA with memory and Voltage level shifter Design PCB that house the HIGH and LOW roms and captures contents to verify. If no success by this point: \u00b6 Determine Faulty component and replace - if not replaceable logic, wrtie FPGA implementation, reflow board, and seat FPGA into logic board.)","title":"first steps"},{"location":"Vintage_Tech/retrotech/first_steps/#first-steps","text":"The first thing I always do is power on the device. I was greeted with this screen. I wasn't quite sure what to do, so naturally I disassembled the machine and took a look inside. The logic board looked surprisingly clean. Finding no obvious issues with the motherboard, I proceeded to find some documentation on the internals of the Macintosh SE. Below I list some documents which proved rather helpful. Macintosh Family 2 nd Edition May 1990 Motorola 68K Microprocessor Datasheet Macintosh SE Schematics and Pinout Macintosh Family Hardware Reference With respect to the large body of knowledge that I gleaned from these resources, the following pieces of information largely determined my next steps. The CRT will behave like a 100KV capacitor if improperly handled The Macintosh SE has a Processor Direct Slot - The 1980s equivalent of a PCIE slot The Macintosh SE System I kernel partially resides on board in a ROM The SE uses a BBU which is essentially a very primitive MMU/bus arbiter","title":"First Steps"},{"location":"Vintage_Tech/retrotech/first_steps/#to-do-list","text":"Remove CopyRighted Material and insert legal reproductions Refresh Board Itemize components on board Remove all capacitors and Resistors Scrub Board with Alcohol Install newly purchased components Add updates to display controller Display Text Take text input from keyboard - (also useful to gain experience with UART for writing automatic C based prober in the future.) Test SUMP2 Design PCB for FPGA with memory and Voltage level shifter Design PCB that house the HIGH and LOW roms and captures contents to verify.","title":"To Do List"},{"location":"Vintage_Tech/retrotech/first_steps/#if-no-success-by-this-point","text":"Determine Faulty component and replace - if not replaceable logic, wrtie FPGA implementation, reflow board, and seat FPGA into logic board.)","title":"If no success by this point:"},{"location":"Vintage_Tech/retrotech/fpga_logic_analyzer/","text":"Due to the impracticality of using DDL's LA(logic analyzer) and the difficulty I encountered using the LA's interface, I decided to implement an LA on my own FPGA. Below are some prerequisites about for such an approach. FPGA has 3.3V logic levels Proficiency in VHDL/Verilog Proficiency with Python or some frontend language Proficiency with bash I found a Logic Analyzer project by the name of SUMP2 written and maintained by Kevin Hubbard from Washington, USA. After a few emails with Kevin Hubbard. I was able to get the RTL LA backend on my HX8k FPGA and the Python frontend working. I document this process below. Python Frontend Viewer \u00b6 The [repository here] contains the RTL modified Python frontend as described below. There you will find installation instructions and configuration specifics for running SUMP2 on modern Macs. Here are a couple notes about changes I made. Added \"Darwin\" to supported OS system - leaving system call behavior unchanged as MacOS terminal calls rarely deviate from those of Linux if self.os_sys != \"Linux\" and self.os_sys != \"Darwin\": And again on lines 2028, 2039, 2050, and 2178 if ( self.os_sys == \"Linux\" or self.os_sys == \"Darwin\"): I also changed the default font for MacOS system to make it more readable - pygame doesn't seem to play nice with non-retina display if(self.os_sys == \"Darwin\"): font_height = int( font_height, 20 ); # Conv String to Int else: font_height = int( font_height, 10 ); # Conv String to Int Verilog Backend and Sump2 Protocol \u00b6 It was necessary to make a number of modifications to the verilog to get it working with the Lattice HX8K. I originally used the proprietary ICEcube2 software to synthesize, time, layout, and verify my designs, but I quickly switched to the IceStorm toolchain by Clifford Wolf as it is much faster and reliable. Assigning Pinouts \u00b6 The sump2 verilog was designed for the Lattice HX1K icesticick board, so I first went through the HX1K's schematics while browsing through top.v. I had to hook up the FTDI ports to their respective pins on the HX8k. To do this, I referenced the HX8K schematics . I also assigned the status LEDs to their respective pins. set_io clk_12m J3 set_io ftdi_wi B10 set_io ftdi_ro B12 set_io spi_sck R11 set_io spi_cs_l R12 set_io spi_mosi P11 set_io spi_miso P12 set_io LED1 B5 set_io LED2 B4 set_io LED3 A2 set_io LED4 A1 set_io LED5 C5 set_io LED6 C4 set_io LED7 B3 set_io LED8 C3","title":"fpga logic analyzer"},{"location":"Vintage_Tech/retrotech/fpga_logic_analyzer/#python-frontend-viewer","text":"The [repository here] contains the RTL modified Python frontend as described below. There you will find installation instructions and configuration specifics for running SUMP2 on modern Macs. Here are a couple notes about changes I made. Added \"Darwin\" to supported OS system - leaving system call behavior unchanged as MacOS terminal calls rarely deviate from those of Linux if self.os_sys != \"Linux\" and self.os_sys != \"Darwin\": And again on lines 2028, 2039, 2050, and 2178 if ( self.os_sys == \"Linux\" or self.os_sys == \"Darwin\"): I also changed the default font for MacOS system to make it more readable - pygame doesn't seem to play nice with non-retina display if(self.os_sys == \"Darwin\"): font_height = int( font_height, 20 ); # Conv String to Int else: font_height = int( font_height, 10 ); # Conv String to Int","title":"Python Frontend Viewer"},{"location":"Vintage_Tech/retrotech/fpga_logic_analyzer/#verilog-backend-and-sump2-protocol","text":"It was necessary to make a number of modifications to the verilog to get it working with the Lattice HX8K. I originally used the proprietary ICEcube2 software to synthesize, time, layout, and verify my designs, but I quickly switched to the IceStorm toolchain by Clifford Wolf as it is much faster and reliable.","title":"Verilog Backend and Sump2 Protocol"},{"location":"Vintage_Tech/retrotech/fpga_logic_analyzer/#assigning-pinouts","text":"The sump2 verilog was designed for the Lattice HX1K icesticick board, so I first went through the HX1K's schematics while browsing through top.v. I had to hook up the FTDI ports to their respective pins on the HX8k. To do this, I referenced the HX8K schematics . I also assigned the status LEDs to their respective pins. set_io clk_12m J3 set_io ftdi_wi B10 set_io ftdi_ro B12 set_io spi_sck R11 set_io spi_cs_l R12 set_io spi_mosi P11 set_io spi_miso P12 set_io LED1 B5 set_io LED2 B4 set_io LED3 A2 set_io LED4 A1 set_io LED5 C5 set_io LED6 C4 set_io LED7 B3 set_io LED8 C3","title":"Assigning Pinouts"},{"location":"Vintage_Tech/retrotech/la_inspection/","text":"Logic Analyzer on the PDS \u00b6 As it turns out, the Macintosh SE has a Processor Direct Slot(PDS) which allowed developers to directly interface with external hardware. Since the Macintosh SE doesn't really have a kernel(just application calls that reside in the rom) or true virtual addressing, application developers would write drivers in assembly to query attached hardware directly. As on might imagine - such application could wreak much havok for the the end user. I decided to take advantage of the processor direct slot by plugging in the Logic Analyzer at Digital Design Lab (DDL) directly into the PDS. I first went to the DDL and wired up the Mac into the LA using this pinout . I then ran the LA software using the PDS clock as the acquisition edge. Unfortunately, I didn't take a picture of data capture on the LA's screen. But the CPU cycled through approximately the same 200 addresses repeatedly with B2E3 constantly showing up on the data bus. I read through the Mac SE hardware documentation on startup procedures, and my best guess is that reset pin keeps getting pulled active. The reset vector resides at virtual address 0x0000 which is later translated by the BBU to different physical address update this I stopped using the LA at DDL because of the difficulty and hassle. I built my own affordable LA described here . PDS Pinout \u00b6 ROW Column 1 Column 2 Column 3 32 -12V -5V +12V 31 Spare +12V +12V 30 Ground +12V Ground 29 D15 Ground C16M 28 D14 Ext.STK/ C8M 27 D13 Reserved E 26 D12 Reserved A23 25 D11 Reserved A22 24 D10 Reserved A21 23 D9 Reserved A20 22 D8 Spare A19 21 D7 BERR/ A18 20 D6 IPL2/ A17 19 D5 IPL1/ A16 18 D4 IPL0/ A15 17 D3 +5V A14 16 D2 +5V A13 15 D1 +5V A12 14 D0 +5V A11 13 +5V +5V A10 12 RESET/ HALT/ A9 11 PMCYC/ Reserved A8 10 AS/ Reserved A7 9 UDS/ Ground A6 8 LDS/ Ground A5 7 R/W/ Ground A4 6 DTACK/ Ground A3 5 BG/ Ground A2 4 BGACK/ Ground A1 3 BR/ Ground FC0 2 VMA/ Ground FC1 1 VPA/ Ground FC2","title":"la bus inpection"},{"location":"Vintage_Tech/retrotech/la_inspection/#logic-analyzer-on-the-pds","text":"As it turns out, the Macintosh SE has a Processor Direct Slot(PDS) which allowed developers to directly interface with external hardware. Since the Macintosh SE doesn't really have a kernel(just application calls that reside in the rom) or true virtual addressing, application developers would write drivers in assembly to query attached hardware directly. As on might imagine - such application could wreak much havok for the the end user. I decided to take advantage of the processor direct slot by plugging in the Logic Analyzer at Digital Design Lab (DDL) directly into the PDS. I first went to the DDL and wired up the Mac into the LA using this pinout . I then ran the LA software using the PDS clock as the acquisition edge. Unfortunately, I didn't take a picture of data capture on the LA's screen. But the CPU cycled through approximately the same 200 addresses repeatedly with B2E3 constantly showing up on the data bus. I read through the Mac SE hardware documentation on startup procedures, and my best guess is that reset pin keeps getting pulled active. The reset vector resides at virtual address 0x0000 which is later translated by the BBU to different physical address update this I stopped using the LA at DDL because of the difficulty and hassle. I built my own affordable LA described here .","title":"Logic Analyzer on the PDS"},{"location":"Vintage_Tech/retrotech/la_inspection/#pds-pinout","text":"ROW Column 1 Column 2 Column 3 32 -12V -5V +12V 31 Spare +12V +12V 30 Ground +12V Ground 29 D15 Ground C16M 28 D14 Ext.STK/ C8M 27 D13 Reserved E 26 D12 Reserved A23 25 D11 Reserved A22 24 D10 Reserved A21 23 D9 Reserved A20 22 D8 Spare A19 21 D7 BERR/ A18 20 D6 IPL2/ A17 19 D5 IPL1/ A16 18 D4 IPL0/ A15 17 D3 +5V A14 16 D2 +5V A13 15 D1 +5V A12 14 D0 +5V A11 13 +5V +5V A10 12 RESET/ HALT/ A9 11 PMCYC/ Reserved A8 10 AS/ Reserved A7 9 UDS/ Ground A6 8 LDS/ Ground A5 7 R/W/ Ground A4 6 DTACK/ Ground A3 5 BG/ Ground A2 4 BGACK/ Ground A1 3 BR/ Ground FC0 2 VMA/ Ground FC1 1 VPA/ Ground FC2","title":"PDS Pinout"},{"location":"Vintage_Tech/retrotech/my_setup/","text":"My Setup | Old Work at Georgia Tech RetroTech Laboratories \u00b6 These particular research efforts have been made possible by RetroTech at the Georgia Tech Library - an organization committed to preserving old technology. I currently write my software on a Mac Mini and Macbook (when I'm not near the Mac Mini) I have various electronic devices such as an FPGA (of course!), a mini UART cable (technically not FTDI - it's prolific tech.) some resistors, a soldering Iron, and some wires.","title":"my setup"},{"location":"Vintage_Tech/retrotech/my_setup/#my-setup-old-work-at-georgia-tech-retrotech-laboratories","text":"These particular research efforts have been made possible by RetroTech at the Georgia Tech Library - an organization committed to preserving old technology. I currently write my software on a Mac Mini and Macbook (when I'm not near the Mac Mini) I have various electronic devices such as an FPGA (of course!), a mini UART cable (technically not FTDI - it's prolific tech.) some resistors, a soldering Iron, and some wires.","title":"My Setup | Old Work at Georgia Tech RetroTech Laboratories"},{"location":"Vintage_Tech/retrotech/why_an_fpga/","text":"Using the Logic Analyzer at the Digital Design Laboratory to probe the Mac SE motherboard became inefficient rather quickly. Most of my time was spent repeatedly packing and transporting the Mac SE to DDL. In addition, setting up at DDL was expensive, requiring me to clean up my workstation daily for future users - a fairly time consuming procedure that prevented much progress. After some research, I decided to purchase an FPGA which could serve multiple purposes. Oscilloscope Logic Analyzer Replacement component for the Mac SE - such as RAM or BBU Reusable in the near future for Apple II restoration bought the Lattice HX8K FPGA for the following reasons Fits in my pocket! Completely reverse engineered with open source ICESTORM Toolchain Affordable at $50 Comes with PLL 100+ pins - suitable for probing 42 pin Mac SE PDS Comes with builtin FTDI/UART communication)","title":"why an fpga?"},{"location":"about/about/","text":"I am currently a master's student at Georgia Tech in Synergy Computer Hardware Systems Lab investigating discrete Convolutional Neural Networks accelerators. I wrote my first program when I was seven or so and have been programming on and off ever since. I am a Christian and an improving follower of Christ. I participate in various local ministries from time to time. You can view my r\u00e9sum\u00e9 here . Things that currently seem to consume my time Reverse Engineering FPGAs Porting Software to the venerable RISCV architecture Adding modern capabilities to really old computers (think 30yr old+) physics, electromagnetics, and mathematics skateboarding","title":"About"},{"location":"computing_culture/intro/","text":"Slowing Down \u00b6 I started programming when I was pretty young. Now I\u2019ve finished undergrad, and will soon finish my masters. I\u2019ve been speeding along through the computer landscape with only a narrow view of computing. A dissatisfaction with the insular understanding of computing I've developed over the years has prompted a new direction. A New Direction \u00b6 Looking to the Future \u00b6 Much has changed in the programmer culture landscape in the past 10 years. The first thing to come to mind is Github. Github is arguable the greatest gift to the programming community - not only because it promotes communal software development practices - but mainly because a notable proportion of computing innovation lives there. Github is the a massive prophecy book - ripe for harvesting - a database of what\u2019s trending in programmer culture and what will trend. Appreciating History \u00b6 There is something quite captivating about history. It was once current news, the state of the art - something worthy of notice that gets documented. Such events set in motion the water wheel of time and slowly make their descent into the pool of history. Computer history is no exception. What began as a mere hobby interest in fascinating antique machines quickly evolved. It quickly became apparent that there is much to be learned from these antiques - secrets of what make good technology. Computer software and accompanying culture is no exception. Computing has come a long way since Unix of \u201969 and has in a sense, become a beast of its own. To understand the nature of this beast, it is important to understand its past. Not just the technical aspects, but also the social aspects. It seems that the best developers and computer engineers among us are so busy creating that there is often little time left to evaluate computing culture and practices as whole, Eric S Raymond being a notable exception. Eric S. Raymond\u2019s The Art of Unix Programming (which will be evaluated here) is replete with information on programming culture and history in general, seems to offer much insight into the beast. The Journey \u00b6 Thus begins the journey to understand the beast. This is no small feat - but history seems like a reasonable starting point. There is more to computing than meets the eye.","title":"looking to the future"},{"location":"computing_culture/intro/#slowing-down","text":"I started programming when I was pretty young. Now I\u2019ve finished undergrad, and will soon finish my masters. I\u2019ve been speeding along through the computer landscape with only a narrow view of computing. A dissatisfaction with the insular understanding of computing I've developed over the years has prompted a new direction.","title":"Slowing Down"},{"location":"computing_culture/intro/#a-new-direction","text":"","title":"A New Direction"},{"location":"computing_culture/intro/#looking-to-the-future","text":"Much has changed in the programmer culture landscape in the past 10 years. The first thing to come to mind is Github. Github is arguable the greatest gift to the programming community - not only because it promotes communal software development practices - but mainly because a notable proportion of computing innovation lives there. Github is the a massive prophecy book - ripe for harvesting - a database of what\u2019s trending in programmer culture and what will trend.","title":"Looking to the Future"},{"location":"computing_culture/intro/#appreciating-history","text":"There is something quite captivating about history. It was once current news, the state of the art - something worthy of notice that gets documented. Such events set in motion the water wheel of time and slowly make their descent into the pool of history. Computer history is no exception. What began as a mere hobby interest in fascinating antique machines quickly evolved. It quickly became apparent that there is much to be learned from these antiques - secrets of what make good technology. Computer software and accompanying culture is no exception. Computing has come a long way since Unix of \u201969 and has in a sense, become a beast of its own. To understand the nature of this beast, it is important to understand its past. Not just the technical aspects, but also the social aspects. It seems that the best developers and computer engineers among us are so busy creating that there is often little time left to evaluate computing culture and practices as whole, Eric S Raymond being a notable exception. Eric S. Raymond\u2019s The Art of Unix Programming (which will be evaluated here) is replete with information on programming culture and history in general, seems to offer much insight into the beast.","title":"Appreciating History"},{"location":"computing_culture/intro/#the-journey","text":"Thus begins the journey to understand the beast. This is no small feat - but history seems like a reasonable starting point. There is more to computing than meets the eye.","title":"The Journey"},{"location":"computing_culture/on_open_source/","text":"As an active programmer, its nearly impossible not to encounter the open source community. I often encounter indivuals that vehemently ascribe to open source doctrine - a group that used to include me not so long ago. With time however, I developed different views. The Truth About Open Source \u00b6 The truth about open source tools is that they usually emerge via a collaboration between academia, industry, and or goverment(more often than not defense research projects). Just look at the table at the end of the article. Open source projects started by an individual with no univeristy, industry, or goverment influence are exceedingly rare. Open source typically doesn't exist in a vacuum. People rarely work for free. And if you trace the most successful open source projects, you will find a long trail of money and funding. The notion of kumbaya open source simply has no basis in reality. Claims to Fame \u00b6 Just better - Open source implmentations are just better There are a number of open source tools that are pretty comptetitive with their proprietary counterparts. Note that this is usually only true for tooling software such as bison, gcc, hugo, apache etc. This is rarely true with software suites such as GIMP or Inkscape - a notable exception being: Blender vs (3Ds Max or Maya or Houdini etc.) - Blender is a fantastic example of excellent open source software. I used to do much animation and video editing in Blender and its physics engine, rendering engine, hair engine, Python frontend API, GUI, and file stucture are light years ahead of the competition. Saves needless reimplmentation of common software needs which effectively results in standardization. - GNU LibC, NewlibC 1000 minds at once Just look at how many contributors the Linux Kernel has Freedom in all senses - (free speech, accessibility etc.) Imagine if you had to wait for your license to activate before you could compile your first program. Security - a symptom of accessibility - a large userbase means bugs don't get to linger around for long Fix it yourself - found a bug - fix it yourself Things that should remain closed source \u00b6 Mainly things that are core to a company's business model - things that make a company competitive such as: MacOS windows manager - arguably the best I've ever used. I have no use for the source code, ie: I don't have a reason to port it the MacOS WM to another system, the WM has virtually no bugs, it does what I want 95% of the time. MSWord - I haven't used Word in ages(mostly LaTeX and MDown). But I don't think anyone is interested in Word's source. I also don't see a huge community lining up to make contributions to Word. If extensibility is the concern, developers can already add extensions to Word. And that's that. Reference Table \u00b6 Project Roots Verilator DEC. GCC MIT Clang Apple, Berkeley Univeristy Firefox Netscape, Google TypeScript Microsoft Linux Kernel RedHat, IBM, Google Chromium Google Android Google Blender NeoGeo Apache Hobbyist MatplotLib Hobbyist NumPy University Research","title":"thoughts on open source"},{"location":"computing_culture/on_open_source/#the-truth-about-open-source","text":"The truth about open source tools is that they usually emerge via a collaboration between academia, industry, and or goverment(more often than not defense research projects). Just look at the table at the end of the article. Open source projects started by an individual with no univeristy, industry, or goverment influence are exceedingly rare. Open source typically doesn't exist in a vacuum. People rarely work for free. And if you trace the most successful open source projects, you will find a long trail of money and funding. The notion of kumbaya open source simply has no basis in reality.","title":"The Truth About Open Source"},{"location":"computing_culture/on_open_source/#claims-to-fame","text":"Just better - Open source implmentations are just better There are a number of open source tools that are pretty comptetitive with their proprietary counterparts. Note that this is usually only true for tooling software such as bison, gcc, hugo, apache etc. This is rarely true with software suites such as GIMP or Inkscape - a notable exception being: Blender vs (3Ds Max or Maya or Houdini etc.) - Blender is a fantastic example of excellent open source software. I used to do much animation and video editing in Blender and its physics engine, rendering engine, hair engine, Python frontend API, GUI, and file stucture are light years ahead of the competition. Saves needless reimplmentation of common software needs which effectively results in standardization. - GNU LibC, NewlibC 1000 minds at once Just look at how many contributors the Linux Kernel has Freedom in all senses - (free speech, accessibility etc.) Imagine if you had to wait for your license to activate before you could compile your first program. Security - a symptom of accessibility - a large userbase means bugs don't get to linger around for long Fix it yourself - found a bug - fix it yourself","title":"Claims to Fame"},{"location":"computing_culture/on_open_source/#things-that-should-remain-closed-source","text":"Mainly things that are core to a company's business model - things that make a company competitive such as: MacOS windows manager - arguably the best I've ever used. I have no use for the source code, ie: I don't have a reason to port it the MacOS WM to another system, the WM has virtually no bugs, it does what I want 95% of the time. MSWord - I haven't used Word in ages(mostly LaTeX and MDown). But I don't think anyone is interested in Word's source. I also don't see a huge community lining up to make contributions to Word. If extensibility is the concern, developers can already add extensions to Word. And that's that.","title":"Things that should remain closed source"},{"location":"computing_culture/on_open_source/#reference-table","text":"Project Roots Verilator DEC. GCC MIT Clang Apple, Berkeley Univeristy Firefox Netscape, Google TypeScript Microsoft Linux Kernel RedHat, IBM, Google Chromium Google Android Google Blender NeoGeo Apache Hobbyist MatplotLib Hobbyist NumPy University Research","title":"Reference Table"},{"location":"fpga/","text":"Getting Started \u00b6 There are a few ways to get started with programming FPGAs. Programming FPGAs doesn't have to be expensive. There are a few open-source and open-hardware FPGA design options out there which will be considered here. Since I mostly do my programming on MacOS, this tutorial will reflect that. You can use the appropiate package manager commands for your platform. Toolchains \u00b6 Writing hardware for FPGAs is most easily facilitated with a workflow. Mine usually consist of an HDL simulator, an HDL synthesizer, and an FPGA specific toolchain. While many of the vendor specific FPGA softwares will provide an all in one solution, I usually find them rather clunky and slow. In addition, licenses are very expensive, which means that anyone lacking a license cannot contribute to or modify any of my designs. To solve this, I turn to open source. Lattice's entire ICE40 FPGA line, along with most of their ECP5 FPGA device line have been fully reverse engineered boasting up to 8000 and 90,000 LUTS respectively. The open source toolchain for the ICE40 is called ICESTORM, the open source toolchain for the ECP5 is called prjtrellis. Since prjtrellis proved particularly difficult to install, I will document that process here. Perequisites \u00b6 Visit software setup and make sure your system has a matching configuration. You will also need to install $brew install qt5 $brew install boost $brew install boost-python3 PrjTrellis \u00b6 Since I use a Mac, my tutorial will be focused on getting PrjTrellis up and running on MacOS and should be somewhat translatable to similar UNIX platforms. You'll want to clone the prjtrellis directory and follow the instructions in the README. I use brew to build all my packages. Steps \u00b6 A few things to watch out for that could trip out your installation. If you choose /usr as your install prefix, you may have to disable MacOS SIP. To get around this, just set /usr/local as your installation prefix. Make sure you can execute ''$python3'' mkdir build cmake -DCMAKE_INSTALL_PREFIX=/usr/local . make make install NextPNR \u00b6 cmake ../ -DARCH=ecp5 DTRELLIS_ROOT=/path/to/cloned/prjtrellis make -j($nproc)","title":"getting started"},{"location":"fpga/#getting-started","text":"There are a few ways to get started with programming FPGAs. Programming FPGAs doesn't have to be expensive. There are a few open-source and open-hardware FPGA design options out there which will be considered here. Since I mostly do my programming on MacOS, this tutorial will reflect that. You can use the appropiate package manager commands for your platform.","title":"Getting Started"},{"location":"fpga/#toolchains","text":"Writing hardware for FPGAs is most easily facilitated with a workflow. Mine usually consist of an HDL simulator, an HDL synthesizer, and an FPGA specific toolchain. While many of the vendor specific FPGA softwares will provide an all in one solution, I usually find them rather clunky and slow. In addition, licenses are very expensive, which means that anyone lacking a license cannot contribute to or modify any of my designs. To solve this, I turn to open source. Lattice's entire ICE40 FPGA line, along with most of their ECP5 FPGA device line have been fully reverse engineered boasting up to 8000 and 90,000 LUTS respectively. The open source toolchain for the ICE40 is called ICESTORM, the open source toolchain for the ECP5 is called prjtrellis. Since prjtrellis proved particularly difficult to install, I will document that process here.","title":"Toolchains"},{"location":"fpga/#perequisites","text":"Visit software setup and make sure your system has a matching configuration. You will also need to install $brew install qt5 $brew install boost $brew install boost-python3","title":"Perequisites"},{"location":"fpga/#prjtrellis","text":"Since I use a Mac, my tutorial will be focused on getting PrjTrellis up and running on MacOS and should be somewhat translatable to similar UNIX platforms. You'll want to clone the prjtrellis directory and follow the instructions in the README. I use brew to build all my packages.","title":"PrjTrellis"},{"location":"fpga/#steps","text":"A few things to watch out for that could trip out your installation. If you choose /usr as your install prefix, you may have to disable MacOS SIP. To get around this, just set /usr/local as your installation prefix. Make sure you can execute ''$python3'' mkdir build cmake -DCMAKE_INSTALL_PREFIX=/usr/local . make make install","title":"Steps"},{"location":"fpga/#nextpnr","text":"cmake ../ -DARCH=ecp5 DTRELLIS_ROOT=/path/to/cloned/prjtrellis make -j($nproc)","title":"NextPNR"},{"location":"fpga/hdl_wars/","text":"Verilog, VHDL, Chisel, Spinal, Migen, PyMtl and others \u00b6 There are many HDLs out there, but the two most important ones are Verilog and VHDL. These are supported by all ASCI design suites and FPGA synthesizer tools(some tools also support the synthesizeable subset of SystemVerilog). Some Analogies \u00b6 If boolean logic gates were assembly, then Verilog and VHDL would be like assembly with macros. PyMtl and SystemVerilog, would be like C. Chisel, SpinalHDL, Migen, nMigen would be C++ or perhaps Python. This is not an entirely accurate comparison(PyMtl and SystemVerilog are somewhat focused on delivering different levels of simulation fidelity depending on the needs), but provide an illustration of sorts. Problems with Verilog and VHDL \u00b6 The main shortcomings of Verilog and VHDL is that they lack any modern software language features, namely: OOP. The other languages listed are not supported by ASIC or FPGA tools, so this means these languages must compile down to Verilog or VHDL. Migen and PyMtl I believe compile down to Verilog only. What's Used in Industry \u00b6 HDL Used by Verilog Everybody at some point in the design process(for those who don\u2019t use VHDL) VHDL Mostly government - makes sense, VHDL was commissioned by DARPA System Verilog Common in industry for ASIC verification Chisel Google in their TPU, Berkeley Architecture Research(Invented here), popular in open source SpinalHDL Fork of ChiselV2, functionally similar to ChisalV3, popular in open source Migen Some European companies nMigen Brand new child of Migen. Not used much yet. BlueSpec HDL router design in industry PyMtl Mostly research - Cornell designed Celerity ASIC with it What I might use \u00b6 In the following pages, I discuss some strengths of migen and verilator(a verilog simulator). My general approach is to avoid Verilog and VHDL where possible as they are rather unpleasant for large designs. Now that the verilator backend for nMigen is nearly complete, I try to avoid Chisel and its Spinal fork for the following reasons. Chisel has excessive simulator boilerplate and no easy simulation support for multiple clock domains. Scala also has really slow SBT compile times and often requires doing the SBT package manager dance - contrast with Python's Pip which just works. I also find Python idiosms much more simple/useful/natural than Scala's I would also avoid VHDL altogether. VHDL has no fast simulators available.","title":"hdl wars"},{"location":"fpga/hdl_wars/#verilog-vhdl-chisel-spinal-migen-pymtl-and-others","text":"There are many HDLs out there, but the two most important ones are Verilog and VHDL. These are supported by all ASCI design suites and FPGA synthesizer tools(some tools also support the synthesizeable subset of SystemVerilog).","title":"Verilog, VHDL, Chisel, Spinal, Migen, PyMtl and others"},{"location":"fpga/hdl_wars/#some-analogies","text":"If boolean logic gates were assembly, then Verilog and VHDL would be like assembly with macros. PyMtl and SystemVerilog, would be like C. Chisel, SpinalHDL, Migen, nMigen would be C++ or perhaps Python. This is not an entirely accurate comparison(PyMtl and SystemVerilog are somewhat focused on delivering different levels of simulation fidelity depending on the needs), but provide an illustration of sorts.","title":"Some Analogies"},{"location":"fpga/hdl_wars/#problems-with-verilog-and-vhdl","text":"The main shortcomings of Verilog and VHDL is that they lack any modern software language features, namely: OOP. The other languages listed are not supported by ASIC or FPGA tools, so this means these languages must compile down to Verilog or VHDL. Migen and PyMtl I believe compile down to Verilog only.","title":"Problems with Verilog and VHDL"},{"location":"fpga/hdl_wars/#whats-used-in-industry","text":"HDL Used by Verilog Everybody at some point in the design process(for those who don\u2019t use VHDL) VHDL Mostly government - makes sense, VHDL was commissioned by DARPA System Verilog Common in industry for ASIC verification Chisel Google in their TPU, Berkeley Architecture Research(Invented here), popular in open source SpinalHDL Fork of ChiselV2, functionally similar to ChisalV3, popular in open source Migen Some European companies nMigen Brand new child of Migen. Not used much yet. BlueSpec HDL router design in industry PyMtl Mostly research - Cornell designed Celerity ASIC with it","title":"What's Used in Industry"},{"location":"fpga/hdl_wars/#what-i-might-use","text":"In the following pages, I discuss some strengths of migen and verilator(a verilog simulator). My general approach is to avoid Verilog and VHDL where possible as they are rather unpleasant for large designs. Now that the verilator backend for nMigen is nearly complete, I try to avoid Chisel and its Spinal fork for the following reasons. Chisel has excessive simulator boilerplate and no easy simulation support for multiple clock domains. Scala also has really slow SBT compile times and often requires doing the SBT package manager dance - contrast with Python's Pip which just works. I also find Python idiosms much more simple/useful/natural than Scala's I would also avoid VHDL altogether. VHDL has no fast simulators available.","title":"What I might use"},{"location":"fpga/migen/ethernet_ecp5/","text":"Purpose \u00b6 This tutorial is intended to help gently introduce the reader to using LiteX+Migen on supported FPGAs with a simple UDP loopback over ethernet RJ45 example. A working understanding of Migen is required. At the end of this tutorial, the FPGA will form a UDP loopback echoing back UDP messages to the sender IP address and sender UDP port. Resources \u00b6 Here are some resources for getting up to speed on Migen. Lambda Concepts Tutorial Github Migen Practical Intro Migen Manual Code \u00b6 First grab the code from here . $./udp.py build #this programs the FPGA $./udp.py load Configuring the Ethernet \u00b6 Your machine will obviously need to have an ethernet RJ45 jack. On my Mac, this usually means plugging in a Thunderbolt dongle. Connected to Internet via RJ45 \u00b6 If you're connected to the internet via your RJ45 ethernet, you'll either have to disconnect from internet or plug both your FPGA and PC into an external router. The router approach is subtly more difficult as it will be necessary to manually configure the router's ARP table as this example comes with no ARP hardware. Connected to Internet via Wi-Fi \u00b6 It is also possible you may be connected to internet via Wi-Fi. If this is the case, run ifconfig and verify that the most significant byte of the IP assigned to your ethernet interface differs from that assigned to your Wi-Fi interface. In this example, the FPGA is programmed to respond to the IP of 169.253.2.50 , so the ethernet interface from which the UDP packet destined for the FPGA originates must be on the same subnet. The IP address selected for the interface in this example is 169.253.2.100 . Configuring the ARP table \u00b6 Whether or not you are connected to the internet, you will have to modify your ARP table so that the kernel knows where to route requests to the IP address of 169.253.2.50 . It is possible to add ARP support to the hardware via the LiteX Liteeth ARP module, but that is not included in this example, so it is necessary to manually modify the ARP table on your machine. On my Mac, I am connected to internet via Wi-Fi while the FPGA is plugged into Thunderbolt RJ45 ethernet. The Thunderbolt ethernet interface is listed as en7 according to ifconfig , so I set the IP and subnet of en7 accordingly. The following command syntax works on BSD as well as Linux. $ifconfig en7 169.253.2.100 netmask 255.255.255.0 Next, I configure the ARP table where 10:e2:d5:00:00:00 is the MAC address of the FPGA. FreeBSD/MacOS \u00b6 $arp -s 169.253.2.50 10:e2:d5:00:00:00 ifscope en7 Linux \u00b6 $arp -s 169.253.2.50 10:e2:d5:00:00:00 -iface en7 Sending and Receiving Packets \u00b6 You should now be able to send and receive UDP packets. $python3 listener.py & $python3 sender.py If everything is working correctly, you should see recieved message:some message as the last line of output. Debugging \u00b6 Wireshark is a good tool that should be in every engineer's toolbox. Select the interface that you want to monitor in wireshark to help with debugging potential problems. As you can see below, the FPGA receives the packet and then queries to determine the physical ethernet address the IP packet originated from.","title":"ethernet on ECP5"},{"location":"fpga/migen/ethernet_ecp5/#purpose","text":"This tutorial is intended to help gently introduce the reader to using LiteX+Migen on supported FPGAs with a simple UDP loopback over ethernet RJ45 example. A working understanding of Migen is required. At the end of this tutorial, the FPGA will form a UDP loopback echoing back UDP messages to the sender IP address and sender UDP port.","title":"Purpose"},{"location":"fpga/migen/ethernet_ecp5/#resources","text":"Here are some resources for getting up to speed on Migen. Lambda Concepts Tutorial Github Migen Practical Intro Migen Manual","title":"Resources"},{"location":"fpga/migen/ethernet_ecp5/#code","text":"First grab the code from here . $./udp.py build #this programs the FPGA $./udp.py load","title":"Code"},{"location":"fpga/migen/ethernet_ecp5/#configuring-the-ethernet","text":"Your machine will obviously need to have an ethernet RJ45 jack. On my Mac, this usually means plugging in a Thunderbolt dongle.","title":"Configuring the Ethernet"},{"location":"fpga/migen/ethernet_ecp5/#connected-to-internet-via-rj45","text":"If you're connected to the internet via your RJ45 ethernet, you'll either have to disconnect from internet or plug both your FPGA and PC into an external router. The router approach is subtly more difficult as it will be necessary to manually configure the router's ARP table as this example comes with no ARP hardware.","title":"Connected to Internet via RJ45"},{"location":"fpga/migen/ethernet_ecp5/#connected-to-internet-via-wi-fi","text":"It is also possible you may be connected to internet via Wi-Fi. If this is the case, run ifconfig and verify that the most significant byte of the IP assigned to your ethernet interface differs from that assigned to your Wi-Fi interface. In this example, the FPGA is programmed to respond to the IP of 169.253.2.50 , so the ethernet interface from which the UDP packet destined for the FPGA originates must be on the same subnet. The IP address selected for the interface in this example is 169.253.2.100 .","title":"Connected to Internet via Wi-Fi"},{"location":"fpga/migen/ethernet_ecp5/#configuring-the-arp-table","text":"Whether or not you are connected to the internet, you will have to modify your ARP table so that the kernel knows where to route requests to the IP address of 169.253.2.50 . It is possible to add ARP support to the hardware via the LiteX Liteeth ARP module, but that is not included in this example, so it is necessary to manually modify the ARP table on your machine. On my Mac, I am connected to internet via Wi-Fi while the FPGA is plugged into Thunderbolt RJ45 ethernet. The Thunderbolt ethernet interface is listed as en7 according to ifconfig , so I set the IP and subnet of en7 accordingly. The following command syntax works on BSD as well as Linux. $ifconfig en7 169.253.2.100 netmask 255.255.255.0 Next, I configure the ARP table where 10:e2:d5:00:00:00 is the MAC address of the FPGA.","title":"Configuring the ARP table"},{"location":"fpga/migen/ethernet_ecp5/#freebsdmacos","text":"$arp -s 169.253.2.50 10:e2:d5:00:00:00 ifscope en7","title":"FreeBSD/MacOS"},{"location":"fpga/migen/ethernet_ecp5/#linux","text":"$arp -s 169.253.2.50 10:e2:d5:00:00:00 -iface en7","title":"Linux"},{"location":"fpga/migen/ethernet_ecp5/#sending-and-receiving-packets","text":"You should now be able to send and receive UDP packets. $python3 listener.py & $python3 sender.py If everything is working correctly, you should see recieved message:some message as the last line of output.","title":"Sending and Receiving Packets"},{"location":"fpga/migen/ethernet_ecp5/#debugging","text":"Wireshark is a good tool that should be in every engineer's toolbox. Select the interface that you want to monitor in wireshark to help with debugging potential problems. As you can see below, the FPGA receives the packet and then queries to determine the physical ethernet address the IP packet originated from.","title":"Debugging"},{"location":"fpga/migen/why_migen/","text":"Introduction \u00b6 Migen is a high level hardware description language(not to be confused with HLS - high level synthesis language). Much like Chisel and SpinalHDL, Migen allows the hardware designer to use modern language features such as OOP to describe hardware. Instantiating a bus between two devices in hardware can be as simple as passign the device to an instantiation of the bus class. Whereas Chisel and Spinal are both written in Scala, Migen is written in Python3. Advantages \u00b6 The primary advantage Migen offers is SOC design. Many complete HDL device libraries are availble and written in Migen. Some devices include, Ethernet with ARP/UDP, HDMI, and DRAM controller just to name a few. Migen+LiteX = builtin support for common Xilinx FPGAs as well as the FOSS toolchains such as yosys+nextpnr for Lattice FPGAs. Instantiating Ethernet on the ECP5 FPGA can be a simple as importing LiteX and migen and then calling build() which will generate the bitstream for the repsective ECP5. All the Python perks are availabe such as dictionaries, lists, list comprehension, etc. Migen is Python, and Python code just looks clean. Disadvantages \u00b6 The primary disadvantage of Migen is that simulation is slow. Also, since Python3 never targeted multithreaded support, it is basically impossible to simulate Migen on more than one core. The way I might use Migen is to add SOC support after I've written a component such as a CPU or ML accelerator in SpinalHDL or Chisel. Migen makes it easy to add and subsequently simulate HDMI video output(for example) to your already existing project. For design components that are simulation intensive, I would use Chisel or Spinal. A new iteration of Migen called nMigen attempts to solve Migen's shortcomings. In particular, nMigen allows generation of modular verilog code and should in the future add verilator backend support. Upon verilator integration, I would recommend nMigen for simulation intense component design.","title":"why migen"},{"location":"fpga/migen/why_migen/#introduction","text":"Migen is a high level hardware description language(not to be confused with HLS - high level synthesis language). Much like Chisel and SpinalHDL, Migen allows the hardware designer to use modern language features such as OOP to describe hardware. Instantiating a bus between two devices in hardware can be as simple as passign the device to an instantiation of the bus class. Whereas Chisel and Spinal are both written in Scala, Migen is written in Python3.","title":"Introduction"},{"location":"fpga/migen/why_migen/#advantages","text":"The primary advantage Migen offers is SOC design. Many complete HDL device libraries are availble and written in Migen. Some devices include, Ethernet with ARP/UDP, HDMI, and DRAM controller just to name a few. Migen+LiteX = builtin support for common Xilinx FPGAs as well as the FOSS toolchains such as yosys+nextpnr for Lattice FPGAs. Instantiating Ethernet on the ECP5 FPGA can be a simple as importing LiteX and migen and then calling build() which will generate the bitstream for the repsective ECP5. All the Python perks are availabe such as dictionaries, lists, list comprehension, etc. Migen is Python, and Python code just looks clean.","title":"Advantages"},{"location":"fpga/migen/why_migen/#disadvantages","text":"The primary disadvantage of Migen is that simulation is slow. Also, since Python3 never targeted multithreaded support, it is basically impossible to simulate Migen on more than one core. The way I might use Migen is to add SOC support after I've written a component such as a CPU or ML accelerator in SpinalHDL or Chisel. Migen makes it easy to add and subsequently simulate HDMI video output(for example) to your already existing project. For design components that are simulation intensive, I would use Chisel or Spinal. A new iteration of Migen called nMigen attempts to solve Migen's shortcomings. In particular, nMigen allows generation of modular verilog code and should in the future add verilator backend support. Upon verilator integration, I would recommend nMigen for simulation intense component design.","title":"Disadvantages"},{"location":"fpga/verilator/dpi/","text":"Adding Verilator as a nMigen-HDL Python Simulator Backend \u00b6 Recently, I've been using Migen and nMigen to build hardware for both Nerual Network accelerators and toy GPUs. HDL simulation in both nMigen and Migen currently occurs in pure Python, which is way too slow for simulating operations of high arithmetic intensity such as matrix multiplies. Verilator is the fastest verilog simulator on the market and is also open-source, making it a perfect candidate for an Migen or nMigen backend. I was originally considering implmenting the verilator backend for Migen, but since nMigen improves on the mistakes of Migen(especially in the of clearly defining what is idiomatic nMigen and what is not, I decided to implment the verilator backend for nMigen. One particular advantage of nMigen over Migen is that it retains the hierarchical structure of the HDL AST when emitting verilog. In simulation, nMigen exposes all ports of the top module and submodules. Verilator however does not natively expose its the internals of submodules. Rolling the verilator backend required the following steps: Compile nMigen top module into verilog Export the relevant ports that the default nMigen simulator would have access too. Generate a wrapper module in verilog that declares DPI setter and getter methods for the nMigen signals of interest. Verilate verilog model and compile verilated into a shared object loaded by Python. It took quite a while for me to figure out how to implment the DPI methods, so I have pasted what I did below. More information on this here . Implmenting DPI \u00b6 rm -rf obj_dir/ verilator --cc --exe test_wrap.v test.v test.cpp make -C obj_dir/ -f Vtest_wrap.mk ./obj_dir/Vtest_wrap test.v \u00b6 module t (input [71:0] a, output [71:0] o); adder add (.a(a), .b(), .o(o)); endmodule module adder (input [71:0] a, input [71:0] b, output [71:0] o); assign o = a + b; endmodule test_wrap.cpp \u00b6 module t_wrap (input [71:0] a, output [71:0] o); t dut (.a(a), .o(o)); export \"DPI-C\" function read_a; function void read_a(output bit [71:0] result); result = dut.add.a; endfunction export \"DPI-C\" function write_a; function void write_a(input bit [71:0] in); dut.add.a = in; endfunction endmodule test.v \u00b6 #include \"Vtest_wrap.h\" #include <cstdio> extern void write_b(char); void print_by_int(uint32_t* val){ printf(\"print_by_int\\n\"); for(int i = 0; i < 3; i++) { printf(\"%d: \",i); printf(\"%2x\\n\",val[i]); } } int main(int argc, char** argv) { Vtest_wrap top; svSetScope(svGetScopeFromName(\"TOP.t_wrap\")); //set and read top.a using verilator public method top.a[0] = 1; top.a[1] = 2; top.a[2] = 3; top.eval(); printf(\"from verilator \"); print_by_int(top.a); //set top.a using DPI and read with DPI uint32_t ret[] = {0,0,128}; top.write_a((svBitVecVal *)&ret); printf(\"from DPI \"); print_by_int(ret); //read top.a with verilator public method - should //match output of DPI printf(\"from verilator \"); print_by_int(top.a); top.final(); return 0; } output \u00b6 from verilator print_by_int 0: 1 1: 2 2: 3 from DPI print_by_int 0: 0 1: 0 2: 80 from verilator print_by_int 0: 0 1: 0 2: 80","title":"verilator as a nMigen Python backend"},{"location":"fpga/verilator/dpi/#adding-verilator-as-a-nmigen-hdl-python-simulator-backend","text":"Recently, I've been using Migen and nMigen to build hardware for both Nerual Network accelerators and toy GPUs. HDL simulation in both nMigen and Migen currently occurs in pure Python, which is way too slow for simulating operations of high arithmetic intensity such as matrix multiplies. Verilator is the fastest verilog simulator on the market and is also open-source, making it a perfect candidate for an Migen or nMigen backend. I was originally considering implmenting the verilator backend for Migen, but since nMigen improves on the mistakes of Migen(especially in the of clearly defining what is idiomatic nMigen and what is not, I decided to implment the verilator backend for nMigen. One particular advantage of nMigen over Migen is that it retains the hierarchical structure of the HDL AST when emitting verilog. In simulation, nMigen exposes all ports of the top module and submodules. Verilator however does not natively expose its the internals of submodules. Rolling the verilator backend required the following steps: Compile nMigen top module into verilog Export the relevant ports that the default nMigen simulator would have access too. Generate a wrapper module in verilog that declares DPI setter and getter methods for the nMigen signals of interest. Verilate verilog model and compile verilated into a shared object loaded by Python. It took quite a while for me to figure out how to implment the DPI methods, so I have pasted what I did below. More information on this here .","title":"Adding Verilator as a nMigen-HDL Python Simulator Backend"},{"location":"fpga/verilator/dpi/#implmenting-dpi","text":"rm -rf obj_dir/ verilator --cc --exe test_wrap.v test.v test.cpp make -C obj_dir/ -f Vtest_wrap.mk ./obj_dir/Vtest_wrap","title":"Implmenting DPI"},{"location":"fpga/verilator/dpi/#testv","text":"module t (input [71:0] a, output [71:0] o); adder add (.a(a), .b(), .o(o)); endmodule module adder (input [71:0] a, input [71:0] b, output [71:0] o); assign o = a + b; endmodule","title":"test.v"},{"location":"fpga/verilator/dpi/#test_wrapcpp","text":"module t_wrap (input [71:0] a, output [71:0] o); t dut (.a(a), .o(o)); export \"DPI-C\" function read_a; function void read_a(output bit [71:0] result); result = dut.add.a; endfunction export \"DPI-C\" function write_a; function void write_a(input bit [71:0] in); dut.add.a = in; endfunction endmodule","title":"test_wrap.cpp"},{"location":"fpga/verilator/dpi/#testv_1","text":"#include \"Vtest_wrap.h\" #include <cstdio> extern void write_b(char); void print_by_int(uint32_t* val){ printf(\"print_by_int\\n\"); for(int i = 0; i < 3; i++) { printf(\"%d: \",i); printf(\"%2x\\n\",val[i]); } } int main(int argc, char** argv) { Vtest_wrap top; svSetScope(svGetScopeFromName(\"TOP.t_wrap\")); //set and read top.a using verilator public method top.a[0] = 1; top.a[1] = 2; top.a[2] = 3; top.eval(); printf(\"from verilator \"); print_by_int(top.a); //set top.a using DPI and read with DPI uint32_t ret[] = {0,0,128}; top.write_a((svBitVecVal *)&ret); printf(\"from DPI \"); print_by_int(ret); //read top.a with verilator public method - should //match output of DPI printf(\"from verilator \"); print_by_int(top.a); top.final(); return 0; }","title":"test.v"},{"location":"fpga/verilator/dpi/#output","text":"from verilator print_by_int 0: 1 1: 2 2: 3 from DPI print_by_int 0: 0 1: 0 2: 80 from verilator print_by_int 0: 0 1: 0 2: 80","title":"output"},{"location":"fpga/verilator/some_notes_on_verilator/","text":"Ports 65 bits wide or greater are represented as arrays of uint_32 NOT uint_64! Notice that the next level down IS represented as uint_64, and after that, back to uint_32. Ports between 33 and 64 bits wide inclusive are represented as uint_64. Ports between 16 and 32 bits wide inclusive are represented as uint_32. Ports between 2 and 16 bits wide inclusive are represented as uint_16. I believe 1 bit ports are represent as bool.","title":"some notes on verilator"},{"location":"fpga/verilator/why_verilator/","text":"Simulating Verilog the Right Way \u00b6 Simulating verilog in an effective manner can be a challenging task. The verilog language provides a few structures for allowing the HDL designer to describe a small testbench effectively - but is rather inadequate for complex designs. Such shortcomings become extremely painful in large complex design such as verifying and pipelining and Out of Order CPU. Verilator to the Rescue \u00b6 Verilator allows you to compile your Verilog source into C++ code. Suddenly, anything that you can do with C++/C, you can do with your RTL. For example, if you have RTL that has video output, you can use verilator to display that video output within your windowing system via the X Windowing System for example. Also, the last time I checked, verilator was the fastest verilog simulator on the market. Simulating with Verilator \u00b6 Simulating with verilator directly by writing C++ can quickly become tiring for large designs. While C++ is a much better language for describing simulations, describing multi port hardware accesses with C++ syntax is a bit unpleasant. To help combat this, it is possible to use verilator to simulate hardware written in Chisel HDL or Spinal HDL as verilator is integrated as a simulation backend in both Spinal and Chisel HDL. Building and Running Your Testbench \u00b6 While you can write your testbench in Chisel or Spinal and invoke verilator as a backend(the easier way of doing things), you may have verilog source code you have already written that you wish to test. Dan Gisselquist has a good tutorial on how to use verilator. Compiling with verilator has changed a little since his tutorial was written, in particular, to build your verilog into C++, you will probably need to do the following: #this compiles your verilog into C++ #main.cpp contains your testbench in C++ $verilator -Wall --cc top.v submodule1.v submodule2.v submoduleN.v --exe main.cpp --Mdir /build --top-module top -Wno-fatal where top is the name of the top module in your verilog source. Verilator will automatically generate a makefile for you too. Invoking this makefile will build an executable that will simulate your design. You can also have verilator spit out a VCD. To build your executable, you do something along the lines of: $make -j -C /build -f Vtop.mk Vtop Example Testbench \u00b6 Here is a testbench i wrote a while back for verilator. The complete source is not shown below, but just enough to give you an idea of how verilator works. For this particular testbench, I was simulating a RISCV core that printed its status to a UART device. Snippet from main.cpp \u00b6 //here we do setup template <typename module> void testbench<module>::setup(void){ //setup inputs m_core->clk = 0; m_core->rx = 0; m_core->eval(); m_core->clk = 1; m_core->eval(); m_core->clk = 0; m_core->eval(); // Tick the clock until we are done int OLD_LED = m_core->led; for(int i = 0; i < 500000; i++){ m_core->clk = 0; m_core->eval(); m_core->clk = 1; m_core->eval(); if(m_core->tx_byte_valid){ //printf(\"%s\",(char)m_core->tx_byte); putc(m_core->tx_byte, stdout); } if(m_core->led != OLD_LED) printf(\"LED VALUE: %d\\n\", (char)m_core->led); OLD_LED = m_core->led; } } template <typename module> void testbench<module>::do_transmit(void){ } void delay(testbench<Vtop> *tb){ for(int i = 0; i < 1250; i++) tb->tick(); } int main(int argc, char **argv){ // initialize verilators variables Verilated::commandArgs(argc, argv); // create an instance of our module under test testbench<Vtop> *tb = new testbench<Vtop>(); //toggle transmit informing the UART core we are //ready to transmit #ifdef TRACE_ON //we've reached a condition of interest in the simulation //we open the trace file to start recording tb->opentrace(\"trace.vcd\"); printf(\"tracing\\n\"); #endif printf(\"BEGIN SIM\\n\"); tb->setup(); printf(\"\\nEND SIM\\n\"); #ifdef TRACE_ON tb->close(); #endif }","title":"why verilator"},{"location":"fpga/verilator/why_verilator/#simulating-verilog-the-right-way","text":"Simulating verilog in an effective manner can be a challenging task. The verilog language provides a few structures for allowing the HDL designer to describe a small testbench effectively - but is rather inadequate for complex designs. Such shortcomings become extremely painful in large complex design such as verifying and pipelining and Out of Order CPU.","title":"Simulating Verilog the Right Way"},{"location":"fpga/verilator/why_verilator/#verilator-to-the-rescue","text":"Verilator allows you to compile your Verilog source into C++ code. Suddenly, anything that you can do with C++/C, you can do with your RTL. For example, if you have RTL that has video output, you can use verilator to display that video output within your windowing system via the X Windowing System for example. Also, the last time I checked, verilator was the fastest verilog simulator on the market.","title":"Verilator to the Rescue"},{"location":"fpga/verilator/why_verilator/#simulating-with-verilator","text":"Simulating with verilator directly by writing C++ can quickly become tiring for large designs. While C++ is a much better language for describing simulations, describing multi port hardware accesses with C++ syntax is a bit unpleasant. To help combat this, it is possible to use verilator to simulate hardware written in Chisel HDL or Spinal HDL as verilator is integrated as a simulation backend in both Spinal and Chisel HDL.","title":"Simulating with Verilator"},{"location":"fpga/verilator/why_verilator/#building-and-running-your-testbench","text":"While you can write your testbench in Chisel or Spinal and invoke verilator as a backend(the easier way of doing things), you may have verilog source code you have already written that you wish to test. Dan Gisselquist has a good tutorial on how to use verilator. Compiling with verilator has changed a little since his tutorial was written, in particular, to build your verilog into C++, you will probably need to do the following: #this compiles your verilog into C++ #main.cpp contains your testbench in C++ $verilator -Wall --cc top.v submodule1.v submodule2.v submoduleN.v --exe main.cpp --Mdir /build --top-module top -Wno-fatal where top is the name of the top module in your verilog source. Verilator will automatically generate a makefile for you too. Invoking this makefile will build an executable that will simulate your design. You can also have verilator spit out a VCD. To build your executable, you do something along the lines of: $make -j -C /build -f Vtop.mk Vtop","title":"Building and Running Your Testbench"},{"location":"fpga/verilator/why_verilator/#example-testbench","text":"Here is a testbench i wrote a while back for verilator. The complete source is not shown below, but just enough to give you an idea of how verilator works. For this particular testbench, I was simulating a RISCV core that printed its status to a UART device.","title":"Example Testbench"},{"location":"fpga/verilator/why_verilator/#snippet-from-maincpp","text":"//here we do setup template <typename module> void testbench<module>::setup(void){ //setup inputs m_core->clk = 0; m_core->rx = 0; m_core->eval(); m_core->clk = 1; m_core->eval(); m_core->clk = 0; m_core->eval(); // Tick the clock until we are done int OLD_LED = m_core->led; for(int i = 0; i < 500000; i++){ m_core->clk = 0; m_core->eval(); m_core->clk = 1; m_core->eval(); if(m_core->tx_byte_valid){ //printf(\"%s\",(char)m_core->tx_byte); putc(m_core->tx_byte, stdout); } if(m_core->led != OLD_LED) printf(\"LED VALUE: %d\\n\", (char)m_core->led); OLD_LED = m_core->led; } } template <typename module> void testbench<module>::do_transmit(void){ } void delay(testbench<Vtop> *tb){ for(int i = 0; i < 1250; i++) tb->tick(); } int main(int argc, char **argv){ // initialize verilators variables Verilated::commandArgs(argc, argv); // create an instance of our module under test testbench<Vtop> *tb = new testbench<Vtop>(); //toggle transmit informing the UART core we are //ready to transmit #ifdef TRACE_ON //we've reached a condition of interest in the simulation //we open the trace file to start recording tb->opentrace(\"trace.vcd\"); printf(\"tracing\\n\"); #endif printf(\"BEGIN SIM\\n\"); tb->setup(); printf(\"\\nEND SIM\\n\"); #ifdef TRACE_ON tb->close(); #endif }","title":"Snippet from main.cpp"},{"location":"fpga/vhdl/getting_started_with_vhdl/","text":"Prologue \u00b6 Unless you absolutely have to, generally, verilog is the preferred language by both hobbyists and those in industry alike. More info on that here . This tutorial will teach you how to simulate a VHDL design I wrote that displays images on a MacSE screen. You will also be able verify the design works using a VHDL testbench, and preview the image results in Python. First Steps \u00b6 A Hardware Description Language(HDL) is purely that. It merely describes hardware. There are a couple types of HDLs, digital and mixed(digital+analog). This tutorial deals with digital HDLs. HDLs aren't very useful by themselves. Typically, they might be passed to a synthesizer which reduces the HDL into a gate list with that describes all the connections between various gates more commonly known as a netlist. There are different kinds of synthesizers. Some synthesizer frameworks target FPGAs while others target physical fabrication or VLSI. In VLSI, HDL is just the first step in a long laborious sequence of tasks. After writing some HDL, you may wish to know whether or not it does what you want. This can be accomplished using an HDL simulator. Currently, the only Free and Open Source(FOSS) simulator being maintained is [GHDL]. Installing GHDL \u00b6 Ubuntu \u00b6 sudo apt update sudo apt install build-essential gnat git llvm clang zlib1g-dev mkdir -p ~/src; cd ~/src git clone https://github.com/ghdl/ghdl cd ghdl mkdir build; cd build ../configure --with-llvm-config --prefix=/usr/local make -j8 make install MacOS \u00b6 mkdir -p ~/src; cd ~/src wget https://github.com/ghdl/ghdl/releases/download/v0.36/ghdl-0.36-macosx-llvm.tgz mkdir -p ghdl tar -C ghdl -xf ghdl-0.36-macosx-llvm.tgz rm ghdl-0.36-macosx-llvm.tgz cd ghdl/bin echo \"export PATH=\\\"`pwd`:\\$PATH\\\"\" >> ~/.bash_profile My workflow consists of this, - Invoke Make - Refresh Waveform in GTKWave All this can be done in a quarter of a second for modest designs such as a pipeline CPU - also nice is that a properly written makefile only requires GHDL to re-evaluate changed components. Here is the source on GitHub https://github.com/BracketMaster/quick-ghdl-toolchain Simulating With GHDL \u00b6 You will probably want a makefile to automate your simulations. One is provided below. Literally name your makefile \"makefile\". No extension necessary. This should be placed in the same directory that contains all the vhdl source. TIME=500ns GHDL = ghdl GHDL_SIM_OPT = --stop-time=$(TIME) GHDL_FLAGS = --ieee=synopsys -fexplicit WORKDIR = Simulate TOP_ENTITY = spim_pipe echoPath = .bash_profile .PHONY : all compile run all : ./Simulate compile run ./Simulate : mkdir -p $(WORKDIR) $(GHDL) -i --workdir=$(WORKDIR) *.vhd compile : $(GHDL) -m $(GHDL_FLAGS) --workdir=$(WORKDIR)/ $(TOP_ENTITY) run : $(GHDL) -r --workdir=$(WORKDIR) $(TOP_ENTITY) --ieee-asserts=disable --stop-time=$(TIME) --wave=$(TOP_ENTITY).ghw --vcd=$(TOP_ENTITY).vcd clean : rm -rf Simulate rm -f $(TOP_ENTITY).ghw rm -f $(TOP_ENTITY).vcd rm -f e~$(TOP_ENTITY).o install-OSX : tar -xzvf ./gtkwave.app.tar sudo cp -rf ./gtkwave.app /Applications/ mkdir -p ~/sources tar -xzvf ./GHDL.tar cp -rf ./GHDL ~/sources printf \"export PATH=\\\"$$(echo ~)/sources/GHDL/bin/:\\$${PATH}\\\"\\n\" >> ~/$(echoPath) source ~/.bash_profile test : printf \"export PATH=\\\"$$(echo ~)/sources/GHDL/bin/:\\$${PATH}\\\"\\n\" >> ~/$(echoPath) Software like Modelsim gives you the option to force a signal high and low as a clock and to hold the reset at the beginning of the simulation. Providing inputs to your HDL model is called a testbench. You may need to write a testbench for your code, if one hasn't been provided already. To determine whether or not your codeset may already have a testbench, you can perform a recursive search over all your vhdl files in the terminal for the ''after'' vhdl keyword which is usually only included is tesbenches because it instructs the simulator to toggle the input after a certain amount of time. Change into the directory containing the vhdl source ''grep -R \"after\" ./'' You should see some results perhaps like this. [macbookPro PS_SPIM_base Original]$ grep -R \"after\" ./ .//ps_clock.vhd: sys_clock <= '0', '1' after 50 ns; .//ps_clock.vhd:reset <= '1', '0' after 75 ns; Binary file .//spim_pipe matches In this case, the file ''ps_clock.vhd'' is the testbench. If you are missing a testbench, an example testbench is provided below. LIBRARY IEEE; USE IEEE.STD_LOGIC_1164.ALL; USE IEEE.STD_LOGIC_ARITH.ALL; USE IEEE.STD_LOGIC_UNSIGNED.ALL; use std.textio.all; entity my_clock is port ( signal sys_clock, reset : out std_logic); end entity my_clock; architecture behavior of my_clock is begin process begin -- generate clock sys_clock <= '0', '1' after 50 ns; wait for 100 ns; end process; -- following statement executes only once reset <= '1', '0' after 75 ns; end architecture behavior; Finally change into the directory of your makefile and in the terminal and type ''make''. You might see something like this once GDHL compiles for the first time","title":"FOSS intro to VHDL"},{"location":"fpga/vhdl/getting_started_with_vhdl/#prologue","text":"Unless you absolutely have to, generally, verilog is the preferred language by both hobbyists and those in industry alike. More info on that here . This tutorial will teach you how to simulate a VHDL design I wrote that displays images on a MacSE screen. You will also be able verify the design works using a VHDL testbench, and preview the image results in Python.","title":"Prologue"},{"location":"fpga/vhdl/getting_started_with_vhdl/#first-steps","text":"A Hardware Description Language(HDL) is purely that. It merely describes hardware. There are a couple types of HDLs, digital and mixed(digital+analog). This tutorial deals with digital HDLs. HDLs aren't very useful by themselves. Typically, they might be passed to a synthesizer which reduces the HDL into a gate list with that describes all the connections between various gates more commonly known as a netlist. There are different kinds of synthesizers. Some synthesizer frameworks target FPGAs while others target physical fabrication or VLSI. In VLSI, HDL is just the first step in a long laborious sequence of tasks. After writing some HDL, you may wish to know whether or not it does what you want. This can be accomplished using an HDL simulator. Currently, the only Free and Open Source(FOSS) simulator being maintained is [GHDL].","title":"First Steps"},{"location":"fpga/vhdl/getting_started_with_vhdl/#installing-ghdl","text":"","title":"Installing GHDL"},{"location":"fpga/vhdl/getting_started_with_vhdl/#ubuntu","text":"sudo apt update sudo apt install build-essential gnat git llvm clang zlib1g-dev mkdir -p ~/src; cd ~/src git clone https://github.com/ghdl/ghdl cd ghdl mkdir build; cd build ../configure --with-llvm-config --prefix=/usr/local make -j8 make install","title":"Ubuntu"},{"location":"fpga/vhdl/getting_started_with_vhdl/#macos","text":"mkdir -p ~/src; cd ~/src wget https://github.com/ghdl/ghdl/releases/download/v0.36/ghdl-0.36-macosx-llvm.tgz mkdir -p ghdl tar -C ghdl -xf ghdl-0.36-macosx-llvm.tgz rm ghdl-0.36-macosx-llvm.tgz cd ghdl/bin echo \"export PATH=\\\"`pwd`:\\$PATH\\\"\" >> ~/.bash_profile My workflow consists of this, - Invoke Make - Refresh Waveform in GTKWave All this can be done in a quarter of a second for modest designs such as a pipeline CPU - also nice is that a properly written makefile only requires GHDL to re-evaluate changed components. Here is the source on GitHub https://github.com/BracketMaster/quick-ghdl-toolchain","title":"MacOS"},{"location":"fpga/vhdl/getting_started_with_vhdl/#simulating-with-ghdl","text":"You will probably want a makefile to automate your simulations. One is provided below. Literally name your makefile \"makefile\". No extension necessary. This should be placed in the same directory that contains all the vhdl source. TIME=500ns GHDL = ghdl GHDL_SIM_OPT = --stop-time=$(TIME) GHDL_FLAGS = --ieee=synopsys -fexplicit WORKDIR = Simulate TOP_ENTITY = spim_pipe echoPath = .bash_profile .PHONY : all compile run all : ./Simulate compile run ./Simulate : mkdir -p $(WORKDIR) $(GHDL) -i --workdir=$(WORKDIR) *.vhd compile : $(GHDL) -m $(GHDL_FLAGS) --workdir=$(WORKDIR)/ $(TOP_ENTITY) run : $(GHDL) -r --workdir=$(WORKDIR) $(TOP_ENTITY) --ieee-asserts=disable --stop-time=$(TIME) --wave=$(TOP_ENTITY).ghw --vcd=$(TOP_ENTITY).vcd clean : rm -rf Simulate rm -f $(TOP_ENTITY).ghw rm -f $(TOP_ENTITY).vcd rm -f e~$(TOP_ENTITY).o install-OSX : tar -xzvf ./gtkwave.app.tar sudo cp -rf ./gtkwave.app /Applications/ mkdir -p ~/sources tar -xzvf ./GHDL.tar cp -rf ./GHDL ~/sources printf \"export PATH=\\\"$$(echo ~)/sources/GHDL/bin/:\\$${PATH}\\\"\\n\" >> ~/$(echoPath) source ~/.bash_profile test : printf \"export PATH=\\\"$$(echo ~)/sources/GHDL/bin/:\\$${PATH}\\\"\\n\" >> ~/$(echoPath) Software like Modelsim gives you the option to force a signal high and low as a clock and to hold the reset at the beginning of the simulation. Providing inputs to your HDL model is called a testbench. You may need to write a testbench for your code, if one hasn't been provided already. To determine whether or not your codeset may already have a testbench, you can perform a recursive search over all your vhdl files in the terminal for the ''after'' vhdl keyword which is usually only included is tesbenches because it instructs the simulator to toggle the input after a certain amount of time. Change into the directory containing the vhdl source ''grep -R \"after\" ./'' You should see some results perhaps like this. [macbookPro PS_SPIM_base Original]$ grep -R \"after\" ./ .//ps_clock.vhd: sys_clock <= '0', '1' after 50 ns; .//ps_clock.vhd:reset <= '1', '0' after 75 ns; Binary file .//spim_pipe matches In this case, the file ''ps_clock.vhd'' is the testbench. If you are missing a testbench, an example testbench is provided below. LIBRARY IEEE; USE IEEE.STD_LOGIC_1164.ALL; USE IEEE.STD_LOGIC_ARITH.ALL; USE IEEE.STD_LOGIC_UNSIGNED.ALL; use std.textio.all; entity my_clock is port ( signal sys_clock, reset : out std_logic); end entity my_clock; architecture behavior of my_clock is begin process begin -- generate clock sys_clock <= '0', '1' after 50 ns; wait for 100 ns; end process; -- following statement executes only once reset <= '1', '0' after 75 ns; end architecture behavior; Finally change into the directory of your makefile and in the terminal and type ''make''. You might see something like this once GDHL compiles for the first time","title":"Simulating With GHDL"},{"location":"mathematics/loss_function_gradient_derivation/","text":"Gradients on Loss Functions \u00b6 The formulation for the gradient on the loss function with respect to the weights arises a lot in machine learning as the following: Consider a score matrix S S , a weight matrix W W , an input matrix X X of compatible dimensions, and a scalar-valued loss function L(S) L(S) . That is, we have: \\mathbf{S} = \\mathbf{W} \\mathbf{X} \\mathbf{S} = \\mathbf{W} \\mathbf{X} I will provide intuition for the following relationships: \\frac{\\partial L}{\\partial \\mathbf{W}_{ij}} = (\\nabla_\\mathbf{X} (L(\\mathbf{X})) X^T))_{ij} \\frac{\\partial L}{\\partial \\mathbf{W}_{ij}} = (\\nabla_\\mathbf{X} (L(\\mathbf{X})) X^T))_{ij} and \\frac{\\partial L}{\\partial \\mathbf{X}_{ij}} = (\\mathbf{W^T} \\nabla_\\mathbf{W} (L(\\mathbf{W}))_{ij} \\frac{\\partial L}{\\partial \\mathbf{X}_{ij}} = (\\mathbf{W^T} \\nabla_\\mathbf{W} (L(\\mathbf{W}))_{ij} It is a simple formula but perhaps not so obvious. Here, I show where it comes from. Also see Wikipedia on Scalar by Matrix . For the following derivation, we have a bias of 0. Setup \u00b6 from sympy import * Weight Matrix and Data Matrix \u00b6 #Weight Matrix - 3 classes and dimensionality of two W = MatrixSymbol('W',3,2) W.is_real = True #Data to be trained on - 3 examples X = MatrixSymbol('X',2,3) X.is_real = True Matrix(W) \\displaystyle \\left[\\begin{array}{cc}W_{0, 0} & W_{0, 1}\\\\W_{1, 0} & W_{1, 1}\\\\W_{2, 0} & W_{2, 1}\\end{array}\\right] \\displaystyle \\left[\\begin{array}{cc}W_{0, 0} & W_{0, 1}\\\\W_{1, 0} & W_{1, 1}\\\\W_{2, 0} & W_{2, 1}\\end{array}\\right] Matrix(X) \\displaystyle \\left[\\begin{array}{ccc}X_{0, 0} & X_{0, 1} & X_{0, 2}\\\\X_{1, 0} & X_{1, 1} & X_{1, 2}\\end{array}\\right] \\displaystyle \\left[\\begin{array}{ccc}X_{0, 0} & X_{0, 1} & X_{0, 2}\\\\X_{1, 0} & X_{1, 1} & X_{1, 2}\\end{array}\\right] Classification \u00b6 Y holds the classification for each vector in X. That is, vector X_0 X_0 (column 0 of X) is labelled beforehand as belonging class 0, X_1 X_1 belongs to class 1 and so on... Y = Matrix([0,1,2]) Score Matrix \u00b6 S = Matrix(MatrixSymbol('s',3,3)) S \\displaystyle \\left[\\begin{array}{ccc}s_{0, 0} & s_{0, 1} & s_{0, 2}\\\\s_{1, 0} & s_{1, 1} & s_{1, 2}\\\\s_{2, 0} & s_{2, 1} & s_{2, 2}\\end{array}\\right] \\displaystyle \\left[\\begin{array}{ccc}s_{0, 0} & s_{0, 1} & s_{0, 2}\\\\s_{1, 0} & s_{1, 1} & s_{1, 2}\\\\s_{2, 0} & s_{2, 1} & s_{2, 2}\\end{array}\\right] Computing the values of the Score Matrix S S S_expanded = W*X Matrix(S_expanded) \\displaystyle \\left[\\begin{array}{ccc}W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0} & W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1} & W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}\\\\W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0} & W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1} & W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}\\\\W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0} & W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1} & W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}\\end{array}\\right] \\displaystyle \\left[\\begin{array}{ccc}W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0} & W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1} & W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}\\\\W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0} & W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1} & W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}\\\\W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0} & W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1} & W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}\\end{array}\\right] Loss Function \u00b6 L_i = -\\log ( \\frac {e^{S_{y[i],i}}} {\\sum _r {e^{S_r,i}}}) L_i = -\\log ( \\frac {e^{S_{y[i],i}}} {\\sum _r {e^{S_r,i}}}) L = \\frac {1} {N} \\sum _i {L_i} L = \\frac {1} {N} \\sum _i {L_i} Softmax computes the above loss on each column and then averages the columns. The elements in a particular column of the score vector above represent the clasification scores for a particular input vector column from X. [S_{0,0}, S_{1,0}, S_{2,0}] [S_{0,0}, S_{1,0}, S_{2,0}] is the first column in S S . S_{0,0} S_{0,0} is the score that vector X_0 X_0 recieves for class 0. S_{2,1} S_{2,1} is the score that vector X_1 X_1 recieves for class 2 and so on. Consider a column S_i S_i from S S ; a perfect weight matrix would generate a 0 in the rows of that column that do not correspond with the label of the corresponding X X column, and a nonzero value in the row that corresponds to the correct label. The index of the correct label in column S_i S_i is given by S_{y[i],i} S_{y[i],i} where y is defined above. Compute denominator for each column \u00b6 denoms = [] f_exp = lambda x : exp(x) for col in range(3): denoms += [sum(Matrix(S[:,col]).applyfunc(f_exp))] Column Sum Sanity Check \u00b6 denoms[2] \\displaystyle e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}} \\displaystyle e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}} Get numerator \u00b6 The Y vector tells us which row from each column in the score matrix S is the ground truth(a.k.a the correct class). The numerator of the operand of the -log in the loss formula is is e raised to the ground truth score. truth_scores = [] for col in range(3): truth_scores += [S[Y[col],col]] numers = [exp(val) for val in truth_scores] Compute L_i L_i \u00b6 L_i = [] for i in range(3): L_i += [-(log(numers[i]) - log(denoms[i]))] Sanity Check \u00b6 L_i[0] \\displaystyle \\log{\\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \\right)} - \\log{\\left(e^{s_{0, 0}} \\right)} \\displaystyle \\log{\\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \\right)} - \\log{\\left(e^{s_{0, 0}} \\right)} Total Loss \u00b6 Below is the analytical expression for our total loss - note that we have a bias of 0 in this example. Also note that the loss function is a function of the entries in the score matrix and the elements in the score matrix are in turn a function of the elements in the weight matrix. from functools import reduce from operator import add Loss = Rational(1,3)*reduce(add, L_i) Loss \\displaystyle \\frac{\\log{\\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \\right)}}{3} + \\frac{\\log{\\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}} \\right)}}{3} + \\frac{\\log{\\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{0, 0}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{1, 1}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{2, 2}} \\right)}}{3} \\displaystyle \\frac{\\log{\\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \\right)}}{3} + \\frac{\\log{\\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}} \\right)}}{3} + \\frac{\\log{\\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{0, 0}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{1, 1}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{2, 2}} \\right)}}{3} Computing the Gradient with respect to: \u00b6 Note: We substitute S S for its expanded W \\times X W \\times X form in the following computations. S_flat = [el for sublist in S.tolist() for el in sublist] S_expanded_flat = [el for sublist in Matrix(S_expanded).tolist() for el in sublist] sublist_S_to_W = [(S_flat[i], S_expanded_flat[i]) for i in range(9)] sublist_W_to_S = [(S_expanded_flat[i],S_flat[i]) for i in range(9)] 1 . \\frac {\\partial L} {\\partial W_{0,0}} \\frac {\\partial L} {\\partial W_{0,0}} \u00b6 diff(Loss.subs(sublist_S_to_W),W[0,0]) \\displaystyle - \\frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3} + \\frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{0, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{0, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)} \\displaystyle - \\frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3} + \\frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{0, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{0, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)} As you should notice, we have a sum of rational terms with X_{0,0} X_{0,0} , X_{0,1} X_{0,1} , and X_{0,2} X_{0,2} . This seems to imply a sort of dot product between row 0 of X and some other vector to get the gradient with respect to W_{0,0} W_{0,0} . 2 . \\frac {\\partial L} {\\partial W_{1,0}} \\frac {\\partial L} {\\partial W_{1,0}} \u00b6 diff(Loss.subs(sublist_S_to_W),W[1,0]) \\displaystyle - \\frac{e^{- W_{1, 0} X_{0, 1} - W_{1, 1} X_{1, 1}} e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3} + \\frac{e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} X_{0, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} X_{0, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)} \\displaystyle - \\frac{e^{- W_{1, 0} X_{0, 1} - W_{1, 1} X_{1, 1}} e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3} + \\frac{e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} X_{0, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} X_{0, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)} We again see a dependency on X_{0,0} X_{0,0} , X_{0,1} X_{0,1} , and X_{0,2} X_{0,2} 3 . \\frac {\\partial L} {\\partial W_{0,1}} \\frac {\\partial L} {\\partial W_{0,1}} \u00b6 diff(Loss.subs(sublist_S_to_W),W[0,1]) \\displaystyle - \\frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3} + \\frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{1, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{1, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)} \\displaystyle - \\frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3} + \\frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{1, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{1, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)} We now see a dependency on X_{1,0} X_{1,0} , X_{1,1} X_{1,1} , and X_{1,2} X_{1,2} Score Matrix \u00b6 Lets take a look at the gradient with respect to the score matrix. We accomplish this by substituing S_{i,j} S_{i,j} = W_{i,0}X_{0,j} + W_{i,1}X_{1,j} W_{i,0}X_{0,j} + W_{i,1}X_{1,j} diff(Loss.subs(sublist_S_to_W),W[0,1]).subs(sublist_W_to_S) \\displaystyle - \\frac{X_{1, 0}}{3} + \\frac{e^{s_{0, 2}} X_{1, 2}}{3 \\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}\\right)} + \\frac{e^{s_{0, 1}} X_{1, 1}}{3 \\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}}\\right)} + \\frac{e^{s_{0, 0}} X_{1, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)} \\displaystyle - \\frac{X_{1, 0}}{3} + \\frac{e^{s_{0, 2}} X_{1, 2}}{3 \\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}\\right)} + \\frac{e^{s_{0, 1}} X_{1, 1}}{3 \\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}}\\right)} + \\frac{e^{s_{0, 0}} X_{1, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)} As you can see, in general, the gradient, \\frac {\\partial L} {\\partial W_{i,j}} \\frac {\\partial L} {\\partial W_{i,j}} is a function of the score matrix S, and the input matrix X. A Closer Look \u00b6 If you play around a bit more, you'll notice the -\\frac {X_{i,j}} {3} -\\frac {X_{i,j}} {3} term that shows up whenever we differentiate with respect to W_{i,j} W_{i,j} . This is no coincidence. Let's take a look at \\frac {\\partial L_0} {\\partial W_{0,0}} \\frac {\\partial L_0} {\\partial W_{0,0}} - that is, the gradient of the loss on the first column in the score matrix with respect to W_{0,0} W_{0,0} diff(Rational(1,3)*L_i[0].subs(sublist_S_to_W), W[0,0]).subs(sublist_W_to_S) \\displaystyle - \\frac{X_{0, 0}}{3} + \\frac{e^{s_{0, 0}} X_{0, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)} \\displaystyle - \\frac{X_{0, 0}}{3} + \\frac{e^{s_{0, 0}} X_{0, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)} We see that - \\frac {X_{0,0}} {3} - \\frac {X_{0,0}} {3} shows up in the first term! We know that the entire loss function gradient with respect to W_{0,0} W_{0,0} is composed of the sum of the loss gradient on each score column again with respect to W_{0,0} W_{0,0} . Another way to say this is: \\frac {\\partial L} {\\partial W_{0,0}} = \\frac {\\partial L_0} {\\partial W_{0,0}} + \\frac {\\partial L_1} {\\partial W_{0,0}} +. \\frac {\\partial L_2} {\\partial W_{0,0}} \\frac {\\partial L} {\\partial W_{0,0}} = \\frac {\\partial L_0} {\\partial W_{0,0}} + \\frac {\\partial L_1} {\\partial W_{0,0}} +. \\frac {\\partial L_2} {\\partial W_{0,0}} To understand where that term comes from, we must look more closely at L_i = -\\log ( \\frac {e^{S_{y[i],i}}} {\\sum _r {e^{S_r,i}}}) = \\log (e^{S_{y[i],i}}) - \\log ({\\sum _r {e^{S_r,i}}}) L_i = -\\log ( \\frac {e^{S_{y[i],i}}} {\\sum _r {e^{S_r,i}}}) = \\log (e^{S_{y[i],i}}) - \\log ({\\sum _r {e^{S_r,i}}}) . Playing with L_0 L_0 \u00b6 We have L_0 = \\log (e^{S_{y[0],0}}) - \\log ({\\sum _r {e^{S_r,0}}}) L_0 = \\log (e^{S_{y[0],0}}) - \\log ({\\sum _r {e^{S_r,0}}}) . = \\log (e^{S_{y[0],0}}) - \\log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}) = \\log (e^{S_{y[0],0}}) - \\log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}) = \\log (e^{S_{0,0}}) - \\log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}) = \\log (e^{S_{0,0}}) - \\log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}) Taking the derivative with respect to S_{0,0} S_{0,0} we have: \\frac {\\partial L_0} {\\partial S_{0,0}} = 1 - \\frac {e^{S_{0,0}}} {e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}} \\frac {\\partial L_0} {\\partial S_{0,0}} = 1 - \\frac {e^{S_{0,0}}} {e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}} And taking the derivative with respect to S_{1,0} S_{1,0} we have: \\frac {\\partial L_0} {\\partial S_{0,0}} = - \\frac {e^{S_{0,0}}} {e^{S_{1,0}} + e^{S_{1,0}} + e^{S_{2,0}}} \\frac {\\partial L_0} {\\partial S_{0,0}} = - \\frac {e^{S_{0,0}}} {e^{S_{1,0}} + e^{S_{1,0}} + e^{S_{2,0}}} So we see that the 1 1 term shows up when we differentiate with respect to S_{y[i],i} S_{y[i],i} . This 1 1 becomes \\frac {1} {3} \\frac {1} {3} when we divide by the number of classes N N . Wrapping it all up \u00b6 Looking at the expanded expression for \\frac {\\partial L} {\\partial W_{0,0}} \\frac {\\partial L} {\\partial W_{0,0}} , \\frac {\\partial L} {\\partial W_{1,0}} \\frac {\\partial L} {\\partial W_{1,0}} , and \\frac {\\partial L} {\\partial W_{0,1}} \\frac {\\partial L} {\\partial W_{0,1}} above suggest that there is a dot product between row of X and the gradient of the Loss function over a column of S S . This is indeed true and careful inspection captures our observation in the following nice Matrix operation. \\begin{bmatrix} \\frac{\\partial L}{\\partial W_{00}}& \\frac{\\partial L}{\\partial W_{01}}\\\\ \\frac{\\partial L}{\\partial W_{10}}& \\frac{\\partial L}{\\partial W_{11}}\\\\ \\frac{\\partial L}{\\partial W_{20}}& \\frac{\\partial L}{\\partial W_{21}} \\end{bmatrix} =\\begin{bmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{bmatrix} X^T \\begin{bmatrix} \\frac{\\partial L}{\\partial W_{00}}& \\frac{\\partial L}{\\partial W_{01}}\\\\ \\frac{\\partial L}{\\partial W_{10}}& \\frac{\\partial L}{\\partial W_{11}}\\\\ \\frac{\\partial L}{\\partial W_{20}}& \\frac{\\partial L}{\\partial W_{21}} \\end{bmatrix} =\\begin{bmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{bmatrix} X^T Here we confirm that \\frac {\\partial L} {\\partial W_{0,0}} \\frac {\\partial L} {\\partial W_{0,0}} is indeed equal to element (0,0) (0,0) of \\begin{bmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{bmatrix} X^T \\begin{bmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{bmatrix} X^T #elementwise derivate of Loss function with respect to elements of #score matrix as shown above dLoss_dX = lambda x : diff(Loss,x) dL_dS = S.applyfunc(dLoss_dX) dW = dL_dS*Matrix(X.T) expand(diff(Loss.subs(sublist_S_to_W),W[0,0])) - expand(dW[0,0].subs(sublist_S_to_W)) \\displaystyle 0 \\displaystyle 0 Numerically evaluating dW would give us an incremental update to add to our weight matrix in order for us to minimize our loss function. Closing Remarks \u00b6 What if we wish to take the derivative of the loss function wit respect to X_{0,0} X_{0,0} ? We can do the following: \\begin{bmatrix} \\frac{\\partial L}{\\partial X_{00}}& \\frac{\\partial L}{\\partial X_{01}}& \\frac{\\partial L}{\\partial X_{02}}\\\\ \\frac{\\partial L}{\\partial X_{10}}& \\frac{\\partial L}{\\partial X_{11}}& \\frac{\\partial L}{\\partial X_{12}} \\end{bmatrix} = W^T \\begin{bmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{bmatrix} \\begin{bmatrix} \\frac{\\partial L}{\\partial X_{00}}& \\frac{\\partial L}{\\partial X_{01}}& \\frac{\\partial L}{\\partial X_{02}}\\\\ \\frac{\\partial L}{\\partial X_{10}}& \\frac{\\partial L}{\\partial X_{11}}& \\frac{\\partial L}{\\partial X_{12}} \\end{bmatrix} = W^T \\begin{bmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{bmatrix} Again, here we confirm that \\frac {\\partial L} {\\partial X_{0,0}} \\frac {\\partial L} {\\partial X_{0,0}} is indeed equal to element (0,0) (0,0) of W^T \\begin{pmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{pmatrix} W^T \\begin{pmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{pmatrix} dX = (Matrix(W.T)*dL_dS) expand(diff(Loss.subs(sublist_S_to_W),X[0,0])) - expand(dX[0,0].subs(sublist_S_to_W)) \\displaystyle 0 \\displaystyle 0","title":"loss function gradient derivation"},{"location":"mathematics/loss_function_gradient_derivation/#gradients-on-loss-functions","text":"The formulation for the gradient on the loss function with respect to the weights arises a lot in machine learning as the following: Consider a score matrix S S , a weight matrix W W , an input matrix X X of compatible dimensions, and a scalar-valued loss function L(S) L(S) . That is, we have: \\mathbf{S} = \\mathbf{W} \\mathbf{X} \\mathbf{S} = \\mathbf{W} \\mathbf{X} I will provide intuition for the following relationships: \\frac{\\partial L}{\\partial \\mathbf{W}_{ij}} = (\\nabla_\\mathbf{X} (L(\\mathbf{X})) X^T))_{ij} \\frac{\\partial L}{\\partial \\mathbf{W}_{ij}} = (\\nabla_\\mathbf{X} (L(\\mathbf{X})) X^T))_{ij} and \\frac{\\partial L}{\\partial \\mathbf{X}_{ij}} = (\\mathbf{W^T} \\nabla_\\mathbf{W} (L(\\mathbf{W}))_{ij} \\frac{\\partial L}{\\partial \\mathbf{X}_{ij}} = (\\mathbf{W^T} \\nabla_\\mathbf{W} (L(\\mathbf{W}))_{ij} It is a simple formula but perhaps not so obvious. Here, I show where it comes from. Also see Wikipedia on Scalar by Matrix . For the following derivation, we have a bias of 0.","title":"Gradients on Loss Functions"},{"location":"mathematics/loss_function_gradient_derivation/#setup","text":"from sympy import *","title":"Setup"},{"location":"mathematics/loss_function_gradient_derivation/#weight-matrix-and-data-matrix","text":"#Weight Matrix - 3 classes and dimensionality of two W = MatrixSymbol('W',3,2) W.is_real = True #Data to be trained on - 3 examples X = MatrixSymbol('X',2,3) X.is_real = True Matrix(W) \\displaystyle \\left[\\begin{array}{cc}W_{0, 0} & W_{0, 1}\\\\W_{1, 0} & W_{1, 1}\\\\W_{2, 0} & W_{2, 1}\\end{array}\\right] \\displaystyle \\left[\\begin{array}{cc}W_{0, 0} & W_{0, 1}\\\\W_{1, 0} & W_{1, 1}\\\\W_{2, 0} & W_{2, 1}\\end{array}\\right] Matrix(X) \\displaystyle \\left[\\begin{array}{ccc}X_{0, 0} & X_{0, 1} & X_{0, 2}\\\\X_{1, 0} & X_{1, 1} & X_{1, 2}\\end{array}\\right] \\displaystyle \\left[\\begin{array}{ccc}X_{0, 0} & X_{0, 1} & X_{0, 2}\\\\X_{1, 0} & X_{1, 1} & X_{1, 2}\\end{array}\\right]","title":"Weight Matrix and Data Matrix"},{"location":"mathematics/loss_function_gradient_derivation/#classification","text":"Y holds the classification for each vector in X. That is, vector X_0 X_0 (column 0 of X) is labelled beforehand as belonging class 0, X_1 X_1 belongs to class 1 and so on... Y = Matrix([0,1,2])","title":"Classification"},{"location":"mathematics/loss_function_gradient_derivation/#score-matrix","text":"S = Matrix(MatrixSymbol('s',3,3)) S \\displaystyle \\left[\\begin{array}{ccc}s_{0, 0} & s_{0, 1} & s_{0, 2}\\\\s_{1, 0} & s_{1, 1} & s_{1, 2}\\\\s_{2, 0} & s_{2, 1} & s_{2, 2}\\end{array}\\right] \\displaystyle \\left[\\begin{array}{ccc}s_{0, 0} & s_{0, 1} & s_{0, 2}\\\\s_{1, 0} & s_{1, 1} & s_{1, 2}\\\\s_{2, 0} & s_{2, 1} & s_{2, 2}\\end{array}\\right] Computing the values of the Score Matrix S S S_expanded = W*X Matrix(S_expanded) \\displaystyle \\left[\\begin{array}{ccc}W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0} & W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1} & W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}\\\\W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0} & W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1} & W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}\\\\W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0} & W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1} & W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}\\end{array}\\right] \\displaystyle \\left[\\begin{array}{ccc}W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0} & W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1} & W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}\\\\W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0} & W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1} & W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}\\\\W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0} & W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1} & W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}\\end{array}\\right]","title":"Score Matrix"},{"location":"mathematics/loss_function_gradient_derivation/#loss-function","text":"L_i = -\\log ( \\frac {e^{S_{y[i],i}}} {\\sum _r {e^{S_r,i}}}) L_i = -\\log ( \\frac {e^{S_{y[i],i}}} {\\sum _r {e^{S_r,i}}}) L = \\frac {1} {N} \\sum _i {L_i} L = \\frac {1} {N} \\sum _i {L_i} Softmax computes the above loss on each column and then averages the columns. The elements in a particular column of the score vector above represent the clasification scores for a particular input vector column from X. [S_{0,0}, S_{1,0}, S_{2,0}] [S_{0,0}, S_{1,0}, S_{2,0}] is the first column in S S . S_{0,0} S_{0,0} is the score that vector X_0 X_0 recieves for class 0. S_{2,1} S_{2,1} is the score that vector X_1 X_1 recieves for class 2 and so on. Consider a column S_i S_i from S S ; a perfect weight matrix would generate a 0 in the rows of that column that do not correspond with the label of the corresponding X X column, and a nonzero value in the row that corresponds to the correct label. The index of the correct label in column S_i S_i is given by S_{y[i],i} S_{y[i],i} where y is defined above.","title":"Loss Function"},{"location":"mathematics/loss_function_gradient_derivation/#compute-denominator-for-each-column","text":"denoms = [] f_exp = lambda x : exp(x) for col in range(3): denoms += [sum(Matrix(S[:,col]).applyfunc(f_exp))]","title":"Compute denominator for each column"},{"location":"mathematics/loss_function_gradient_derivation/#column-sum-sanity-check","text":"denoms[2] \\displaystyle e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}} \\displaystyle e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}","title":"Column Sum Sanity Check"},{"location":"mathematics/loss_function_gradient_derivation/#get-numerator","text":"The Y vector tells us which row from each column in the score matrix S is the ground truth(a.k.a the correct class). The numerator of the operand of the -log in the loss formula is is e raised to the ground truth score. truth_scores = [] for col in range(3): truth_scores += [S[Y[col],col]] numers = [exp(val) for val in truth_scores]","title":"Get numerator"},{"location":"mathematics/loss_function_gradient_derivation/#compute-l_il_i","text":"L_i = [] for i in range(3): L_i += [-(log(numers[i]) - log(denoms[i]))]","title":"Compute L_iL_i"},{"location":"mathematics/loss_function_gradient_derivation/#sanity-check","text":"L_i[0] \\displaystyle \\log{\\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \\right)} - \\log{\\left(e^{s_{0, 0}} \\right)} \\displaystyle \\log{\\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \\right)} - \\log{\\left(e^{s_{0, 0}} \\right)}","title":"Sanity Check"},{"location":"mathematics/loss_function_gradient_derivation/#total-loss","text":"Below is the analytical expression for our total loss - note that we have a bias of 0 in this example. Also note that the loss function is a function of the entries in the score matrix and the elements in the score matrix are in turn a function of the elements in the weight matrix. from functools import reduce from operator import add Loss = Rational(1,3)*reduce(add, L_i) Loss \\displaystyle \\frac{\\log{\\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \\right)}}{3} + \\frac{\\log{\\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}} \\right)}}{3} + \\frac{\\log{\\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{0, 0}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{1, 1}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{2, 2}} \\right)}}{3} \\displaystyle \\frac{\\log{\\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \\right)}}{3} + \\frac{\\log{\\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}} \\right)}}{3} + \\frac{\\log{\\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{0, 0}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{1, 1}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{2, 2}} \\right)}}{3}","title":"Total Loss"},{"location":"mathematics/loss_function_gradient_derivation/#computing-the-gradient-with-respect-to","text":"Note: We substitute S S for its expanded W \\times X W \\times X form in the following computations. S_flat = [el for sublist in S.tolist() for el in sublist] S_expanded_flat = [el for sublist in Matrix(S_expanded).tolist() for el in sublist] sublist_S_to_W = [(S_flat[i], S_expanded_flat[i]) for i in range(9)] sublist_W_to_S = [(S_expanded_flat[i],S_flat[i]) for i in range(9)]","title":"Computing the Gradient with respect to:"},{"location":"mathematics/loss_function_gradient_derivation/#1-frac-partial-l-partial-w_00frac-partial-l-partial-w_00","text":"diff(Loss.subs(sublist_S_to_W),W[0,0]) \\displaystyle - \\frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3} + \\frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{0, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{0, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)} \\displaystyle - \\frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3} + \\frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{0, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{0, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)} As you should notice, we have a sum of rational terms with X_{0,0} X_{0,0} , X_{0,1} X_{0,1} , and X_{0,2} X_{0,2} . This seems to imply a sort of dot product between row 0 of X and some other vector to get the gradient with respect to W_{0,0} W_{0,0} .","title":"1 .\\frac {\\partial L} {\\partial W_{0,0}}\\frac {\\partial L} {\\partial W_{0,0}}"},{"location":"mathematics/loss_function_gradient_derivation/#2-frac-partial-l-partial-w_10frac-partial-l-partial-w_10","text":"diff(Loss.subs(sublist_S_to_W),W[1,0]) \\displaystyle - \\frac{e^{- W_{1, 0} X_{0, 1} - W_{1, 1} X_{1, 1}} e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3} + \\frac{e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} X_{0, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} X_{0, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)} \\displaystyle - \\frac{e^{- W_{1, 0} X_{0, 1} - W_{1, 1} X_{1, 1}} e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3} + \\frac{e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} X_{0, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} X_{0, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)} We again see a dependency on X_{0,0} X_{0,0} , X_{0,1} X_{0,1} , and X_{0,2} X_{0,2}","title":"2 . \\frac {\\partial L} {\\partial W_{1,0}}\\frac {\\partial L} {\\partial W_{1,0}}"},{"location":"mathematics/loss_function_gradient_derivation/#3-frac-partial-l-partial-w_01frac-partial-l-partial-w_01","text":"diff(Loss.subs(sublist_S_to_W),W[0,1]) \\displaystyle - \\frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3} + \\frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{1, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{1, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)} \\displaystyle - \\frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3} + \\frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{1, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{1, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)} We now see a dependency on X_{1,0} X_{1,0} , X_{1,1} X_{1,1} , and X_{1,2} X_{1,2}","title":"3 . \\frac {\\partial L} {\\partial W_{0,1}}\\frac {\\partial L} {\\partial W_{0,1}}"},{"location":"mathematics/loss_function_gradient_derivation/#score-matrix_1","text":"Lets take a look at the gradient with respect to the score matrix. We accomplish this by substituing S_{i,j} S_{i,j} = W_{i,0}X_{0,j} + W_{i,1}X_{1,j} W_{i,0}X_{0,j} + W_{i,1}X_{1,j} diff(Loss.subs(sublist_S_to_W),W[0,1]).subs(sublist_W_to_S) \\displaystyle - \\frac{X_{1, 0}}{3} + \\frac{e^{s_{0, 2}} X_{1, 2}}{3 \\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}\\right)} + \\frac{e^{s_{0, 1}} X_{1, 1}}{3 \\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}}\\right)} + \\frac{e^{s_{0, 0}} X_{1, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)} \\displaystyle - \\frac{X_{1, 0}}{3} + \\frac{e^{s_{0, 2}} X_{1, 2}}{3 \\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}\\right)} + \\frac{e^{s_{0, 1}} X_{1, 1}}{3 \\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}}\\right)} + \\frac{e^{s_{0, 0}} X_{1, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)} As you can see, in general, the gradient, \\frac {\\partial L} {\\partial W_{i,j}} \\frac {\\partial L} {\\partial W_{i,j}} is a function of the score matrix S, and the input matrix X.","title":"Score Matrix"},{"location":"mathematics/loss_function_gradient_derivation/#a-closer-look","text":"If you play around a bit more, you'll notice the -\\frac {X_{i,j}} {3} -\\frac {X_{i,j}} {3} term that shows up whenever we differentiate with respect to W_{i,j} W_{i,j} . This is no coincidence. Let's take a look at \\frac {\\partial L_0} {\\partial W_{0,0}} \\frac {\\partial L_0} {\\partial W_{0,0}} - that is, the gradient of the loss on the first column in the score matrix with respect to W_{0,0} W_{0,0} diff(Rational(1,3)*L_i[0].subs(sublist_S_to_W), W[0,0]).subs(sublist_W_to_S) \\displaystyle - \\frac{X_{0, 0}}{3} + \\frac{e^{s_{0, 0}} X_{0, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)} \\displaystyle - \\frac{X_{0, 0}}{3} + \\frac{e^{s_{0, 0}} X_{0, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)} We see that - \\frac {X_{0,0}} {3} - \\frac {X_{0,0}} {3} shows up in the first term! We know that the entire loss function gradient with respect to W_{0,0} W_{0,0} is composed of the sum of the loss gradient on each score column again with respect to W_{0,0} W_{0,0} . Another way to say this is: \\frac {\\partial L} {\\partial W_{0,0}} = \\frac {\\partial L_0} {\\partial W_{0,0}} + \\frac {\\partial L_1} {\\partial W_{0,0}} +. \\frac {\\partial L_2} {\\partial W_{0,0}} \\frac {\\partial L} {\\partial W_{0,0}} = \\frac {\\partial L_0} {\\partial W_{0,0}} + \\frac {\\partial L_1} {\\partial W_{0,0}} +. \\frac {\\partial L_2} {\\partial W_{0,0}} To understand where that term comes from, we must look more closely at L_i = -\\log ( \\frac {e^{S_{y[i],i}}} {\\sum _r {e^{S_r,i}}}) = \\log (e^{S_{y[i],i}}) - \\log ({\\sum _r {e^{S_r,i}}}) L_i = -\\log ( \\frac {e^{S_{y[i],i}}} {\\sum _r {e^{S_r,i}}}) = \\log (e^{S_{y[i],i}}) - \\log ({\\sum _r {e^{S_r,i}}}) .","title":"A Closer Look"},{"location":"mathematics/loss_function_gradient_derivation/#playing-with-l_0l_0","text":"We have L_0 = \\log (e^{S_{y[0],0}}) - \\log ({\\sum _r {e^{S_r,0}}}) L_0 = \\log (e^{S_{y[0],0}}) - \\log ({\\sum _r {e^{S_r,0}}}) . = \\log (e^{S_{y[0],0}}) - \\log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}) = \\log (e^{S_{y[0],0}}) - \\log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}) = \\log (e^{S_{0,0}}) - \\log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}) = \\log (e^{S_{0,0}}) - \\log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}) Taking the derivative with respect to S_{0,0} S_{0,0} we have: \\frac {\\partial L_0} {\\partial S_{0,0}} = 1 - \\frac {e^{S_{0,0}}} {e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}} \\frac {\\partial L_0} {\\partial S_{0,0}} = 1 - \\frac {e^{S_{0,0}}} {e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}} And taking the derivative with respect to S_{1,0} S_{1,0} we have: \\frac {\\partial L_0} {\\partial S_{0,0}} = - \\frac {e^{S_{0,0}}} {e^{S_{1,0}} + e^{S_{1,0}} + e^{S_{2,0}}} \\frac {\\partial L_0} {\\partial S_{0,0}} = - \\frac {e^{S_{0,0}}} {e^{S_{1,0}} + e^{S_{1,0}} + e^{S_{2,0}}} So we see that the 1 1 term shows up when we differentiate with respect to S_{y[i],i} S_{y[i],i} . This 1 1 becomes \\frac {1} {3} \\frac {1} {3} when we divide by the number of classes N N .","title":"Playing with L_0L_0"},{"location":"mathematics/loss_function_gradient_derivation/#wrapping-it-all-up","text":"Looking at the expanded expression for \\frac {\\partial L} {\\partial W_{0,0}} \\frac {\\partial L} {\\partial W_{0,0}} , \\frac {\\partial L} {\\partial W_{1,0}} \\frac {\\partial L} {\\partial W_{1,0}} , and \\frac {\\partial L} {\\partial W_{0,1}} \\frac {\\partial L} {\\partial W_{0,1}} above suggest that there is a dot product between row of X and the gradient of the Loss function over a column of S S . This is indeed true and careful inspection captures our observation in the following nice Matrix operation. \\begin{bmatrix} \\frac{\\partial L}{\\partial W_{00}}& \\frac{\\partial L}{\\partial W_{01}}\\\\ \\frac{\\partial L}{\\partial W_{10}}& \\frac{\\partial L}{\\partial W_{11}}\\\\ \\frac{\\partial L}{\\partial W_{20}}& \\frac{\\partial L}{\\partial W_{21}} \\end{bmatrix} =\\begin{bmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{bmatrix} X^T \\begin{bmatrix} \\frac{\\partial L}{\\partial W_{00}}& \\frac{\\partial L}{\\partial W_{01}}\\\\ \\frac{\\partial L}{\\partial W_{10}}& \\frac{\\partial L}{\\partial W_{11}}\\\\ \\frac{\\partial L}{\\partial W_{20}}& \\frac{\\partial L}{\\partial W_{21}} \\end{bmatrix} =\\begin{bmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{bmatrix} X^T Here we confirm that \\frac {\\partial L} {\\partial W_{0,0}} \\frac {\\partial L} {\\partial W_{0,0}} is indeed equal to element (0,0) (0,0) of \\begin{bmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{bmatrix} X^T \\begin{bmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{bmatrix} X^T #elementwise derivate of Loss function with respect to elements of #score matrix as shown above dLoss_dX = lambda x : diff(Loss,x) dL_dS = S.applyfunc(dLoss_dX) dW = dL_dS*Matrix(X.T) expand(diff(Loss.subs(sublist_S_to_W),W[0,0])) - expand(dW[0,0].subs(sublist_S_to_W)) \\displaystyle 0 \\displaystyle 0 Numerically evaluating dW would give us an incremental update to add to our weight matrix in order for us to minimize our loss function.","title":"Wrapping it all up"},{"location":"mathematics/loss_function_gradient_derivation/#closing-remarks","text":"What if we wish to take the derivative of the loss function wit respect to X_{0,0} X_{0,0} ? We can do the following: \\begin{bmatrix} \\frac{\\partial L}{\\partial X_{00}}& \\frac{\\partial L}{\\partial X_{01}}& \\frac{\\partial L}{\\partial X_{02}}\\\\ \\frac{\\partial L}{\\partial X_{10}}& \\frac{\\partial L}{\\partial X_{11}}& \\frac{\\partial L}{\\partial X_{12}} \\end{bmatrix} = W^T \\begin{bmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{bmatrix} \\begin{bmatrix} \\frac{\\partial L}{\\partial X_{00}}& \\frac{\\partial L}{\\partial X_{01}}& \\frac{\\partial L}{\\partial X_{02}}\\\\ \\frac{\\partial L}{\\partial X_{10}}& \\frac{\\partial L}{\\partial X_{11}}& \\frac{\\partial L}{\\partial X_{12}} \\end{bmatrix} = W^T \\begin{bmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{bmatrix} Again, here we confirm that \\frac {\\partial L} {\\partial X_{0,0}} \\frac {\\partial L} {\\partial X_{0,0}} is indeed equal to element (0,0) (0,0) of W^T \\begin{pmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{pmatrix} W^T \\begin{pmatrix} \\frac{\\partial L}{\\partial S_{00}}& \\frac{\\partial L}{\\partial S_{01}}& \\frac{\\partial L}{\\partial S_{02}}\\\\ \\frac{\\partial L}{\\partial S_{10}}& \\frac{\\partial L}{\\partial S_{11}}& \\frac{\\partial L}{\\partial S_{12}}\\\\ \\frac{\\partial L}{\\partial S_{20}}& \\frac{\\partial L}{\\partial S_{21}}& \\frac{\\partial L}{\\partial S_{22}}\\\\ \\end{pmatrix} dX = (Matrix(W.T)*dL_dS) expand(diff(Loss.subs(sublist_S_to_W),X[0,0])) - expand(dX[0,0].subs(sublist_S_to_W)) \\displaystyle 0 \\displaystyle 0","title":"Closing Remarks"},{"location":"mathematics/matlabs_very_real_problems/","text":"Matlab's Very Real Problems \u00b6 Here , Mathworks discusses reasons to choose Matlab over Python. Function names and signatures are familiar and memorable, making them as easy to write as they are to read. Try again Mathworks. Matlab's function signatures don't even support multiple outputs like some other nice languages I could name: cough Python cough ... The matrix-based MATLAB language lets you express math directly. Linear algebra in MATLAB is intuitive and concise. The same is true for data analytics, signal and image processing, control design, and other applications. This is somewhat superficial... I guess you need 7 extra character in Python to declare an array in Python. After the initial work of 7 characters, everything between Python in Matlab linear algebra wise is more or less comparable... #initial setup, done once from numpy import array #array(...) is 7 characters long, not including the \"...\" my_matrix = array([[1,2,3],[4,5,6]]) The desktop environment is tuned for iterative engineering and scientific workflows. Is this why Matlab STILL doesn't support code browsing in the IDE? Mathwork's last argument reads something like this and sums up the rest of their arguments: Engineers are not computer scientists - so we made computing easy with a natural language... What??!! First of all - electrical engineers WERE the first computer scientists. As well as mathematicians, chemists, and biologists... Computer Science didn't nominally exist until the late 60s or early 70's depending on who you ask. What Mathworks is really trying to hide is that Matlab is just a very dated programming language, and even when it was young - Matlab simply ignored many standard programming practices of the era. Matlab Doesn't Have Types \u00b6 OK - that is not entirely true - but types were added as an afterthought. The following code snippet will highlight the problem... >> a = [1 2 3 4; 5 6 7 8] a = 1 2 3 4 5 6 7 8 >> a(1,1) ans = 1 >> a(1,2) ans = 2 Ok... So we use parentheses to index into array elements. So, what if we want to get the size of array a, does Matlab return the result to me as an array? If so - I should be able to index into elements of that array right? >> size(a) ans = 2 4 >> size(a)(1) Error: Indexing with parentheses '()' must appear as the last operation of a valid indexing expression. NOPE Not even close!!! And literally every other programming language I can think of from the top of my head supports this... C, Python,... So in modern programming languages, to resolve an operator on an object, the type erm these days class of the object is evaluated before looking up the implementation of the operator for the class. Matlab's behavior here leads me to believe size is a rather old function - (1984 anyone?), and as soon as Matlab's parser sees size, it chokes because () after size probably isn't part of the original Matlab grammar. Let me just take a moment to let off steam about the fact that Matlab is one-indexed instead of zero-indexed unlike literally EVERY other language in existance. OK... technically - the spiritual parent of Matlab, Fortran, is one-indexed by default, but, you can change that in Fortran - not so in Matlab. Anyways - back to business - so it turns out that we can actually index into the result returned by the size function... >> my_size = size(a) my_size = 2 4 >> my_size(1) ans = 2 I Thought You Said that Wasn't Possible... \u00b6 So yes - it is possible - otherwise many of Mathwork's engineers would have a fit(it turns out to be quite important to be able determine matrix dimensions in Linear Algebra). So what's the problem here? size(a)(1) should work. Basically, it boils down to the sad fact that Matlab has a spaghetti implementation of a type system. Similar idiosyncrasies plague the implementation of Matlab's classes, import behaviors and more. Python Syntax - just easier as expected... \u00b6 >>> from numpy import array >>> a = array([[1,2,3,4],[5,6,7,8]]) >>> a array([[1, 2, 3, 4], [5, 6, 7, 8]]) >>> a.shape (2, 4) >>> a.shape[0] 2 >>> a.shape[1] 4 And the good thing about numpy is that the default backend is C if you're worried about speed. Nvidia provides an implementation of numpy built for CUDA GPUs and even Intel provides an implementation that tales advantage of x86 AVX2. Python can do virtually everything that Matlab can do just as fast and in some cases even faster, especially in Machine Learning with Google and Facebook pouring millions into Python C++ ML backends. Mathworks simply doesn't have that bandwidth. But of course Mathworks didn't mention this in their article. I'll just assume they forgot :)","title":"Matlab's very real problems"},{"location":"mathematics/matlabs_very_real_problems/#matlabs-very-real-problems","text":"Here , Mathworks discusses reasons to choose Matlab over Python. Function names and signatures are familiar and memorable, making them as easy to write as they are to read. Try again Mathworks. Matlab's function signatures don't even support multiple outputs like some other nice languages I could name: cough Python cough ... The matrix-based MATLAB language lets you express math directly. Linear algebra in MATLAB is intuitive and concise. The same is true for data analytics, signal and image processing, control design, and other applications. This is somewhat superficial... I guess you need 7 extra character in Python to declare an array in Python. After the initial work of 7 characters, everything between Python in Matlab linear algebra wise is more or less comparable... #initial setup, done once from numpy import array #array(...) is 7 characters long, not including the \"...\" my_matrix = array([[1,2,3],[4,5,6]]) The desktop environment is tuned for iterative engineering and scientific workflows. Is this why Matlab STILL doesn't support code browsing in the IDE? Mathwork's last argument reads something like this and sums up the rest of their arguments: Engineers are not computer scientists - so we made computing easy with a natural language... What??!! First of all - electrical engineers WERE the first computer scientists. As well as mathematicians, chemists, and biologists... Computer Science didn't nominally exist until the late 60s or early 70's depending on who you ask. What Mathworks is really trying to hide is that Matlab is just a very dated programming language, and even when it was young - Matlab simply ignored many standard programming practices of the era.","title":"Matlab's Very Real Problems"},{"location":"mathematics/matlabs_very_real_problems/#matlab-doesnt-have-types","text":"OK - that is not entirely true - but types were added as an afterthought. The following code snippet will highlight the problem... >> a = [1 2 3 4; 5 6 7 8] a = 1 2 3 4 5 6 7 8 >> a(1,1) ans = 1 >> a(1,2) ans = 2 Ok... So we use parentheses to index into array elements. So, what if we want to get the size of array a, does Matlab return the result to me as an array? If so - I should be able to index into elements of that array right? >> size(a) ans = 2 4 >> size(a)(1) Error: Indexing with parentheses '()' must appear as the last operation of a valid indexing expression. NOPE Not even close!!! And literally every other programming language I can think of from the top of my head supports this... C, Python,... So in modern programming languages, to resolve an operator on an object, the type erm these days class of the object is evaluated before looking up the implementation of the operator for the class. Matlab's behavior here leads me to believe size is a rather old function - (1984 anyone?), and as soon as Matlab's parser sees size, it chokes because () after size probably isn't part of the original Matlab grammar. Let me just take a moment to let off steam about the fact that Matlab is one-indexed instead of zero-indexed unlike literally EVERY other language in existance. OK... technically - the spiritual parent of Matlab, Fortran, is one-indexed by default, but, you can change that in Fortran - not so in Matlab. Anyways - back to business - so it turns out that we can actually index into the result returned by the size function... >> my_size = size(a) my_size = 2 4 >> my_size(1) ans = 2","title":"Matlab Doesn't Have Types"},{"location":"mathematics/matlabs_very_real_problems/#i-thought-you-said-that-wasnt-possible","text":"So yes - it is possible - otherwise many of Mathwork's engineers would have a fit(it turns out to be quite important to be able determine matrix dimensions in Linear Algebra). So what's the problem here? size(a)(1) should work. Basically, it boils down to the sad fact that Matlab has a spaghetti implementation of a type system. Similar idiosyncrasies plague the implementation of Matlab's classes, import behaviors and more.","title":"I Thought You Said that Wasn't Possible..."},{"location":"mathematics/matlabs_very_real_problems/#python-syntax-just-easier-as-expected","text":">>> from numpy import array >>> a = array([[1,2,3,4],[5,6,7,8]]) >>> a array([[1, 2, 3, 4], [5, 6, 7, 8]]) >>> a.shape (2, 4) >>> a.shape[0] 2 >>> a.shape[1] 4 And the good thing about numpy is that the default backend is C if you're worried about speed. Nvidia provides an implementation of numpy built for CUDA GPUs and even Intel provides an implementation that tales advantage of x86 AVX2. Python can do virtually everything that Matlab can do just as fast and in some cases even faster, especially in Machine Learning with Google and Facebook pouring millions into Python C++ ML backends. Mathworks simply doesn't have that bandwidth. But of course Mathworks didn't mention this in their article. I'll just assume they forgot :)","title":"Python Syntax - just easier as expected..."},{"location":"mathematics/EMAG_HW3/EMAG_HW3/","text":"Goodbye Matlab, Hello Python : Emag by example \u00b6 Here is an excerpt of some emag problems I worked on some time ago. The point here is not to follow the math, but to get an idea of how to do matlab and mathematica things in Python. I used python + sympy + numpy + matplotlib + jupyter to create a CAS mathematica like notebook. To install and repeat, simply make sure that you have python3 and pip3. pip3 install jupyterlab numpy sympy matplotlib What you see below is my exported jupyterlab notebook session as markdown with latex+mathjax enabled. Setup \u00b6 from ipywidgets import * import numpy as np import matplotlib.pyplot as plt from sympy import * from mpl_toolkits.mplot3d import Axes3D init_printing() #enable inline to print %matplotlib inline #enable notebook for interactice features such as sliders #matplotlib notebook #Some physics stuff from sympy.physics.vector import ReferenceFrame from sympy.physics.vector import curl #Arbitrary precision from mpmath import mp, degrees mp.dps = 15; mp.pretty = True Some Constants fE_0 = Float(8.85*10**(-12)) mu_0 = Float(1.256*10**(-6)) #Region 1 which is air fE_1 = E_0 #Region 2 - pexiglass fE_2 = 3.45*E_0 fE_1 \\displaystyle 8.85 \\cdot 10^{-12} \\displaystyle 8.85 \\cdot 10^{-12} fE_2 \\displaystyle 3.05325 \\cdot 10^{-11} \\displaystyle 3.05325 \\cdot 10^{-11} Problem 1 \u00b6 We can get \\eta_1 \\eta_1 and \\eta_2 \\eta_2 using \\eta = \\sqrt{\\frac {\\mu} {\\epsilon}} \\eta = \\sqrt{\\frac {\\mu} {\\epsilon}} #Get eta_1 and eta_2 eta_1 = sqrt(mu_0/fE_1) eta_2 = sqrt(mu_0/fE_2) Brewster's angle of incidence implies \\eta_2 \\cos(\\theta^t) = \\eta_1 \\cos(\\theta_B) \\eta_2 \\cos(\\theta^t) = \\eta_1 \\cos(\\theta_B) We derive: \\begin{align} \\cos \\theta^t = \\sqrt{1 - \\frac {\\epsilon_1}{\\epsilon_1 + \\epsilon_2}} \\end{align} \\begin{align} \\cos \\theta^t = \\sqrt{1 - \\frac {\\epsilon_1}{\\epsilon_1 + \\epsilon_2}} \\end{align} \\begin{align} \\cos \\theta^i = \\sqrt{\\frac{\\epsilon_1}{\\epsilon_2} \\cos^2 \\theta^t} \\end{align} \\begin{align} \\cos \\theta^i = \\sqrt{\\frac{\\epsilon_1}{\\epsilon_2} \\cos^2 \\theta^t} \\end{align} And we can solve for \\cos \\theta^i \\cos \\theta^i and \\cos \\theta^t \\cos \\theta^t below. cos_theta_t = sqrt(1 - (fE_1)/(fE_1 + fE_2)) cos_theta_i = sqrt((fE_1/fE_2)*(cos_theta_t**2)) We need the following reflection coefficients both for the parallel and perpendicular cases to compute the power fractions. \\begin{align} \\Gamma_{||} = \\frac {\\eta_2 \\cos \\theta^t- \\eta_1 \\cos \\theta^i} {\\eta_2 \\cos \\theta^t + \\eta_1 \\cos \\theta^i} \\end{align} \\begin{align} \\Gamma_{||} = \\frac {\\eta_2 \\cos \\theta^t- \\eta_1 \\cos \\theta^i} {\\eta_2 \\cos \\theta^t + \\eta_1 \\cos \\theta^i} \\end{align} \\begin{align} \\tau_{||} = \\frac {2 \\eta_2 \\cos \\theta^i} {\\eta_2 \\cos \\theta^t + \\eta_1 \\cos \\theta^i} \\end{align} \\begin{align} \\tau_{||} = \\frac {2 \\eta_2 \\cos \\theta^i} {\\eta_2 \\cos \\theta^t + \\eta_1 \\cos \\theta^i} \\end{align} \\begin{align} \\Gamma_\\perp = \\frac {\\eta_2 \\cos \\theta^i - \\eta_1 \\cos \\theta^t} {\\eta_2 \\cos \\theta^i - \\eta_1 \\cos \\theta^t} \\end{align} \\begin{align} \\Gamma_\\perp = \\frac {\\eta_2 \\cos \\theta^i - \\eta_1 \\cos \\theta^t} {\\eta_2 \\cos \\theta^i - \\eta_1 \\cos \\theta^t} \\end{align} \\begin{align} \\tau_\\perp = \\frac {2 \\eta_2 \\cos \\theta^i} {\\eta_2 \\cos \\theta^i + \\eta_1 \\cos \\theta^t} \\end{align} \\begin{align} \\tau_\\perp = \\frac {2 \\eta_2 \\cos \\theta^i} {\\eta_2 \\cos \\theta^i + \\eta_1 \\cos \\theta^t} \\end{align} ref_perp = (eta_2*cos_theta_i - eta_1*cos_theta_t)/(eta_2*cos_theta_i + eta_1*cos_theta_t) ref_par = (eta_2*cos_theta_t - eta_1*cos_theta_i)/(eta_2*cos_theta_t + eta_1*cos_theta_i) trans_par = (2*eta_2*cos_theta_i)/(eta_2*cos_theta_t + eta_1*cos_theta_i) trans_perp = (2*eta_2*cos_theta_i)/(eta_2*cos_theta_i+eta_1*cos_theta_t) Transmitted Power \u00b6 Now we can solve for the transmitted power using \\frac {1} {2} ( \\tau_\\perp^2 + \\tau_{||}^2 ) \\frac {1} {2} ( \\tau_\\perp^2 + \\tau_{||}^2 ) (1/2)*(trans_par**2) + (1/2)*(trans_perp**2) \\displaystyle 0.245924885051478 \\displaystyle 0.245924885051478 Reflected Power \u00b6 And a similar process for reflected power \\frac {1} {2} ( \\Gamma_\\perp^2 + \\Gamma_{||}^2 ) \\frac {1} {2} ( \\Gamma_\\perp^2 + \\Gamma_{||}^2 ) (1/2)*(ref_par**2) + (1/2)*(ref_perp**2) \\displaystyle 0.151559146572402 \\displaystyle 0.151559146572402 Polarization \u00b6 There is no reflected component. The transmitted wave is right hand circular. Problem 2 \u00b6 To start off, we find values \\epsilon_2 \\epsilon_2 and \\epsilon_1 \\epsilon_1 \\begin{align} \\eta_1 = \\sqrt{\\frac {\\mu_0}{70 \\epsilon_0}} \\end{align} \\begin{align} \\eta_1 = \\sqrt{\\frac {\\mu_0}{70 \\epsilon_0}} \\end{align} \\begin{align} \\eta_2 = \\sqrt{\\frac {\\mu_0}{\\epsilon_0}} \\end{align} \\begin{align} \\eta_2 = \\sqrt{\\frac {\\mu_0}{\\epsilon_0}} \\end{align} #Some constants feta_2 = Float(sqrt(mu_0/fE_0)) feta_1 = Float(sqrt(mu_0/(fE_0*70))) part a. \u00b6 Between the critical angles, the transmitted wave is able to pass the air-water boundary into the air. \\begin{align} \\theta_c = sin^{-1}(\\sqrt{\\frac{\\epsilon_2}{\\epsilon_1}}) \\end{align} \\begin{align} \\theta_c = sin^{-1}(\\sqrt{\\frac{\\epsilon_2}{\\epsilon_1}}) \\end{align} degrees(asin(sqrt((fE_1)/(70*fE_0)))) \\displaystyle 6.86456633770632 \\displaystyle 6.86456633770632 We must limit \\theta_i \\theta_i to [-\\theta_c, \\theta_c] [-\\theta_c, \\theta_c] In degrees this is [-6.9, 6.9] [-6.9, 6.9] part b. \u00b6 A parallel wave at the brewster angle provides the maximum power transmitted into the air. The brewster angle is easily found with: \\begin{align} \\theta_B = \\sin^{-1} \\sqrt{\\frac{\\epsilon_2}{\\epsilon_1 + \\epsilon_2}} \\end{align} \\begin{align} \\theta_B = \\sin^{-1} \\sqrt{\\frac{\\epsilon_2}{\\epsilon_1 + \\epsilon_2}} \\end{align} The brewster angle in degrees is: degrees(N(asin(sqrt(1/71)))) \\displaystyle 6.81582191825822 \\displaystyle 6.81582191825822 Some Graphs \u00b6 You can see what is happening with the reflection and transmission coefficients for both the parallel and perpendicular wave cases. epsilon_1, epsilon_2, theta_i = var('epsilon_1 epsilon_2 theta_i') Fi_to_t = asin((sqrt(epsilon_1)*sin(theta_i))/sqrt(epsilon_2)) Fi_to_t \\displaystyle \\operatorname{asin}{\\left(\\frac{\\sqrt{\\epsilon_{1}} \\sin{\\left(\\theta_{i} \\right)}}{\\sqrt{\\epsilon_{2}}} \\right)} \\displaystyle \\operatorname{asin}{\\left(\\frac{\\sqrt{\\epsilon_{1}} \\sin{\\left(\\theta_{i} \\right)}}{\\sqrt{\\epsilon_{2}}} \\right)} Reflection Perpendicular and Parallel \u00b6 eta_1, eta_2, theta_t = var('eta_1 eta_2 theta_t') F_ref_par = (eta_2*cos(theta_t) - eta_1*cos(theta_i))/(eta_2*cos(theta_t) + eta_1*cos(theta_i)) F_ref_par = F_ref_par.subs(theta_t,Fi_to_t) F_ref_perp = (eta_2*cos(theta_i) - eta_1*cos(theta_t))/(eta_2*cos(theta_i) + eta_1*cos(theta_t)) F_ref_perp = F_ref_perp.subs(theta_t,Fi_to_t) p = plot( abs(F_ref_perp.subs({eta_1:feta_1, eta_2:feta_2, epsilon_2:fE_0, epsilon_1:(70*fE_0)})), abs(F_ref_par.subs({eta_1:feta_1, eta_2:feta_2, epsilon_2:fE_0, epsilon_1:(70*fE_0)})), (theta_i,0,pi/20), show=False, legend=True ) p[0].line_color = 'green' p[0].label = '$|\\\\Gamma \\\\perp|$' p[1].label = '$|\\\\Gamma_{||}|$' p.title = '$|\\\\Gamma|$ vs. $\\\\theta^i$' p.xlabel = '$\\\\theta^i$' p.ylabel = '$\\\\Gamma$' p.show() Tranmission Perpendicular and Parallel \u00b6 F_trans_perp = (2*eta_2*cos(theta_i))/(eta_2*cos(theta_i) + eta_1*cos(theta_t)) F_trans_perp = F_trans_perp.subs(theta_t,Fi_to_t) #F_trans_par = F_trans_par.subs(theta_t,Fi_to_t) F_trans_par = (2*eta_2*cos(theta_i))/(eta_2*cos(theta_t) + eta_1*cos(theta_i)) F_trans_par = F_trans_par.subs(theta_t,Fi_to_t) #F_trans_par = F_trans_par.subs(theta_t,Fi_to_t) p = plot( abs(F_trans_perp.subs({eta_1:feta_1, eta_2:feta_2, epsilon_2:fE_0, epsilon_1:(70*fE_0)})), abs(F_trans_par.subs({eta_1:feta_1, eta_2:feta_2, epsilon_2:fE_0, epsilon_1:(70*fE_0)})), (theta_i,0,pi/20), show=False, legend=True ) p[0].line_color = 'green' p[0].label = '|$\\\\tau_\\\\perp|$' p[1].label = '|$\\\\tau_{||}|$' p.title = '$\\\\tau$ vs. $\\\\theta^i$' p.xlabel = '$\\\\theta^i$' p.ylabel = '$\\\\tau$' p.show()","title":"goodbye matlab, hello python with emag examples"},{"location":"mathematics/EMAG_HW3/EMAG_HW3/#goodbye-matlab-hello-python-emag-by-example","text":"Here is an excerpt of some emag problems I worked on some time ago. The point here is not to follow the math, but to get an idea of how to do matlab and mathematica things in Python. I used python + sympy + numpy + matplotlib + jupyter to create a CAS mathematica like notebook. To install and repeat, simply make sure that you have python3 and pip3. pip3 install jupyterlab numpy sympy matplotlib What you see below is my exported jupyterlab notebook session as markdown with latex+mathjax enabled.","title":"Goodbye Matlab, Hello Python : Emag by example"},{"location":"mathematics/EMAG_HW3/EMAG_HW3/#setup","text":"from ipywidgets import * import numpy as np import matplotlib.pyplot as plt from sympy import * from mpl_toolkits.mplot3d import Axes3D init_printing() #enable inline to print %matplotlib inline #enable notebook for interactice features such as sliders #matplotlib notebook #Some physics stuff from sympy.physics.vector import ReferenceFrame from sympy.physics.vector import curl #Arbitrary precision from mpmath import mp, degrees mp.dps = 15; mp.pretty = True Some Constants fE_0 = Float(8.85*10**(-12)) mu_0 = Float(1.256*10**(-6)) #Region 1 which is air fE_1 = E_0 #Region 2 - pexiglass fE_2 = 3.45*E_0 fE_1 \\displaystyle 8.85 \\cdot 10^{-12} \\displaystyle 8.85 \\cdot 10^{-12} fE_2 \\displaystyle 3.05325 \\cdot 10^{-11} \\displaystyle 3.05325 \\cdot 10^{-11}","title":"Setup"},{"location":"mathematics/EMAG_HW3/EMAG_HW3/#problem-1","text":"We can get \\eta_1 \\eta_1 and \\eta_2 \\eta_2 using \\eta = \\sqrt{\\frac {\\mu} {\\epsilon}} \\eta = \\sqrt{\\frac {\\mu} {\\epsilon}} #Get eta_1 and eta_2 eta_1 = sqrt(mu_0/fE_1) eta_2 = sqrt(mu_0/fE_2) Brewster's angle of incidence implies \\eta_2 \\cos(\\theta^t) = \\eta_1 \\cos(\\theta_B) \\eta_2 \\cos(\\theta^t) = \\eta_1 \\cos(\\theta_B) We derive: \\begin{align} \\cos \\theta^t = \\sqrt{1 - \\frac {\\epsilon_1}{\\epsilon_1 + \\epsilon_2}} \\end{align} \\begin{align} \\cos \\theta^t = \\sqrt{1 - \\frac {\\epsilon_1}{\\epsilon_1 + \\epsilon_2}} \\end{align} \\begin{align} \\cos \\theta^i = \\sqrt{\\frac{\\epsilon_1}{\\epsilon_2} \\cos^2 \\theta^t} \\end{align} \\begin{align} \\cos \\theta^i = \\sqrt{\\frac{\\epsilon_1}{\\epsilon_2} \\cos^2 \\theta^t} \\end{align} And we can solve for \\cos \\theta^i \\cos \\theta^i and \\cos \\theta^t \\cos \\theta^t below. cos_theta_t = sqrt(1 - (fE_1)/(fE_1 + fE_2)) cos_theta_i = sqrt((fE_1/fE_2)*(cos_theta_t**2)) We need the following reflection coefficients both for the parallel and perpendicular cases to compute the power fractions. \\begin{align} \\Gamma_{||} = \\frac {\\eta_2 \\cos \\theta^t- \\eta_1 \\cos \\theta^i} {\\eta_2 \\cos \\theta^t + \\eta_1 \\cos \\theta^i} \\end{align} \\begin{align} \\Gamma_{||} = \\frac {\\eta_2 \\cos \\theta^t- \\eta_1 \\cos \\theta^i} {\\eta_2 \\cos \\theta^t + \\eta_1 \\cos \\theta^i} \\end{align} \\begin{align} \\tau_{||} = \\frac {2 \\eta_2 \\cos \\theta^i} {\\eta_2 \\cos \\theta^t + \\eta_1 \\cos \\theta^i} \\end{align} \\begin{align} \\tau_{||} = \\frac {2 \\eta_2 \\cos \\theta^i} {\\eta_2 \\cos \\theta^t + \\eta_1 \\cos \\theta^i} \\end{align} \\begin{align} \\Gamma_\\perp = \\frac {\\eta_2 \\cos \\theta^i - \\eta_1 \\cos \\theta^t} {\\eta_2 \\cos \\theta^i - \\eta_1 \\cos \\theta^t} \\end{align} \\begin{align} \\Gamma_\\perp = \\frac {\\eta_2 \\cos \\theta^i - \\eta_1 \\cos \\theta^t} {\\eta_2 \\cos \\theta^i - \\eta_1 \\cos \\theta^t} \\end{align} \\begin{align} \\tau_\\perp = \\frac {2 \\eta_2 \\cos \\theta^i} {\\eta_2 \\cos \\theta^i + \\eta_1 \\cos \\theta^t} \\end{align} \\begin{align} \\tau_\\perp = \\frac {2 \\eta_2 \\cos \\theta^i} {\\eta_2 \\cos \\theta^i + \\eta_1 \\cos \\theta^t} \\end{align} ref_perp = (eta_2*cos_theta_i - eta_1*cos_theta_t)/(eta_2*cos_theta_i + eta_1*cos_theta_t) ref_par = (eta_2*cos_theta_t - eta_1*cos_theta_i)/(eta_2*cos_theta_t + eta_1*cos_theta_i) trans_par = (2*eta_2*cos_theta_i)/(eta_2*cos_theta_t + eta_1*cos_theta_i) trans_perp = (2*eta_2*cos_theta_i)/(eta_2*cos_theta_i+eta_1*cos_theta_t)","title":"Problem 1"},{"location":"mathematics/EMAG_HW3/EMAG_HW3/#transmitted-power","text":"Now we can solve for the transmitted power using \\frac {1} {2} ( \\tau_\\perp^2 + \\tau_{||}^2 ) \\frac {1} {2} ( \\tau_\\perp^2 + \\tau_{||}^2 ) (1/2)*(trans_par**2) + (1/2)*(trans_perp**2) \\displaystyle 0.245924885051478 \\displaystyle 0.245924885051478","title":"Transmitted Power"},{"location":"mathematics/EMAG_HW3/EMAG_HW3/#reflected-power","text":"And a similar process for reflected power \\frac {1} {2} ( \\Gamma_\\perp^2 + \\Gamma_{||}^2 ) \\frac {1} {2} ( \\Gamma_\\perp^2 + \\Gamma_{||}^2 ) (1/2)*(ref_par**2) + (1/2)*(ref_perp**2) \\displaystyle 0.151559146572402 \\displaystyle 0.151559146572402","title":"Reflected Power"},{"location":"mathematics/EMAG_HW3/EMAG_HW3/#polarization","text":"There is no reflected component. The transmitted wave is right hand circular.","title":"Polarization"},{"location":"mathematics/EMAG_HW3/EMAG_HW3/#problem-2","text":"To start off, we find values \\epsilon_2 \\epsilon_2 and \\epsilon_1 \\epsilon_1 \\begin{align} \\eta_1 = \\sqrt{\\frac {\\mu_0}{70 \\epsilon_0}} \\end{align} \\begin{align} \\eta_1 = \\sqrt{\\frac {\\mu_0}{70 \\epsilon_0}} \\end{align} \\begin{align} \\eta_2 = \\sqrt{\\frac {\\mu_0}{\\epsilon_0}} \\end{align} \\begin{align} \\eta_2 = \\sqrt{\\frac {\\mu_0}{\\epsilon_0}} \\end{align} #Some constants feta_2 = Float(sqrt(mu_0/fE_0)) feta_1 = Float(sqrt(mu_0/(fE_0*70)))","title":"Problem 2"},{"location":"mathematics/EMAG_HW3/EMAG_HW3/#part-a","text":"Between the critical angles, the transmitted wave is able to pass the air-water boundary into the air. \\begin{align} \\theta_c = sin^{-1}(\\sqrt{\\frac{\\epsilon_2}{\\epsilon_1}}) \\end{align} \\begin{align} \\theta_c = sin^{-1}(\\sqrt{\\frac{\\epsilon_2}{\\epsilon_1}}) \\end{align} degrees(asin(sqrt((fE_1)/(70*fE_0)))) \\displaystyle 6.86456633770632 \\displaystyle 6.86456633770632 We must limit \\theta_i \\theta_i to [-\\theta_c, \\theta_c] [-\\theta_c, \\theta_c] In degrees this is [-6.9, 6.9] [-6.9, 6.9]","title":"part a."},{"location":"mathematics/EMAG_HW3/EMAG_HW3/#part-b","text":"A parallel wave at the brewster angle provides the maximum power transmitted into the air. The brewster angle is easily found with: \\begin{align} \\theta_B = \\sin^{-1} \\sqrt{\\frac{\\epsilon_2}{\\epsilon_1 + \\epsilon_2}} \\end{align} \\begin{align} \\theta_B = \\sin^{-1} \\sqrt{\\frac{\\epsilon_2}{\\epsilon_1 + \\epsilon_2}} \\end{align} The brewster angle in degrees is: degrees(N(asin(sqrt(1/71)))) \\displaystyle 6.81582191825822 \\displaystyle 6.81582191825822","title":"part b."},{"location":"mathematics/EMAG_HW3/EMAG_HW3/#some-graphs","text":"You can see what is happening with the reflection and transmission coefficients for both the parallel and perpendicular wave cases. epsilon_1, epsilon_2, theta_i = var('epsilon_1 epsilon_2 theta_i') Fi_to_t = asin((sqrt(epsilon_1)*sin(theta_i))/sqrt(epsilon_2)) Fi_to_t \\displaystyle \\operatorname{asin}{\\left(\\frac{\\sqrt{\\epsilon_{1}} \\sin{\\left(\\theta_{i} \\right)}}{\\sqrt{\\epsilon_{2}}} \\right)} \\displaystyle \\operatorname{asin}{\\left(\\frac{\\sqrt{\\epsilon_{1}} \\sin{\\left(\\theta_{i} \\right)}}{\\sqrt{\\epsilon_{2}}} \\right)}","title":"Some Graphs"},{"location":"mathematics/EMAG_HW3/EMAG_HW3/#reflection-perpendicular-and-parallel","text":"eta_1, eta_2, theta_t = var('eta_1 eta_2 theta_t') F_ref_par = (eta_2*cos(theta_t) - eta_1*cos(theta_i))/(eta_2*cos(theta_t) + eta_1*cos(theta_i)) F_ref_par = F_ref_par.subs(theta_t,Fi_to_t) F_ref_perp = (eta_2*cos(theta_i) - eta_1*cos(theta_t))/(eta_2*cos(theta_i) + eta_1*cos(theta_t)) F_ref_perp = F_ref_perp.subs(theta_t,Fi_to_t) p = plot( abs(F_ref_perp.subs({eta_1:feta_1, eta_2:feta_2, epsilon_2:fE_0, epsilon_1:(70*fE_0)})), abs(F_ref_par.subs({eta_1:feta_1, eta_2:feta_2, epsilon_2:fE_0, epsilon_1:(70*fE_0)})), (theta_i,0,pi/20), show=False, legend=True ) p[0].line_color = 'green' p[0].label = '$|\\\\Gamma \\\\perp|$' p[1].label = '$|\\\\Gamma_{||}|$' p.title = '$|\\\\Gamma|$ vs. $\\\\theta^i$' p.xlabel = '$\\\\theta^i$' p.ylabel = '$\\\\Gamma$' p.show()","title":"Reflection Perpendicular and Parallel"},{"location":"mathematics/EMAG_HW3/EMAG_HW3/#tranmission-perpendicular-and-parallel","text":"F_trans_perp = (2*eta_2*cos(theta_i))/(eta_2*cos(theta_i) + eta_1*cos(theta_t)) F_trans_perp = F_trans_perp.subs(theta_t,Fi_to_t) #F_trans_par = F_trans_par.subs(theta_t,Fi_to_t) F_trans_par = (2*eta_2*cos(theta_i))/(eta_2*cos(theta_t) + eta_1*cos(theta_i)) F_trans_par = F_trans_par.subs(theta_t,Fi_to_t) #F_trans_par = F_trans_par.subs(theta_t,Fi_to_t) p = plot( abs(F_trans_perp.subs({eta_1:feta_1, eta_2:feta_2, epsilon_2:fE_0, epsilon_1:(70*fE_0)})), abs(F_trans_par.subs({eta_1:feta_1, eta_2:feta_2, epsilon_2:fE_0, epsilon_1:(70*fE_0)})), (theta_i,0,pi/20), show=False, legend=True ) p[0].line_color = 'green' p[0].label = '|$\\\\tau_\\\\perp|$' p[1].label = '|$\\\\tau_{||}|$' p.title = '$\\\\tau$ vs. $\\\\theta^i$' p.xlabel = '$\\\\theta^i$' p.ylabel = '$\\\\tau$' p.show()","title":"Tranmission Perpendicular and Parallel"},{"location":"my_setup/hardware/","text":"my hardware \u00b6 general purpose computer \u00b6 I currently use a Mac running MacOS. I like the Mac because of its robust hardware and MacOS because it is POSIX compliant making it more a less a UNIX descendant. I also periodically edit videos and write music, and Apple provides a Final Cut Pro and Logic Pro for about $300. Video editing with Final Cut is also second to none when it comes to video acceleration and great battery life. I also like the flexibility of the OS X windows manager and its re-programmability, which I make extensive use of. I truly do prefer the Linux UNIX-like ecosystem, but Mac OS was the best practical compromise I could find. I truly dislike Windows as a daily driver, and the only purpose it serves in my life is for playing occasional triple A titles that never quite make it to Mac OS. I interact with Linux regularly and like its robust and constantly updated secure kernel. It's the only system I would trust if I had to deploy a computer product. The i3Wm for X on Linux also has a special place in my heart for its clean and utility centric design. devices \u00b6 I list the devices i usually use, in no particular order. FPGA: ICE40HX8K Dev Board FPGA: ECP5 Dev Board Raspberry Pi Arduino Mkr1000 iMac G3 Powerbook Pismo","title":"hardware"},{"location":"my_setup/hardware/#my-hardware","text":"","title":"my hardware"},{"location":"my_setup/hardware/#general-purpose-computer","text":"I currently use a Mac running MacOS. I like the Mac because of its robust hardware and MacOS because it is POSIX compliant making it more a less a UNIX descendant. I also periodically edit videos and write music, and Apple provides a Final Cut Pro and Logic Pro for about $300. Video editing with Final Cut is also second to none when it comes to video acceleration and great battery life. I also like the flexibility of the OS X windows manager and its re-programmability, which I make extensive use of. I truly do prefer the Linux UNIX-like ecosystem, but Mac OS was the best practical compromise I could find. I truly dislike Windows as a daily driver, and the only purpose it serves in my life is for playing occasional triple A titles that never quite make it to Mac OS. I interact with Linux regularly and like its robust and constantly updated secure kernel. It's the only system I would trust if I had to deploy a computer product. The i3Wm for X on Linux also has a special place in my heart for its clean and utility centric design.","title":"general purpose computer"},{"location":"my_setup/hardware/#devices","text":"I list the devices i usually use, in no particular order. FPGA: ICE40HX8K Dev Board FPGA: ECP5 Dev Board Raspberry Pi Arduino Mkr1000 iMac G3 Powerbook Pismo","title":"devices"},{"location":"my_setup/software/","text":"Software Setup \u00b6 Sometimes I find myself forgetting how I configured and built particular software. This page is to remind myself of my base configuration and perhaps help others with similar issues. Package Manager and Environment \u00b6 I usually use the brew package manager due to its apparent and rather surprising robustness. I use MacPorts on my vintage Macs such as my iMac G3 since its PowerPC packages are still maintained. But brew seems to be far less invasive than MacPorts, going so far as to not require administrative privileges to install packages. Whatever package manager is used, I suggest being consistent, brew built software tends to easily find other brew packages - same with MacPorts. It is difficult to mix and match packages from different package managers. Various Packages \u00b6 GNU \u00b6 Since this is the 21 st and GNU really is the UNIX standard somewhat ironically - it is important to replace the deprecated BSD utils with their respective GNU coreutils. For example, ''sed'' becomes ''gsed'', ''readlink'' becomes ''greadlink'' and so on. Just do brew install coreutils Also, if you want to do any sort of useful/common software development, you'll want to install the following packages. Common \u00b6 $brew install cmake $brew install python3 $brew install pkg-config $brew install ctags Mathematics \u00b6 If you like to do mathematics(which there is really no reason you shouldn't), you have a couple of good options when it comes to open source. The Python route The Maxima route Note Technically this is not true. For those of you who like to reason about abstract algebras or work with the infinite, there are software like CoCoa (see Wikipedia ). I know there are other options that are big in research such as Julia(which i found to be rather brittle in my experience). But I've found the python route to be sufficient for my needs as it provides calculus operators, vector calculus operators, common linear algebra operators, and symbolic and or numerical manipulation for all the above. Sympy handles symbolics and integrates nicely with numpy for numerics. Sympy also generates LaTeX output which Jupyter readily consumes. Python Route \u00b6 Warning If you're not familiar with the python language, it is much easier to get started with WxMaxima. But I've found mathematics with python to be far more flexible when it comes to more complex operations such as plotting the gradient of an electromagnetic field. $pip3 install sympy numpy scipy jupyterlab Maxima Route \u00b6 $brew install maxima $brew install wxmaxima $pip3 install --user numpy scipy matplotlib ipython jupyter pandas sympy nose FPGA Dev \u00b6 brew install yosys brew install icarus-verilog brew install verilator Embedded Dev \u00b6 In case you have an MBED or Arduino lying around and don't already have enough troubles as it is. $brew tap ArmMbed/homebrew-formulae $brew install arm-none-eabi-gcc $brew tap osx-cross/avr $brew install avr-libc GUI Dev \u00b6 WxPython On MacOS - it's pretty simple brew install wxPython Ubuntu - it's not so simple replace 18.04 with your ubuntu version which you can determine by invoking lsb_release -a If you tried installing wxPython with apt - you may notice its broken. You might have to remove the wx.py in your home directory. rm ~/wx.py pip3 install -U -f https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-18.04 wxPython Verify your wxPython was correctly installed by running the wxPython demo. $git https://github.com/wxWidgets/Phoenix.git $cd Phoenix/demo $python3 demo.py","title":"software"},{"location":"my_setup/software/#software-setup","text":"Sometimes I find myself forgetting how I configured and built particular software. This page is to remind myself of my base configuration and perhaps help others with similar issues.","title":"Software Setup"},{"location":"my_setup/software/#package-manager-and-environment","text":"I usually use the brew package manager due to its apparent and rather surprising robustness. I use MacPorts on my vintage Macs such as my iMac G3 since its PowerPC packages are still maintained. But brew seems to be far less invasive than MacPorts, going so far as to not require administrative privileges to install packages. Whatever package manager is used, I suggest being consistent, brew built software tends to easily find other brew packages - same with MacPorts. It is difficult to mix and match packages from different package managers.","title":"Package Manager and Environment"},{"location":"my_setup/software/#various-packages","text":"","title":"Various Packages"},{"location":"my_setup/software/#gnu","text":"Since this is the 21 st and GNU really is the UNIX standard somewhat ironically - it is important to replace the deprecated BSD utils with their respective GNU coreutils. For example, ''sed'' becomes ''gsed'', ''readlink'' becomes ''greadlink'' and so on. Just do brew install coreutils Also, if you want to do any sort of useful/common software development, you'll want to install the following packages.","title":"GNU"},{"location":"my_setup/software/#common","text":"$brew install cmake $brew install python3 $brew install pkg-config $brew install ctags","title":"Common"},{"location":"my_setup/software/#mathematics","text":"If you like to do mathematics(which there is really no reason you shouldn't), you have a couple of good options when it comes to open source. The Python route The Maxima route Note Technically this is not true. For those of you who like to reason about abstract algebras or work with the infinite, there are software like CoCoa (see Wikipedia ). I know there are other options that are big in research such as Julia(which i found to be rather brittle in my experience). But I've found the python route to be sufficient for my needs as it provides calculus operators, vector calculus operators, common linear algebra operators, and symbolic and or numerical manipulation for all the above. Sympy handles symbolics and integrates nicely with numpy for numerics. Sympy also generates LaTeX output which Jupyter readily consumes.","title":"Mathematics"},{"location":"my_setup/software/#python-route","text":"Warning If you're not familiar with the python language, it is much easier to get started with WxMaxima. But I've found mathematics with python to be far more flexible when it comes to more complex operations such as plotting the gradient of an electromagnetic field. $pip3 install sympy numpy scipy jupyterlab","title":"Python Route"},{"location":"my_setup/software/#maxima-route","text":"$brew install maxima $brew install wxmaxima $pip3 install --user numpy scipy matplotlib ipython jupyter pandas sympy nose","title":"Maxima Route"},{"location":"my_setup/software/#fpga-dev","text":"brew install yosys brew install icarus-verilog brew install verilator","title":"FPGA Dev"},{"location":"my_setup/software/#embedded-dev","text":"In case you have an MBED or Arduino lying around and don't already have enough troubles as it is. $brew tap ArmMbed/homebrew-formulae $brew install arm-none-eabi-gcc $brew tap osx-cross/avr $brew install avr-libc","title":"Embedded Dev"},{"location":"my_setup/software/#gui-dev","text":"WxPython On MacOS - it's pretty simple brew install wxPython Ubuntu - it's not so simple replace 18.04 with your ubuntu version which you can determine by invoking lsb_release -a If you tried installing wxPython with apt - you may notice its broken. You might have to remove the wx.py in your home directory. rm ~/wx.py pip3 install -U -f https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-18.04 wxPython Verify your wxPython was correctly installed by running the wxPython demo. $git https://github.com/wxWidgets/Phoenix.git $cd Phoenix/demo $python3 demo.py","title":"GUI Dev"}]}