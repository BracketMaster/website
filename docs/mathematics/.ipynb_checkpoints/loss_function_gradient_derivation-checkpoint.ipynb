{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients on Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formulation for the gradient on the loss function with respect to the weights arises a lot in machine learning.\n",
    "\n",
    "It is a simple formula but perhaps not so obvious. Here, I show where it comes from.\n",
    "\n",
    "For the following derivation, we have a bias of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Matrix and Data Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight Matrix - 3 classes and dimensionality of two\n",
    "W = MatrixSymbol('W',3,2)\n",
    "W.is_real = True\n",
    "#Data to be trained on - 3 examples\n",
    "X.is_real = True\n",
    "X = MatrixSymbol('X',2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{array}{cc}W_{0, 0} & W_{0, 1}\\\\W_{1, 0} & W_{1, 1}\\\\W_{2, 0} & W_{2, 1}\\end{array}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[W[0, 0], W[0, 1]],\n",
       "[W[1, 0], W[1, 1]],\n",
       "[W[2, 0], W[2, 1]]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{array}{ccc}X_{0, 0} & X_{0, 1} & X_{0, 2}\\\\X_{1, 0} & X_{1, 1} & X_{1, 2}\\end{array}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[X[0, 0], X[0, 1], X[0, 2]],\n",
       "[X[1, 0], X[1, 1], X[1, 2]]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y holds the classification for each vector in X. That is, vector $X_0$(column 0 of X) is labelled beforehand as belonging class 0, $X_1$ belongs to class 1 and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Matrix([0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{array}{ccc}s_{0, 0} & s_{0, 1} & s_{0, 2}\\\\s_{1, 0} & s_{1, 1} & s_{1, 2}\\\\s_{2, 0} & s_{2, 1} & s_{2, 2}\\end{array}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[s[0, 0], s[0, 1], s[0, 2]],\n",
       "[s[1, 0], s[1, 1], s[1, 2]],\n",
       "[s[2, 0], s[2, 1], s[2, 2]]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = Matrix(MatrixSymbol('s',3,3))\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the values of the Score Matrix $S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{array}{ccc}W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0} & W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1} & W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}\\\\W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0} & W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1} & W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}\\\\W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0} & W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1} & W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}\\end{array}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[W[0, 0]*X[0, 0] + W[0, 1]*X[1, 0], W[0, 0]*X[0, 1] + W[0, 1]*X[1, 1], W[0, 0]*X[0, 2] + W[0, 1]*X[1, 2]],\n",
       "[W[1, 0]*X[0, 0] + W[1, 1]*X[1, 0], W[1, 0]*X[0, 1] + W[1, 1]*X[1, 1], W[1, 0]*X[0, 2] + W[1, 1]*X[1, 2]],\n",
       "[W[2, 0]*X[0, 0] + W[2, 1]*X[1, 0], W[2, 0]*X[0, 1] + W[2, 1]*X[1, 1], W[2, 0]*X[0, 2] + W[2, 1]*X[1, 2]]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_expanded = W*X\n",
    "Matrix(S_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_i = -\\log ( \\frac {e^{S_{y[i],i}}} {\\sum _r {e^{S_r,i}}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L = \\frac {1} {N} \\sum _i {L_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax computes the above loss on each column and then averages the columns.\n",
    "\n",
    "The elements in a particular column of the score vector above represent the clasification scores for a particular input vector column from X.\n",
    "\n",
    "$[S_{0,0}, S_{1,0}, S_{2,0}]$ is the first column in $S$.\n",
    "$S_{0,0}$ is the score that vector $X_0$ recieves for class 0.\n",
    "$S_{2,1}$ is the score that vector $X_1$ recieves for class 2 and so on.\n",
    "\n",
    "Consider a column $S_i$ from $S$; a perfect weight matrix would generate a 0 in the rows of that column that do not correspond with the label of the corresponding $X$ column, and a nonzero value in the row that corresponds to the correct label. \n",
    "\n",
    "The index of the correct label in column $S_i$ is given by $S_{y[i],i}$ where y is defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute denominator for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoms = []\n",
    "f_exp = lambda x : exp(x)\n",
    "for col in range(3):\n",
    "    denoms += [sum(Matrix(S[:,col]).applyfunc(f_exp))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Sum Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}$"
      ],
      "text/plain": [
       "exp(s[0, 2]) + exp(s[1, 2]) + exp(s[2, 2])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoms[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get numerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Y vector tells us which row from each column in the score matrix S is the ground truth(a.k.a the correct class).\n",
    "\n",
    "The numerator of the operand of the -log in the loss formula is is e raised to the ground truth score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_scores = []\n",
    "for col in range(3):\n",
    "    truth_scores += [S[Y[col],col]]\n",
    "numers = [exp(val) for val in truth_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute $L_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_i = []\n",
    "for i in range(3):\n",
    "    L_i += [-(log(numers[i]) - log(denoms[i]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\log{\\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \\right)} - \\log{\\left(e^{s_{0, 0}} \\right)}$"
      ],
      "text/plain": [
       "log(exp(s[0, 0]) + exp(s[1, 0]) + exp(s[2, 0])) - log(exp(s[0, 0]))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_i[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the analytical expression for our total loss - note that we have a bias of 0 in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\log{\\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}} \\right)}}{3} + \\frac{\\log{\\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}} \\right)}}{3} + \\frac{\\log{\\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{0, 0}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{1, 1}} \\right)}}{3} - \\frac{\\log{\\left(e^{s_{2, 2}} \\right)}}{3}$"
      ],
      "text/plain": [
       "log(exp(s[0, 0]) + exp(s[1, 0]) + exp(s[2, 0]))/3 + log(exp(s[0, 1]) + exp(s[1, 1]) + exp(s[2, 1]))/3 + log(exp(s[0, 2]) + exp(s[1, 2]) + exp(s[2, 2]))/3 - log(exp(s[0, 0]))/3 - log(exp(s[1, 1]))/3 - log(exp(s[2, 2]))/3"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from operator import add\n",
    "Loss = Rational(1,3)*reduce(add, L_i)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Gradient with respect to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We substitute $S$ for its expanded $W \\times X$ form in the following computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_flat = [el for sublist in S.tolist() for el in sublist]\n",
    "S_expanded_flat = [el for sublist in Matrix(S_expanded).tolist() for el in sublist]\n",
    "sublist_S_to_W = [(S_flat[i], S_expanded_flat[i]) for i in range(9)]\n",
    "sublist_W_to_S = [(S_expanded_flat[i],S_flat[i]) for i in range(9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 .$\\frac {\\partial L} {\\partial W_{0,0}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3} + \\frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{0, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{0, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{0, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)}$"
      ],
      "text/plain": [
       "-exp(-W[0, 0]*X[0, 0] - W[0, 1]*X[1, 0])*exp(W[0, 0]*X[0, 0] + W[0, 1]*X[1, 0])*X[0, 0]/3 + exp(W[0, 0]*X[0, 2] + W[0, 1]*X[1, 2])*X[0, 2]/(3*(exp(W[0, 0]*X[0, 2] + W[0, 1]*X[1, 2]) + exp(W[1, 0]*X[0, 2] + W[1, 1]*X[1, 2]) + exp(W[2, 0]*X[0, 2] + W[2, 1]*X[1, 2]))) + exp(W[0, 0]*X[0, 1] + W[0, 1]*X[1, 1])*X[0, 1]/(3*(exp(W[0, 0]*X[0, 1] + W[0, 1]*X[1, 1]) + exp(W[1, 0]*X[0, 1] + W[1, 1]*X[1, 1]) + exp(W[2, 0]*X[0, 1] + W[2, 1]*X[1, 1]))) + exp(W[0, 0]*X[0, 0] + W[0, 1]*X[1, 0])*X[0, 0]/(3*(exp(W[0, 0]*X[0, 0] + W[0, 1]*X[1, 0]) + exp(W[1, 0]*X[0, 0] + W[1, 1]*X[1, 0]) + exp(W[2, 0]*X[0, 0] + W[2, 1]*X[1, 0])))"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(Loss.subs(sublist_S_to_W),W[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you should notice, we have a sum of rational terms with $X_{0,0}$, $X_{0,1}$, and $X_{0,2}$. This seems to imply a sort of dot product between row 0 of X and some other vector to get the gradient with respect to $W_{0,0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 . $\\frac {\\partial L} {\\partial W_{1,0}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{e^{- W_{1, 0} X_{0, 1} - W_{1, 1} X_{1, 1}} e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3} + \\frac{e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} X_{0, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} X_{0, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} X_{0, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)}$"
      ],
      "text/plain": [
       "-exp(-W[1, 0]*X[0, 1] - W[1, 1]*X[1, 1])*exp(W[1, 0]*X[0, 1] + W[1, 1]*X[1, 1])*X[0, 1]/3 + exp(W[1, 0]*X[0, 2] + W[1, 1]*X[1, 2])*X[0, 2]/(3*(exp(W[0, 0]*X[0, 2] + W[0, 1]*X[1, 2]) + exp(W[1, 0]*X[0, 2] + W[1, 1]*X[1, 2]) + exp(W[2, 0]*X[0, 2] + W[2, 1]*X[1, 2]))) + exp(W[1, 0]*X[0, 1] + W[1, 1]*X[1, 1])*X[0, 1]/(3*(exp(W[0, 0]*X[0, 1] + W[0, 1]*X[1, 1]) + exp(W[1, 0]*X[0, 1] + W[1, 1]*X[1, 1]) + exp(W[2, 0]*X[0, 1] + W[2, 1]*X[1, 1]))) + exp(W[1, 0]*X[0, 0] + W[1, 1]*X[1, 0])*X[0, 0]/(3*(exp(W[0, 0]*X[0, 0] + W[0, 1]*X[1, 0]) + exp(W[1, 0]*X[0, 0] + W[1, 1]*X[1, 0]) + exp(W[2, 0]*X[0, 0] + W[2, 1]*X[1, 0])))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(Loss.subs(sublist_S_to_W),W[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again see a dependency on $X_{0,0}$, $X_{0,1}$, and $X_{0,2}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 . $\\frac {\\partial L} {\\partial W_{0,1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{e^{- W_{0, 0} X_{0, 0} - W_{0, 1} X_{1, 0}} e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3} + \\frac{e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} X_{1, 2}}{3 \\left(e^{W_{0, 0} X_{0, 2} + W_{0, 1} X_{1, 2}} + e^{W_{1, 0} X_{0, 2} + W_{1, 1} X_{1, 2}} + e^{W_{2, 0} X_{0, 2} + W_{2, 1} X_{1, 2}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} X_{1, 1}}{3 \\left(e^{W_{0, 0} X_{0, 1} + W_{0, 1} X_{1, 1}} + e^{W_{1, 0} X_{0, 1} + W_{1, 1} X_{1, 1}} + e^{W_{2, 0} X_{0, 1} + W_{2, 1} X_{1, 1}}\\right)} + \\frac{e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} X_{1, 0}}{3 \\left(e^{W_{0, 0} X_{0, 0} + W_{0, 1} X_{1, 0}} + e^{W_{1, 0} X_{0, 0} + W_{1, 1} X_{1, 0}} + e^{W_{2, 0} X_{0, 0} + W_{2, 1} X_{1, 0}}\\right)}$"
      ],
      "text/plain": [
       "-exp(-W[0, 0]*X[0, 0] - W[0, 1]*X[1, 0])*exp(W[0, 0]*X[0, 0] + W[0, 1]*X[1, 0])*X[1, 0]/3 + exp(W[0, 0]*X[0, 2] + W[0, 1]*X[1, 2])*X[1, 2]/(3*(exp(W[0, 0]*X[0, 2] + W[0, 1]*X[1, 2]) + exp(W[1, 0]*X[0, 2] + W[1, 1]*X[1, 2]) + exp(W[2, 0]*X[0, 2] + W[2, 1]*X[1, 2]))) + exp(W[0, 0]*X[0, 1] + W[0, 1]*X[1, 1])*X[1, 1]/(3*(exp(W[0, 0]*X[0, 1] + W[0, 1]*X[1, 1]) + exp(W[1, 0]*X[0, 1] + W[1, 1]*X[1, 1]) + exp(W[2, 0]*X[0, 1] + W[2, 1]*X[1, 1]))) + exp(W[0, 0]*X[0, 0] + W[0, 1]*X[1, 0])*X[1, 0]/(3*(exp(W[0, 0]*X[0, 0] + W[0, 1]*X[1, 0]) + exp(W[1, 0]*X[0, 0] + W[1, 1]*X[1, 0]) + exp(W[2, 0]*X[0, 0] + W[2, 1]*X[1, 0])))"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(Loss.subs(sublist_S_to_W),W[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see a dependency on $X_{1,0}$, $X_{1,1}$, and $X_{1,2}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the gradient with respect to the score matrix.\n",
    "We accomplish this by substituing $S_{i,j}$ = $W_{i,0}X_{0,j} + W_{i,1}X_{1,j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{X_{1, 0}}{3} + \\frac{e^{s_{0, 2}} X_{1, 2}}{3 \\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}\\right)} + \\frac{e^{s_{0, 1}} X_{1, 1}}{3 \\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}}\\right)} + \\frac{e^{s_{0, 0}} X_{1, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)}$"
      ],
      "text/plain": [
       "-X[1, 0]/3 + exp(s[0, 2])*X[1, 2]/(3*(exp(s[0, 2]) + exp(s[1, 2]) + exp(s[2, 2]))) + exp(s[0, 1])*X[1, 1]/(3*(exp(s[0, 1]) + exp(s[1, 1]) + exp(s[2, 1]))) + exp(s[0, 0])*X[1, 0]/(3*(exp(s[0, 0]) + exp(s[1, 0]) + exp(s[2, 0])))"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(Loss.subs(sublist_S_to_W),W[0,1]).subs(sublist_W_to_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in general, the gradient, $\\frac {\\partial L} {\\partial W_{i,j}}$ is a function of the score matrix S, and the input matrix X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Closer Look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you play around a bit more, you'll notice the $-\\frac {X_{i,j}} {3}$ term that shows up whenever we differentiate with respect to $W_{i,j}$. This is no coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at $\\frac {\\partial L_0} {\\partial W_{0,0}}$ - that is, the gradient of the loss on the first column in the score matrix with respect to $W_{0,0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{X_{0, 0}}{3} + \\frac{e^{s_{0, 0}} X_{0, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)}$"
      ],
      "text/plain": [
       "-X[0, 0]/3 + exp(s[0, 0])*X[0, 0]/(3*(exp(s[0, 0]) + exp(s[1, 0]) + exp(s[2, 0])))"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(Rational(1,3)*L_i[0].subs(sublist_S_to_W), W[0,0]).subs(sublist_W_to_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $- \\frac {X_{0,0}} {3}$ shows up in the first term!\n",
    "\n",
    "We know that the entire loss function gradient with respect to $W_{0,0}$ is composed of the sum of the loss gradient on each score column again with respect to $W_{0,0}$. Another way to say this is:\n",
    "\n",
    "$\\frac {\\partial L} {\\partial W_{0,0}} = \\frac {\\partial L_0} {\\partial W_{0,0}} + \\frac {\\partial L_1} {\\partial W_{0,0}} +. \\frac {\\partial L_2} {\\partial W_{0,0}}$\n",
    "\n",
    "To understand where that term comes from, we must look more closely at \n",
    "$L_i = -\\log ( \\frac {e^{S_{y[i],i}}} {\\sum _r {e^{S_r,i}}}) = \\log (e^{S_{y[i],i}}) - \\log ({\\sum _r {e^{S_r,i}}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with $L_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have \n",
    "$L_0 = \\log (e^{S_{y[0],0}}) - \\log ({\\sum _r {e^{S_r,0}}})$.\n",
    "\n",
    "$= \\log (e^{S_{y[0],0}}) - \\log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}})$\n",
    "\n",
    "$= \\log (e^{S_{0,0}}) - \\log (e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}})$\n",
    "\n",
    "Taking the derivative with respect to $S_{0,0}$ we have:\n",
    "\n",
    "$\\frac {\\partial L_0} {\\partial S_{0,0}} = 1 - \\frac {e^{S_{0,0}}} {e^{S_{0,0}} + e^{S_{1,0}} + e^{S_{2,0}}}$\n",
    "\n",
    "And taking the derivative with respect to $S_{1,0}$ we have:\n",
    "\n",
    "$\\frac {\\partial L_0} {\\partial S_{0,0}} = - \\frac {e^{S_{0,0}}} {e^{S_{1,0}} + e^{S_{1,0}} + e^{S_{2,0}}}$\n",
    "\n",
    "So we see that the $1$ term shows up when we differentiate with respect to $S_{y[i],i}$.\n",
    "\n",
    "This $1$ becomes $\\frac {1} {3}$ when we divide by the number of classes $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping it all up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the expanded expression for $\\frac {\\partial L} {\\partial W_{0,0}}$, $\\frac {\\partial L} {\\partial W_{1,0}}$, and $\\frac {\\partial L} {\\partial W_{0,1}}$ above suggest that there is a dot product between row of X and the gradient of the Loss function over a column of $S$. \n",
    "\n",
    "This is indeed true and careful inspection captures our observation in the following nice Matrix operation.\n",
    "\n",
    "$\n",
    " \\begin{bmatrix}\n",
    " \\frac{\\partial L}{\\partial W_{00}}&\n",
    " \\frac{\\partial L}{\\partial W_{01}}\\\\\n",
    " \\frac{\\partial L}{\\partial W_{10}}&\n",
    " \\frac{\\partial L}{\\partial W_{11}}\\\\\n",
    " \\frac{\\partial L}{\\partial W_{20}}&\n",
    " \\frac{\\partial L}{\\partial W_{21}}\n",
    " \\end{bmatrix}\n",
    " =\\begin{bmatrix}\n",
    " \\frac{\\partial L}{\\partial S_{00}}&\n",
    " \\frac{\\partial L}{\\partial S_{01}}&\n",
    " \\frac{\\partial L}{\\partial S_{02}}\\\\\n",
    " \\frac{\\partial L}{\\partial S_{10}}&\n",
    " \\frac{\\partial L}{\\partial S_{11}}&\n",
    " \\frac{\\partial L}{\\partial S_{12}}\\\\\n",
    " \\frac{\\partial L}{\\partial S_{20}}&\n",
    " \\frac{\\partial L}{\\partial S_{21}}&\n",
    " \\frac{\\partial L}{\\partial S_{22}}\\\\\n",
    " \\end{bmatrix} \\times X^T$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{array}{cc}\\left(- \\frac{1}{3} + \\frac{e^{s_{0, 0}}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)}\\right) X_{0, 0} + \\frac{e^{s_{0, 2}} X_{0, 2}}{3 \\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}\\right)} + \\frac{e^{s_{0, 1}} X_{0, 1}}{3 \\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}}\\right)} & \\left(- \\frac{1}{3} + \\frac{e^{s_{0, 0}}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)}\\right) X_{1, 0} + \\frac{e^{s_{0, 2}} X_{1, 2}}{3 \\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}\\right)} + \\frac{e^{s_{0, 1}} X_{1, 1}}{3 \\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}}\\right)}\\\\\\left(- \\frac{1}{3} + \\frac{e^{s_{1, 1}}}{3 \\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}}\\right)}\\right) X_{0, 1} + \\frac{e^{s_{1, 2}} X_{0, 2}}{3 \\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}\\right)} + \\frac{e^{s_{1, 0}} X_{0, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)} & \\left(- \\frac{1}{3} + \\frac{e^{s_{1, 1}}}{3 \\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}}\\right)}\\right) X_{1, 1} + \\frac{e^{s_{1, 2}} X_{1, 2}}{3 \\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}\\right)} + \\frac{e^{s_{1, 0}} X_{1, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)}\\\\\\left(- \\frac{1}{3} + \\frac{e^{s_{2, 2}}}{3 \\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}\\right)}\\right) X_{0, 2} + \\frac{e^{s_{2, 1}} X_{0, 1}}{3 \\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}}\\right)} + \\frac{e^{s_{2, 0}} X_{0, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)} & \\left(- \\frac{1}{3} + \\frac{e^{s_{2, 2}}}{3 \\left(e^{s_{0, 2}} + e^{s_{1, 2}} + e^{s_{2, 2}}\\right)}\\right) X_{1, 2} + \\frac{e^{s_{2, 1}} X_{1, 1}}{3 \\left(e^{s_{0, 1}} + e^{s_{1, 1}} + e^{s_{2, 1}}\\right)} + \\frac{e^{s_{2, 0}} X_{1, 0}}{3 \\left(e^{s_{0, 0}} + e^{s_{1, 0}} + e^{s_{2, 0}}\\right)}\\end{array}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[(-1/3 + exp(s[0, 0])/(3*(exp(s[0, 0]) + exp(s[1, 0]) + exp(s[2, 0]))))*X[0, 0] + exp(s[0, 2])*X[0, 2]/(3*(exp(s[0, 2]) + exp(s[1, 2]) + exp(s[2, 2]))) + exp(s[0, 1])*X[0, 1]/(3*(exp(s[0, 1]) + exp(s[1, 1]) + exp(s[2, 1]))), (-1/3 + exp(s[0, 0])/(3*(exp(s[0, 0]) + exp(s[1, 0]) + exp(s[2, 0]))))*X[1, 0] + exp(s[0, 2])*X[1, 2]/(3*(exp(s[0, 2]) + exp(s[1, 2]) + exp(s[2, 2]))) + exp(s[0, 1])*X[1, 1]/(3*(exp(s[0, 1]) + exp(s[1, 1]) + exp(s[2, 1])))],\n",
       "[(-1/3 + exp(s[1, 1])/(3*(exp(s[0, 1]) + exp(s[1, 1]) + exp(s[2, 1]))))*X[0, 1] + exp(s[1, 2])*X[0, 2]/(3*(exp(s[0, 2]) + exp(s[1, 2]) + exp(s[2, 2]))) + exp(s[1, 0])*X[0, 0]/(3*(exp(s[0, 0]) + exp(s[1, 0]) + exp(s[2, 0]))), (-1/3 + exp(s[1, 1])/(3*(exp(s[0, 1]) + exp(s[1, 1]) + exp(s[2, 1]))))*X[1, 1] + exp(s[1, 2])*X[1, 2]/(3*(exp(s[0, 2]) + exp(s[1, 2]) + exp(s[2, 2]))) + exp(s[1, 0])*X[1, 0]/(3*(exp(s[0, 0]) + exp(s[1, 0]) + exp(s[2, 0])))],\n",
       "[(-1/3 + exp(s[2, 2])/(3*(exp(s[0, 2]) + exp(s[1, 2]) + exp(s[2, 2]))))*X[0, 2] + exp(s[2, 1])*X[0, 1]/(3*(exp(s[0, 1]) + exp(s[1, 1]) + exp(s[2, 1]))) + exp(s[2, 0])*X[0, 0]/(3*(exp(s[0, 0]) + exp(s[1, 0]) + exp(s[2, 0]))), (-1/3 + exp(s[2, 2])/(3*(exp(s[0, 2]) + exp(s[1, 2]) + exp(s[2, 2]))))*X[1, 2] + exp(s[2, 1])*X[1, 1]/(3*(exp(s[0, 1]) + exp(s[1, 1]) + exp(s[2, 1]))) + exp(s[2, 0])*X[1, 0]/(3*(exp(s[0, 0]) + exp(s[1, 0]) + exp(s[2, 0])))]])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW = Matrix(\n",
    "    [\n",
    "        [diff(Loss,S[0,0]),diff(Loss,S[0,1]),diff(Loss,S[0,2])],\n",
    "        [diff(Loss,S[1,0]),diff(Loss,S[1,1]),diff(Loss,S[1,2])],\n",
    "        [diff(Loss,S[2,0]),diff(Loss,S[2,1]),diff(Loss,S[2,2])]\n",
    "    ])*Matrix(X.T)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerically evaluating dW would give us an incremental update to add to our weight matrix in order for us to minimize our loss function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
